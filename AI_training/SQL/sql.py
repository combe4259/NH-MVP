# -*- coding: utf-8 -*-
"""sql.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vaGZTZ7y0SYLarCX0QemkUernLyohswz

#1ì°¨ Fine-tuning
#pko-t5-baseë¥¼ ì´ìš©í•œ Fine-tuning ëª¨ë¸ ìƒì„±
"""

# 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
!pip install transformers[torch] datasets sentencepiece huggingface_hub safetensors accelerate

import json
import torch
import pandas as pd
from datasets import Dataset, DatasetDict
from huggingface_hub import notebook_login
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer
)

print("-" * 50)
print(f"PyTorch Device: {'cuda' if torch.cuda.is_available() else 'cpu'}")
print("í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ëª¨ë‘ ì„¤ì¹˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
print("-" * 50)



# ----------------------------------------------------------------------
# 2. Hugging Face ë¡œê·¸ì¸
# ----------------------------------------------------------------------
print("Hugging Face ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤ (ëª¨ë¸ ì—…ë¡œë“œìš©)...")
notebook_login()

"""#pko-t5-base ëª¨ë¸"""

# ----------------------------------------------------------------------
# paust/pko-t5-base ëª¨ë¸ Fine-tuning
# ----------------------------------------------------------------------
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

# í•œêµ­ì–´ T5 ëª¨ë¸
BASE_MODEL_NAME = "paust/pko-t5-base"
NEW_MODEL_REPO_ID = "combe4259/SQLNL"

print(f"ìƒˆ ëª¨ë¸ ë¡œë“œ ì‹œì‘: {BASE_MODEL_NAME}")

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)
model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_NAME)

print("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
print(f"í† í¬ë‚˜ì´ì € vocab: {tokenizer.vocab_size}")
print(f"ëª¨ë¸ vocab: {model.config.vocab_size}")
print(f"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")

# ëª¨ë¸ì„ í›ˆë ¨ ëª¨ë“œë¡œ ì„¤ì •
model.train()
print(f"í›ˆë ¨ ëª¨ë“œ í™œì„±í™”: {model.training}")

# Vocab í¬ê¸° í™•ì¸ ë° ì¡°ì •
if tokenizer.vocab_size != model.config.vocab_size:
    print("Vocab í¬ê¸° ë¶ˆì¼ì¹˜ - ì¡°ì •")
    model.resize_token_embeddings(tokenizer.vocab_size)
    print(f"ì¡°ì • ì™„ë£Œ: {model.config.vocab_size}")
else:
    print("Vocab í¬ê¸° ì¼ì¹˜ - ì¡°ì • ë¶ˆí•„ìš”")

# ê¸°ë³¸ í…ŒìŠ¤íŠ¸
print("\nê¸°ë³¸ ë™ì‘ í…ŒìŠ¤íŠ¸:")
test_input = "í…ŒìŠ¤íŠ¸"
inputs = tokenizer(test_input, return_tensors="pt")
print(f"í† í°í™” ì„±ê³µ: {inputs['input_ids'].shape}")

# GPU ì´ë™ ë° forward pass í…ŒìŠ¤íŠ¸
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
inputs = {k: v.to(device) for k, v in inputs.items()}

model.eval()
with torch.no_grad():
    try:
        outputs = model(**inputs)
        print(f"Forward pass ì„±ê³µ: logits shape = {outputs.logits.shape}")
        print(f"Logits ë²”ìœ„: {outputs.logits.min().item():.4f} ~ {outputs.logits.max().item():.4f}")
        print(f"Logitsì— NaN: {torch.isnan(outputs.logits).any()}")
    except Exception as e:
        print(f"Forward pass ì‹¤íŒ¨: {e}")

print("paust/pko-t5-base ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")

# ----------------------------------------------------------------------
# 3.5 Google Drive ë§ˆìš´íŠ¸
# ----------------------------------------------------------------------
from google.colab import drive
import os

print("Google Drive ë§ˆìš´íŠ¸ë¥¼ ì‹œì‘")
drive.mount('/content/drive')

DRIVE_SQL_PATH = "/content/drive/MyDrive/sqldataset"

# ----------------------------------------------------------------------
# 4. ë°ì´í„°ì…‹ ì¤€ë¹„ ë° ì „ì²˜ë¦¬
# ----------------------------------------------------------------------
import os
import glob
from datasets import Dataset, DatasetDict
import json

ROOT_DATASET_PATH = "/content/drive/MyDrive/sqldataset"
TRAIN_DATA_PATH = os.path.join(ROOT_DATASET_PATH, "Training")
VALIDATION_DATA_PATH = os.path.join(ROOT_DATASET_PATH, "Validation")

def create_schema_text_map(root_data_path):
    """
    ì§€ì •ëœ ê²½ë¡œì˜ ëª¨ë“  í•˜ìœ„ í´ë”ë¥¼ ê²€ìƒ‰í•˜ì—¬
    ëª¨ë“  '...db_annotation.json' íŒŒì¼ì„ ì°¾ì•„ í•˜ë‚˜ì˜ ìŠ¤í‚¤ë§ˆ ë§µìœ¼ë¡œ
    """
    print(f"  ... ìŠ¤í‚¤ë§ˆ íŒŒì¼ ê²€ìƒ‰ ì‹œì‘ (ì „ì²´ ê²½ë¡œ): {root_data_path}")

    # Training/dataì™€ Validation/data ê²½ë¡œì—ì„œ ìŠ¤í‚¤ë§ˆ íŒŒì¼ ê²€ìƒ‰
    schema_files = []
    for sub_path in ['Training/data', 'Validation/data']:
        full_path = os.path.join(root_data_path, sub_path)
        if os.path.exists(full_path):
            schema_files.extend(glob.glob(f"{full_path}/**/*_db_annotation.json", recursive=True))

    if not schema_files:
        print(f"[ì˜¤ë¥˜] ìŠ¤í‚¤ë§ˆ íŒŒì¼(*_db_annotation.json)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    print(f"  ... ì´ {len(schema_files)}ê°œì˜ ìŠ¤í‚¤ë§ˆ íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

    schema_map = {}
    for file_path in schema_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                raw_schema_data = json.load(f)

            if 'data' not in raw_schema_data:
                continue

            schema_data_list = raw_schema_data['data']
            for db in schema_data_list:
                db_id = db.get('db_id')
                if not db_id: continue

                schema_parts = []
                table_names = db.get('table_names_original', [])
                column_names_data = db.get('column_names_original', [])

                for i, table_name in enumerate(table_names):
                    cols = [col[1] for col in column_names_data if col[0] == i]
                    schema_parts.append(f"{table_name}: {', '.join(cols)}")

                if db_id not in schema_map:
                    schema_map[db_id] = " | ".join(schema_parts)
        except Exception as e:
            print(f"   ê²½ê³ : {file_path} íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    print(f"  ... ì´ {len(schema_map)}ê°œì˜ ê³ ìœ  DB ìŠ¤í‚¤ë§ˆë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    return schema_map

def load_labels_from_path(specific_data_path, schema_map):
    """
    íŠ¹ì • ê²½ë¡œ(Training ë˜ëŠ” Validation)ì—ì„œë§Œ ë¼ë²¨ íŒŒì¼ì„ ê²€ìƒ‰í•˜ì—¬ ì…ë ¥/ì¶œë ¥ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜.
    """
    print(f" ë¼ë²¨ íŒŒì¼ ê²€ìƒ‰ ì¤‘: {specific_data_path}")

    # label í´ë”ì—ì„œ ë¼ë²¨ íŒŒì¼ ê²€ìƒ‰
    label_path = os.path.join(specific_data_path, "label")
    label_files = []
    if os.path.exists(label_path):
        label_files = glob.glob(f"{label_path}/**/TEXT_NL2SQL_label_*.json", recursive=True)

    if not label_files:
        print(f"[ì˜¤ë¥˜] ë¼ë²¨ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    print(f"  ... {len(label_files)}ê°œì˜ ë¼ë²¨ íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

    inputs = []
    outputs = []
    missing_schema_count = 0
    total_items = 0

    for file_path in label_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                raw_data = json.load(f)

            if 'data' not in raw_data:
                continue

            data_list = raw_data['data']
            total_items += len(data_list)

            for item in data_list:
                db_id = item.get('db_id')
                if db_id in schema_map:
                    schema_text = schema_map[db_id]
                    utterance = item.get('utterance')
                    sql_query = item.get('query')

                    if not all([utterance, sql_query]):
                        continue

                    model_input_text = f"[SCHEMA: {schema_text}] [UTTERANCE: {utterance}]"
                    inputs.append(model_input_text)
                    outputs.append(sql_query)
                else:
                    missing_schema_count += 1
        except Exception as e:
            print(f" ê²½ê³ : {file_path} íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    print(f"  ... ì´ {total_items}ê°œ ë°ì´í„° ì¤‘ {len(inputs)}ê°œ ì²˜ë¦¬ ì™„ë£Œ.")
    if missing_schema_count > 0:
        print(f" ê²½ê³ : {missing_schema_count}ê°œ ë°ì´í„°ì˜ ìŠ¤í‚¤ë§ˆë¥¼ ì°¾ì§€ ëª»í•´ ì œì™¸í–ˆìŠµë‹ˆë‹¤.")

    return Dataset.from_dict({"input_text": inputs, "target_text": outputs})

# ì „ì²˜ë¦¬ í•¨ìˆ˜ - paddingì„ -100ìœ¼ë¡œ ë³€í™˜
def preprocess_function(examples):
    # ì…ë ¥ í† í°í™”
    model_inputs = tokenizer(
        examples['input_text'],
        max_length=512,
        padding="max_length",
        truncation=True
    )

    # íƒ€ê²Ÿ í† í°í™”
    labels = tokenizer(
        examples['target_text'],
        max_length=256,
        padding="max_length",
        truncation=True
    )

    # ë¼ë²¨ ì²˜ë¦¬
    model_inputs["labels"] = [
        [(label if label != tokenizer.pad_token_id else -100) for label in label_seq]
        for label_seq in labels["input_ids"]
    ]

    return model_inputs

# ë©”ì¸ ë°ì´í„° ë¡œì§ ì‹¤í–‰
print("\n[2/7] AI Hub ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹œì‘...")

# 1. ìŠ¤í‚¤ë§ˆ ë§µ ìƒì„±
schema_dictionary = create_schema_text_map(ROOT_DATASET_PATH)

if schema_dictionary and len(schema_dictionary) > 0:
    # 2. Training ë°ì´í„° ë¡œë“œ
    train_dataset = load_labels_from_path(TRAIN_DATA_PATH, schema_dictionary)
    # 3. Validation ë°ì´í„° ë¡œë“œ
    validation_dataset = load_labels_from_path(VALIDATION_DATA_PATH, schema_dictionary)

    if train_dataset and validation_dataset:
        # 4. DatasetDict ìƒì„±
        # 4. DatasetDict ìƒì„±
        split_dataset = DatasetDict({
            'train': train_dataset,
            'test': validation_dataset
        })

        print(f"âœ… í›ˆë ¨ ë°ì´í„°ì…‹ í¬ê¸°: {len(split_dataset['train'])}")
        print(f"âœ… ê²€ì¦ ë°ì´í„°ì…‹ í¬ê¸°: {len(split_dataset['test'])}")
        print("âœ… ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ!")
    else:
        raise Exception("ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨")
else:
    raise Exception("ìŠ¤í‚¤ë§ˆ ë¡œë“œ ì‹¤íŒ¨")

# ë³¸ê²©ì ì¸ í›ˆë ¨ ì„¤ì •
from transformers import Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, Seq2SeqTrainer


DRIVE_OUTPUT_DIR = "/content/ko-t5-nl2sql-stage1-results"

# í›ˆë ¨ ì„¤ì •
training_args = Seq2SeqTrainingArguments(
    output_dir=DRIVE_OUTPUT_DIR,

    learning_rate=5e-5,

    per_device_train_batch_size=16,
    per_device_eval_batch_size=8,
    gradient_accumulation_steps=2,

    max_grad_norm=1.0,
    warmup_steps=200,
    weight_decay=0.01,

    num_train_epochs=3,

    eval_strategy="steps",
    eval_steps=2000,
    save_strategy="steps",
    save_steps=4000,
    save_total_limit=1,

    logging_steps=500,
    logging_first_step=True,

    fp16=True,
    dataloader_drop_last=True,
    dataloader_num_workers=2,

    load_best_model_at_end=False,

    seed=42,
    push_to_hub=False,
)

# ë°ì´í„° ì½œë ˆì´í„°
data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=model,
    label_pad_token_id=-100,
    return_tensors="pt"
)

# ì „ì²´ ë°ì´í„°ì…‹ í† í°í™”
print("ë°ì´í„°ì…‹ í† í°í™” ì¤‘...")
full_tokenized_datasets = split_dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=["input_text", "target_text"],
    desc="Tokenizing datasets"
)

print(f"ì „ì²´ ë°ì´í„°ì…‹: í›ˆë ¨ {len(full_tokenized_datasets['train'])}ê°œ, í…ŒìŠ¤íŠ¸ {len(full_tokenized_datasets['test'])}ê°œ")

# Trainer ìƒì„±
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=full_tokenized_datasets["train"],
    eval_dataset=full_tokenized_datasets["test"],
    data_collator=data_collator,
    tokenizer=tokenizer
)

print("ë³¸ê²©ì ì¸ í›ˆë ¨ ì¤€ë¹„ ì™„ë£Œ!")
print("trainer.train() ì‹¤í–‰í•˜ë©´ ë³¸ê²©ì ì¸ í›ˆë ¨ì´ ì‹œì‘ë©ë‹ˆë‹¤.")

# ----------------------------------------------------------------------
# 6. í›ˆë ¨ê¸°(Trainer) ì´ˆê¸°í™” ë° í›ˆë ¨ ì‹œì‘
# ----------------------------------------------------------------------
print("\n[4/7] Seq2Seq í›ˆë ¨ê¸°(Trainer) ì´ˆê¸°í™”...")
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=full_tokenized_datasets["train"],
    eval_dataset=full_tokenized_datasets["test"],
    data_collator=data_collator,
    tokenizer=tokenizer
)

print("\n[5/7] Fine-tuningì„ ì‹œì‘")
trainer.train()

print(" í›ˆë ¨ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

"""#2ì°¨ fine tuning
#ìµœì¢… ëª¨ë¸ í•™ìŠµ


"""

# ----------------------------------------------------------------------
# combe4259/SQLNL ëª¨ë¸ ì‚¬ìš©
# ----------------------------------------------------------------------
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

BASE_MODEL_NAME = "combe4259/SQLNL"
NEW_MODEL_REPO_ID = "combe4259/SQLNL-NH-MVP"

print(f"ëª¨ë¸ ë¡œë“œ ì‹œì‘: {BASE_MODEL_NAME}")

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)
model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_NAME)

print("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
print(f"í† í¬ë‚˜ì´ì € vocab: {tokenizer.vocab_size}")
print(f"ëª¨ë¸ vocab: {model.config.vocab_size}")
print(f"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")

# ëª¨ë¸ì„ í›ˆë ¨ ëª¨ë“œë¡œ ì„¤ì •
model.train()
print(f"í›ˆë ¨ ëª¨ë“œ í™œì„±í™”: {model.training}")

# Vocab í¬ê¸° í™•ì¸ ë° ì¡°ì • (í•„ìš”ì‹œ)
if tokenizer.vocab_size != model.config.vocab_size:
    print("Vocab í¬ê¸° ë¶ˆì¼ì¹˜ - ì¡°ì • ì¤‘...")
    model.resize_token_embeddings(tokenizer.vocab_size)
    print(f"ì¡°ì • ì™„ë£Œ: {model.config.vocab_size}")
else:
    print("Vocab í¬ê¸° ì¼ì¹˜ - ì¡°ì • ë¶ˆí•„ìš”")

# ê¸°ë³¸ í…ŒìŠ¤íŠ¸
print("\nê¸°ë³¸ ë™ì‘ í…ŒìŠ¤íŠ¸:")
test_input = "ë‚´ ì˜ˆê¸ˆ ìƒë‹´ ë‚´ì—­ ë³´ì—¬ì¤˜"  # NH-MVPì— ë§ëŠ” í…ŒìŠ¤íŠ¸ ì…ë ¥
inputs = tokenizer(test_input, return_tensors="pt")
print(f"í† í°í™” ì„±ê³µ: {inputs['input_ids'].shape}")

# GPU ì´ë™ ë° forward pass í…ŒìŠ¤íŠ¸
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
inputs = {k: v.to(device) for k, v in inputs.items()}

model.eval()
with torch.no_grad():
    try:
        outputs = model(**inputs)
        print(f"Forward pass ì„±ê³µ: logits shape = {outputs.logits.shape}")
        print(f"Logits ë²”ìœ„: {outputs.logits.min().item():.4f} ~ {outputs.logits.max().item():.4f}")
        print(f"Logitsì— NaN: {torch.isnan(outputs.logits).any()}")

        # ì‹¤ì œ SQL ìƒì„± í…ŒìŠ¤íŠ¸
        generated = model.generate(**inputs, max_length=200)
        result = tokenizer.decode(generated[0], skip_special_tokens=True)
        print(f"\nìƒì„±ëœ SQL: {result}")

    except Exception as e:
        print(f"Forward pass ì‹¤íŒ¨: {e}")

print(f"\n{BASE_MODEL_NAME} ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")
print("ì´ì œ NH-MVP ë°ì´í„°ë¡œ Fine-tuningì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")

# ----------------------------------------------------------------------
# 3.5 Google Drive ë§ˆìš´íŠ¸
# ----------------------------------------------------------------------
from google.colab import drive
import os

print("Google Drive ë§ˆìš´íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
drive.mount('/content/drive')

DRIVE_SQL_PATH = "/content/drive/MyDrive/NHsqldataset"

# ----------------------------------------------------------------------
# 4. NH-MVP ë°ì´í„°ì…‹ ì¤€ë¹„ ë° ì „ì²˜ë¦¬ (AI Hub í˜•ì‹) - ëœë¤ ë¶„í•  ì ìš©
# ----------------------------------------------------------------------
import os
import json
import random
import numpy as np
from datasets import Dataset, DatasetDict

# NH-MVP ë°ì´í„° ê²½ë¡œ
ROOT_DATASET_PATH = "/content/drive/MyDrive/NHsqldataset"

def load_nh_mvp_dataset(root_path, seed=42):
    """
    NH-MVP ë°ì´í„°ì…‹ ë¡œë“œ (AI Hub í˜•ì‹) - ëœë¤ ë¶„í• 
    - nh_consultation_db_annotation.json: DB ìŠ¤í‚¤ë§ˆ ì •ë³´
    - TEXT_NL2SQL_label_nh_consultation.json: í•™ìŠµ ë°ì´í„°
    """
    print("\n[NH-MVP ë°ì´í„°ì…‹ ë¡œë“œ ì‹œì‘]")

    # 1. ìŠ¤í‚¤ë§ˆ íŒŒì¼ ë¡œë“œ
    schema_path = os.path.join(root_path, "nh_consultation_db_annotation.json")
    schema_map = {}

    if os.path.exists(schema_path):
        with open(schema_path, 'r', encoding='utf-8') as f:
            schema_data = json.load(f)

        # AI Hub í˜•ì‹ ìŠ¤í‚¤ë§ˆ íŒŒì‹±
        if 'data' in schema_data:
            for db in schema_data['data']:
                db_id = db.get('db_id', 'nh_consultation_db')

                # í…Œì´ë¸”ê³¼ ì»¬ëŸ¼ ì •ë³´ ì¶”ì¶œ
                table_names = db.get('table_names_original', [])
                column_names = db.get('column_names_original', [])

                schema_parts = []
                for i, table_name in enumerate(table_names):
                    # í•´ë‹¹ í…Œì´ë¸”ì˜ ì»¬ëŸ¼ë§Œ í•„í„°ë§
                    cols = [col[1] for col in column_names if col[0] == i]
                    schema_parts.append(f"{table_name}: {', '.join(cols)}")

                schema_map[db_id] = " | ".join(schema_parts)
                print(f" ìŠ¤í‚¤ë§ˆ ë¡œë“œ ì™„ë£Œ: {db_id}")
                print(f"   í…Œì´ë¸”: {', '.join(table_names)}")
    else:
        print(f" ìŠ¤í‚¤ë§ˆ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {schema_path}")
        return None, None

    # 2. ë¼ë²¨ ë°ì´í„° ë¡œë“œ
    label_path = os.path.join(root_path, "TEXT_NL2SQL_label_nh_consultation.json")

    if not os.path.exists(label_path):
        print(f" ë¼ë²¨ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {label_path}")
        return None, None

    with open(label_path, 'r', encoding='utf-8') as f:
        label_data = json.load(f)

    # 3. ë°ì´í„° ì²˜ë¦¬
    inputs = []
    outputs = []

    # AI Hub í˜•ì‹: data í‚¤ ì•ˆì— ë°°ì—´
    if 'data' in label_data:
        data_list = label_data['data']
    else:
        data_list = label_data if isinstance(label_data, list) else []

    print(f" ì´ {len(data_list)}ê°œì˜ í•™ìŠµ ë°ì´í„° ë°œê²¬")

    # 4. ì…ë ¥/ì¶œë ¥ ìŒ ìƒì„±
    for item in data_list:
        # db_idë¡œ ìŠ¤í‚¤ë§ˆ ì°¾ê¸°
        db_id = item.get('db_id', 'nh_consultation_db')

        if db_id not in schema_map:
            print(f" ìŠ¤í‚¤ë§ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {db_id}")
            # ê¸°ë³¸ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©
            if schema_map:
                db_id = list(schema_map.keys())[0]
            else:
                continue

        schema_text = schema_map[db_id]

        # utteranceì™€ query ì¶”ì¶œ
        utterance = item.get('utterance', '')
        sql_query = item.get('query', '')

        if not utterance or not sql_query:
            continue

        # ëª¨ë¸ ì…ë ¥ í˜•ì‹ ìƒì„±
        model_input = f"[SCHEMA: {schema_text}] [UTTERANCE: {utterance}]"
        inputs.append(model_input)
        outputs.append(sql_query)

    print(f"âœ… {len(inputs)}ê°œì˜ í•™ìŠµ ìŒ ìƒì„± ì™„ë£Œ")

    # 5. ìƒ˜í”Œ ì¶œë ¥
    if inputs:
        print("\n ë°ì´í„° ìƒ˜í”Œ (ì…”í”Œ ì „):")
        print(f"  ì§ˆë¬¸: {data_list[0].get('utterance', '')}")
        print(f"  SQL: {data_list[0].get('query', '')}")

    # 6. ëœë¤ ì…”í”Œ
    random.seed(seed)
    np.random.seed(seed)

    # ì¸ë±ìŠ¤ ìƒì„± ë° ì…”í”Œ
    indices = list(range(len(inputs)))
    random.shuffle(indices)

    # ì…”í”Œëœ ìˆœì„œë¡œ ë°ì´í„° ì¬ë°°ì—´
    inputs = [inputs[i] for i in indices]
    outputs = [outputs[i] for i in indices]


    # 7. í›ˆë ¨/ê²€ì¦ ë°ì´í„° ë¶„í•  (80:20)
    split_idx = int(len(inputs) * 0.8)

    train_inputs = inputs[:split_idx]
    train_outputs = outputs[:split_idx]
    val_inputs = inputs[split_idx:]
    val_outputs = outputs[split_idx:]

    print(f"\n ë°ì´í„° ë¶„í•  (ëœë¤):")
    print(f"  - í›ˆë ¨ ë°ì´í„°: {len(train_inputs)}ê°œ")
    print(f"  - ê²€ì¦ ë°ì´í„°: {len(val_inputs)}ê°œ")

    # Dataset ê°ì²´ ìƒì„±
    train_dataset = Dataset.from_dict({
        "input_text": train_inputs,
        "target_text": train_outputs
    })

    val_dataset = Dataset.from_dict({
        "input_text": val_inputs,
        "target_text": val_outputs
    })

    return train_dataset, val_dataset

# ì „ì²˜ë¦¬ í•¨ìˆ˜ (ê¸°ì¡´ê³¼ ë™ì¼)
def preprocess_function(examples):
    # ì…ë ¥ í† í°í™”
    model_inputs = tokenizer(
        examples['input_text'],
        max_length=512,
        padding="max_length",
        truncation=True
    )

    # íƒ€ê²Ÿ í† í°í™”
    labels = tokenizer(
        examples['target_text'],
        max_length=256,
        padding="max_length",
        truncation=True
    )

    # ë¼ë²¨ ì²˜ë¦¬ (paddingì„ -100ìœ¼ë¡œ)
    model_inputs["labels"] = [
        [(label if label != tokenizer.pad_token_id else -100) for label in label_seq]
        for label_seq in labels["input_ids"]
    ]

    return model_inputs

# ë©”ì¸ ì‹¤í–‰
print("\n[2/7] NH-MVP ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹œì‘...")
print(f" ë°ì´í„° ê²½ë¡œ: {ROOT_DATASET_PATH}")

# íŒŒì¼ ì¡´ì¬ í™•ì¸
files_to_check = [
    "nh_consultation_db_annotation.json",
    "TEXT_NL2SQL_label_nh_consultation.json"
]

print("\nğŸ“‹ íŒŒì¼ í™•ì¸:")
for filename in files_to_check:
    file_path = os.path.join(ROOT_DATASET_PATH, filename)
    if os.path.exists(file_path):
        file_size = os.path.getsize(file_path) / 1024  # KB
        print(f"   {filename} ({file_size:.1f} KB)")
    else:
        print(f"   {filename} (ì—†ìŒ)")

# ë°ì´í„° ë¡œë“œ (ëœë¤ ë¶„í• )
train_dataset, validation_dataset = load_nh_mvp_dataset(ROOT_DATASET_PATH, seed=42)

if train_dataset and validation_dataset:
    # DatasetDict ìƒì„±
    split_dataset = DatasetDict({
        'train': train_dataset,
        'test': validation_dataset
    })

    print("\n" + "="*50)
    print(" NH-MVP ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ")
    print(f" í›ˆë ¨ ë°ì´í„°: {len(split_dataset['train'])}ê°œ")
    print(f" ê²€ì¦ ë°ì´í„°: {len(split_dataset['test'])}ê°œ")
    print("="*50)

else:
    raise Exception("NH-MVP ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨. íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

# ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ í›ˆë ¨ ì„¤ì • - Extended Training
from transformers import Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, Seq2SeqTrainer
from transformers import EarlyStoppingCallback

print("Extended Training ì„¤ì • (50 epochs)...")

DRIVE_OUTPUT_DIR = "/content/NH-SQL-finetuned"

# í›ˆë ¨ ì„¤ì • - 50 epochs
training_args = Seq2SeqTrainingArguments(
    output_dir=DRIVE_OUTPUT_DIR,

    # í•™ìŠµë¥ 
    learning_rate=5e-6,

    # ë°°ì¹˜ í¬ê¸°
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    gradient_accumulation_steps=2,

    # Gradient clipping
    max_grad_norm=0.5,

    # Weight decay (L2 ê·œì œ)
    weight_decay=0.1,

    # Epoch ìˆ˜ ì¦ê°€
    num_train_epochs=50,

    # í‰ê°€ ì „ëµ
    eval_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=5,

    # ë¡œê¹…
    logging_steps=20,
    logging_first_step=True,
    report_to="tensorboard",  # TensorBoard ì¶”ê°€

    # Mixed precision
    fp16=True,

    # Best model ì„ íƒ
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,

    # Warmup
    warmup_ratio=0.1,  # 0.2 â†’ 0.1

    # Learning rate scheduler
    lr_scheduler_type="cosine",  # cosine annealing ì¶”ê°€

    # ê¸°íƒ€
    seed=42,
    push_to_hub=False,

    # ì¶”ê°€ ìµœì í™”
    gradient_checkpointing=True,  # ë©”ëª¨ë¦¬ ì ˆì•½
    optim="adamw_torch",
)

# Dropout ì„¤ì • (ê³¼ì í•© ë°©ì§€)
if hasattr(model, 'config'):
    model.config.dropout = 0.3
    model.config.attention_dropout = 0.3
    print("Dropout 0.3 ì ìš©")

# ë°ì´í„° ì½œë ˆì´í„°
data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=model,
    label_pad_token_id=-100,
    return_tensors="pt",
    padding=True,
    max_length=512
)

# í† í°í™”
print("ë°ì´í„°ì…‹ í† í°í™” ì¤‘...")
full_tokenized_datasets = split_dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=["input_text", "target_text"],
    desc="Tokenizing datasets"
)

# Early Stopping - ë” ê´€ëŒ€í•˜ê²Œ
early_stopping = EarlyStoppingCallback(
    early_stopping_patience=10,
    early_stopping_threshold=0.001

# ì»¤ìŠ¤í…€ ì½œë°± - ë§¤ 10 epochë§ˆë‹¤ ìƒíƒœ ì¶œë ¥
from transformers import TrainerCallback

class ProgressCallback(TrainerCallback):
    def on_epoch_end(self, args, state, control, **kwargs):
        epoch = int(state.epoch)
        if epoch % 10 == 0:  # ë§¤ 10 epochë§ˆë‹¤
            print(f"\nğŸ“Š === Epoch {epoch}/50 ì™„ë£Œ ===")
            if state.log_history:
                recent_logs = [log for log in state.log_history if 'loss' in log or 'eval_loss' in log]
                if recent_logs:
                    last_train = next((log['loss'] for log in reversed(recent_logs) if 'loss' in log), None)
                    last_eval = next((log['eval_loss'] for log in reversed(recent_logs) if 'eval_loss' in log), None)

                    if last_train and last_eval:
                        gap = last_eval - last_train
                        print(f"   Train Loss: {last_train:.4f}")
                        print(f"   Val Loss: {last_eval:.4f}")
                        print(f"   Gap: {gap:.4f}")

                        if gap > 1.5:
                            print("   âš ï¸ ê³¼ì í•© ì§•í›„ ê°ì§€")

# Trainer ìƒì„±
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=full_tokenized_datasets["train"],
    eval_dataset=full_tokenized_datasets["test"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    callbacks=[early_stopping, ProgressCallback()]
)

print(f"\n Extended Training ì„¤ì • ì™„ë£Œ")
print(f" í›ˆë ¨ ì„¤ì •:")
print(f"  - Epochs: 50")
print(f"  - í›ˆë ¨ ë°ì´í„°: {len(full_tokenized_datasets['train'])}ê°œ")
print(f"  - ê²€ì¦ ë°ì´í„°: {len(full_tokenized_datasets['test'])}ê°œ")
print(f"  - Early Stopping: patience=10")
print(f"  - LR Scheduler: Cosine Annealing")
print(f"  - Dropout: 0.3")

# ì´ˆê¸° í‰ê°€
initial_eval = trainer.evaluate()
print(f"\nì´ˆê¸° ê²€ì¦ ì†ì‹¤: {initial_eval['eval_loss']:.4f}")

print("\n trainer.train()ìœ¼ë¡œ 50 epoch í›ˆë ¨ ì‹œì‘")
print("   ì˜ˆìƒ ì‹œê°„: 1-2ì‹œê°„ (GPU ê¸°ì¤€)")

# ----------------------------------------------------------------------
# 6. í›ˆë ¨ê¸°(Trainer) ì´ˆê¸°í™” ë° í›ˆë ¨ ì‹œì‘
# ----------------------------------------------------------------------
print("\n[4/7] Seq2Seq í›ˆë ¨ê¸°(Trainer) ì´ˆê¸°í™”...")
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=full_tokenized_datasets["train"],
    eval_dataset=full_tokenized_datasets["test"],
    data_collator=data_collator,
    tokenizer=tokenizer
)

print("\n[5/7]  2ë‹¨ê³„ Fine-tuningì„ ì‹œì‘")
trainer.train()

print("2ë‹¨ê³„ í›ˆë ¨ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")