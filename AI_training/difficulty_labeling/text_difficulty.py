# -*- coding: utf-8 -*-
"""text_difficulty.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/112GWo0LrRls5B_uF6ghjZXzY6PxXzrnV
"""

!pip install transformers datasets accelerate scikit-learn

"""
KLUE-BERT Fine-tuning for Text Difficulty Classification
금융 문서 난이도 분류를 위한 KLUE-BERT 파인튜닝
"""

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback
)
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight
import pandas as pd
import numpy as np
import json
import os
import glob
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

class TextDifficultyDataset(Dataset):
    """텍스트 난이도 데이터셋"""

    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]

        # 토크나이징
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

def load_data(json_path):
    """JSON 데이터 로드"""
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    texts = []
    labels = []

    for item in data:
        texts.append(item['text'])
        # label이 0-9면 그대로, difficulty가 1-10이면 -1
        if 'label' in item:
            labels.append(item['label'])  # 0-indexed (0-9)
        else:
            labels.append(item['difficulty'] - 1)  # 1-10 → 0-9

    return texts, labels

def compute_metrics(eval_pred):
    """평가 메트릭 계산"""
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)

    # 정확도
    accuracy = accuracy_score(labels, predictions)

    # Precision, Recall, F1
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, predictions, average='weighted', zero_division=0
    )

    # MAE (Mean Absolute Error) - 난이도 차이
    mae = np.mean(np.abs(predictions - labels))

    # ±1 범위 내 정확도 (난이도 1 차이는 허용)
    within_1 = np.mean(np.abs(predictions - labels) <= 1)

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'mae': mae,
        'within_1_acc': within_1  # 추가: 1단계 차이 내 정확도
    }

def plot_confusion_matrix(y_true, y_pred, save_path=None):
    """혼동 행렬 시각화"""
    cm = confusion_matrix(y_true, y_pred)

    plt.figure(figsize=(12, 10))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('Confusion Matrix - Text Difficulty Classification')
    plt.xlabel('Predicted Difficulty')
    plt.ylabel('True Difficulty')

    # 레이블 설정 (1-10)
    tick_labels = [str(i+1) for i in range(10)]
    plt.xticks(np.arange(10) + 0.5, tick_labels)
    plt.yticks(np.arange(10) + 0.5, tick_labels)

    if save_path:
        plt.savefig(save_path, dpi=100, bbox_inches='tight')
    plt.show()

def main():
    """메인 실행 함수"""

    # ===== Colab 환경 확인 =====
    try:
        from google.colab import drive
        drive.mount('/content/drive')
        print(" Google Drive 마운트 완료")
        is_colab = True
    except:
        print(" 로컬 환경에서 실행 중...")
        is_colab = False

    # ===== 설정 =====
    MODEL_NAME = "klue/bert-base"

    # 파일 경로 설정 (Colab vs 로컬)
    if is_colab:
        # Colab에서 Google Drive 경로
        BASE_DIR = "/content/drive/MyDrive"
        JSON_PATH = f"{BASE_DIR}/text_difficulty_labels/training_data_20250910_092856.json"
        OUTPUT_DIR = f"{BASE_DIR}/klue_bert_difficulty_model"

        # JSON 파일이 없으면 다른 경로 시도
        if not os.path.exists(JSON_PATH):
            # 가장 최근 JSON 파일 찾기
            import glob
            json_files = glob.glob(f"{BASE_DIR}/text_difficulty_labels/training_data_*.json")
            if json_files:
                JSON_PATH = sorted(json_files)[-1]  # 가장 최근 파일
                print(f" JSON 파일 사용: {JSON_PATH.split('/')[-1]}")
            else:
                print(" JSON 파일을 찾을 수 없습니다!")
                print(f"   경로: {BASE_DIR}/text_difficulty_labels/")
                return None, None
    else:
        # 로컬 경로
        JSON_PATH = "/Users/inter4259/Downloads/training_data_20250910_092856.json"
        OUTPUT_DIR = "./klue_bert_difficulty_model"

    # 출력 디렉토리 생성
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    print(f"📁 출력 디렉토리: {OUTPUT_DIR}")

    # 하이퍼파라미터
    MAX_LENGTH = 512
    BATCH_SIZE = 16  # GPU 메모리에 따라 조정
    LEARNING_RATE = 2e-5
    NUM_EPOCHS = 10
    WARMUP_STEPS = 500

    # GPU 설정
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f" 사용 디바이스: {device}")
    if device.type == 'cuda':
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        print(f"   메모리: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")

    # ===== 데이터 로드 =====
    print("📚 데이터 로딩...")
    texts, labels = load_data(JSON_PATH)
    print(f"총 샘플 수: {len(texts)}")

    # 레이블 분포 확인
    unique_labels, counts = np.unique(labels, return_counts=True)
    print("\n레이블 분포:")
    for label, count in zip(unique_labels, counts):
        print(f"  난이도 {label+1}: {count}개 ({count/len(labels)*100:.1f}%)")

    # Train/Validation/Test 분할 (70/15/15)
    # 난이도 10이 1개뿐이므로 stratify 제거 또는 조건부 사용
    try:
        X_train, X_temp, y_train, y_temp = train_test_split(
            texts, labels, test_size=0.3, random_state=42, stratify=labels
        )
        X_val, X_test, y_val, y_test = train_test_split(
            X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
        )
    except ValueError as e:
        print(f" Stratified split 실패: {e}")
        print("   일반 random split으로 전환...")
        X_train, X_temp, y_train, y_temp = train_test_split(
            texts, labels, test_size=0.3, random_state=42
        )
        X_val, X_test, y_val, y_test = train_test_split(
            X_temp, y_temp, test_size=0.5, random_state=42
        )

    print(f"\n데이터 분할:")
    print(f"  Train: {len(X_train)}")
    print(f"  Validation: {len(X_val)}")
    print(f"  Test: {len(X_test)}")

    # ===== 토크나이저 & 모델 로드 =====
    print("\n 모델 로딩...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

    # 10개 클래스 (난이도 1-10)
    model = AutoModelForSequenceClassification.from_pretrained(
        MODEL_NAME,
        num_labels=10,
        problem_type="single_label_classification"
    )
    model.to(device)

    # ===== 데이터셋 생성 =====
    train_dataset = TextDifficultyDataset(X_train, y_train, tokenizer, MAX_LENGTH)
    val_dataset = TextDifficultyDataset(X_val, y_val, tokenizer, MAX_LENGTH)
    test_dataset = TextDifficultyDataset(X_test, y_test, tokenizer, MAX_LENGTH)

    # ===== 학습 설정 =====
    # transformers 버전 호환성을 위한 설정
    try:
        # 새 버전 (4.20+)
        training_args = TrainingArguments(
            output_dir=OUTPUT_DIR,
            num_train_epochs=NUM_EPOCHS,
            per_device_train_batch_size=BATCH_SIZE,
            per_device_eval_batch_size=BATCH_SIZE,
            warmup_steps=WARMUP_STEPS,
            learning_rate=LEARNING_RATE,
            logging_dir='./logs',
            logging_steps=50,
            eval_strategy="steps",  # 새 버전
            eval_steps=100,
            save_strategy="steps",
            save_steps=500,
            load_best_model_at_end=True,
            metric_for_best_model="f1",
            greater_is_better=True,
            save_total_limit=3,
            fp16=torch.cuda.is_available(),
            dataloader_num_workers=2,
            remove_unused_columns=False,
        )
    except TypeError:
        # 구 버전 (4.19 이하)
        print("⚠️ 구버전 transformers 감지, 호환 모드 사용")
        training_args = TrainingArguments(
            output_dir=OUTPUT_DIR,
            num_train_epochs=NUM_EPOCHS,
            per_device_train_batch_size=BATCH_SIZE,
            per_device_eval_batch_size=BATCH_SIZE,
            warmup_steps=WARMUP_STEPS,
            learning_rate=LEARNING_RATE,
            logging_dir='./logs',
            logging_steps=50,
            evaluation_strategy="steps",  # 구 버전
            eval_steps=100,
            save_strategy="steps",
            save_steps=500,
            load_best_model_at_end=True,
            metric_for_best_model="f1",
            greater_is_better=True,
            save_total_limit=3,
            fp16=torch.cuda.is_available(),
            dataloader_num_workers=2,
            remove_unused_columns=False,
        )

    # ===== 클래스 가중치 계산 (불균형 해결) =====
    label_counts = Counter(y_train)
    print(f"\n학습 데이터 클래스 분포:")
    for label in range(10):
        count = label_counts.get(label, 0)
        if count > 0:
            print(f"  난이도 {label+1}: {count}개 ({count/len(y_train)*100:.1f}%)")
        else:
            print(f"  난이도 {label+1}: 0개 ")

    # 학습 데이터에 있는 클래스만으로 가중치 계산
    unique_train_labels = np.unique(y_train)
    if len(unique_train_labels) < 10:
        print(f"   존재하는 클래스: {[l+1 for l in unique_train_labels]}")

        # 난이도 10이 없을 가능성이 높으므로, 9와 병합 제안
        if 9 not in unique_train_labels:  # label 9 = difficulty 10
            print("\n 제안: 난이도 10 샘플이 너무 적음.")
            print("   1. 더 많은 데이터 수집")
            print("   2. 난이도 9와 10을 하나로 병합")
            print("   3. 난이도 10 샘플을 복제(oversampling)")

    # 클래스 가중치 계산
    class_weights = None
    try:
        # 모든 클래스(0-9)에 대한 가중치 계산
        # 없는 클래스는 1.0으로 설정
        weights = np.ones(10)
        if len(unique_train_labels) > 0:
            calculated_weights = compute_class_weight(
                'balanced',
                classes=unique_train_labels,
                y=y_train
            )
            for i, label in enumerate(unique_train_labels):
                weights[label] = calculated_weights[i]

        # 극단적인 가중치 제한 (최대 10배까지만)
        max_weight = 10.0
        weights = np.clip(weights, 0.1, max_weight)

        class_weights = torch.tensor(weights, dtype=torch.float)
        print(f"\n클래스 가중치 적용 (최대 {max_weight}배 제한):")
        for i in range(10):
            if label_counts.get(i, 0) > 0:
                print(f"  난이도 {i+1}: {weights[i]:.2f} (샘플: {label_counts.get(i, 0)}개)")
    except Exception as e:
        print(f"\n 클래스 가중치 계산 실패: {e}")
        print("   기본 가중치(1.0) 사용")

    class WeightedTrainer(Trainer):
        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
            labels = inputs.pop("labels")
            outputs = model(**inputs)
            logits = outputs.get('logits')

            # 가중치 적용 손실 함수
            if class_weights is not None:
                loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights.to(logits.device))
            else:
                loss_fct = torch.nn.CrossEntropyLoss()

            loss = loss_fct(logits.view(-1, 10), labels.view(-1))
            return (loss, outputs) if return_outputs else loss

    # ===== Trainer 생성 =====
    trainer = WeightedTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
    )

    # ===== 학습 =====
    print("\n 학습 시작...")
    trainer.train()

    # ===== 평가 =====
    print("\n 테스트셋 평가...")
    test_results = trainer.evaluate(test_dataset)

    print("\n테스트 결과:")
    for key, value in test_results.items():
        if key.startswith('eval_'):
            metric_name = key.replace('eval_', '')
            print(f"  {metric_name}: {value:.4f}")

    # ===== 예측 & 혼동행렬 =====
    print("\n🔍 상세 분석...")
    predictions = trainer.predict(test_dataset)
    y_pred = np.argmax(predictions.predictions, axis=1)

    # 혼동 행렬 그리기
    plot_confusion_matrix(y_test, y_pred, save_path=f"{OUTPUT_DIR}/confusion_matrix.png")

    # ===== 모델 저장 =====
    print(f"\n💾 모델 저장: {OUTPUT_DIR}")
    trainer.save_model(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)

    # ===== 샘플 예측 =====
    print("\n 샘플 예측:")
    sample_texts = [
        "은행에 돈을 맡겨요",
        "예금자보호법에 따라 5천만원까지 보호됩니다",
        "신용파생결합증권의 CDS 스프레드 변동에 따른 수익구조"
    ]

    for text in sample_texts:
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=MAX_LENGTH)
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model(**inputs)
            prediction = torch.argmax(outputs.logits, dim=-1).item()

        print(f"  텍스트: {text[:50]}...")
        print(f"  예측 난이도: {prediction + 1}")

    # ===== 메타데이터 저장 =====
    metadata = {
        'model_name': MODEL_NAME,
        'num_classes': 10,
        'max_length': MAX_LENGTH,
        'training_samples': len(X_train),
        'validation_samples': len(X_val),
        'test_samples': len(X_test),
        'test_accuracy': test_results.get('eval_accuracy', 0),
        'test_f1': test_results.get('eval_f1', 0),
        'test_mae': test_results.get('eval_mae', 0),
        'test_within_1_acc': test_results.get('eval_within_1_acc', 0),
        'json_path': JSON_PATH,
        'output_dir': OUTPUT_DIR,
        'device': str(device),
        'is_colab': is_colab
    }

    metadata_path = os.path.join(OUTPUT_DIR, "training_metadata.json")
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    print(f"\n 메타데이터 저장: {metadata_path}")

    print("\n Fine-tuning 완료!")
    print(f" 모델 저장 위치: {OUTPUT_DIR}")

    if is_colab:
        print("\n💡 Colab에서 모델 사용하기:")
        print(f"   from transformers import AutoTokenizer, AutoModelForSequenceClassification")
        print(f"   model = AutoModelForSequenceClassification.from_pretrained('{OUTPUT_DIR}')")
        print(f"   tokenizer = AutoTokenizer.from_pretrained('{OUTPUT_DIR}')")

    return model, tokenizer

# ===== 추론용 함수 =====
def predict_difficulty(text, model, tokenizer, device='cpu'):
    """단일 텍스트 난이도 예측"""
    model.eval()
    model.to(device)

    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=512,
        padding=True
    )
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probabilities = torch.softmax(logits, dim=-1)
        prediction = torch.argmax(logits, dim=-1).item()

    return {
        'difficulty': prediction + 1,  # 1-10 스케일로 변환
        'confidence': probabilities[0][prediction].item(),
        'probabilities': probabilities[0].cpu().numpy()
    }

# ===== 배치 추론 =====
def predict_batch(texts, model, tokenizer, batch_size=32, device='cpu'):
    """여러 텍스트 배치 처리"""
    model.eval()
    model.to(device)

    results = []

    for i in tqdm(range(0, len(texts), batch_size), desc="배치 추론"):
        batch_texts = texts[i:i+batch_size]

        inputs = tokenizer(
            batch_texts,
            return_tensors="pt",
            truncation=True,
            max_length=512,
            padding=True
        )
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
            predictions = torch.argmax(logits, dim=-1)

            for j, pred in enumerate(predictions):
                results.append({
                    'text': batch_texts[j],
                    'difficulty': pred.item() + 1
                })

    return results

if __name__ == "__main__":
    model, tokenizer = main()