"""
í…ìŠ¤íŠ¸ ë‚œì´ë„ + ìœ„í—˜ë„ ìë™ ë¼ë²¨ë§ í”„ë¡œê·¸ë¨ (ë“€ì–¼ ë¼ë²¨ëŸ¬)
Google Colabì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”.

ì‚¬ìš©ë²•:
1. Google Colabì—ì„œ ì´ íŒŒì¼ ì—…ë¡œë“œ
2. GPU ëŸ°íƒ€ì„ ì„¤ì •
3. ì‹¤í–‰: !python text_difficulty_labeler.py

ì¶œë ¥: ë‚œì´ë„(1-10) + ìœ„í—˜ë„(1-10) ë™ì‹œ í‰ê°€
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from huggingface_hub import login
import pandas as pd
from tqdm import tqdm
import re
import time
import os
import json
import matplotlib.pyplot as plt
import numpy as np
from datetime import datetime

# PDF ì²˜ë¦¬ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import pdfplumber
    PDF_SUPPORT = True
    # ê°œì„ ëœ ì¶”ì¶œê¸°ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
    try:
        from improved_pdf_extractor import ImprovedPDFExtractor
        IMPROVED_EXTRACTOR = True
    except ImportError:
        IMPROVED_EXTRACTOR = False
except ImportError:
    PDF_SUPPORT = False
    IMPROVED_EXTRACTOR = False
    print("âš ï¸ PDF ì§€ì›ì„ ìœ„í•´ ì„¤ì¹˜ í•„ìš”: pip install pdfplumber")

class TextDifficultyLabeler:
    def __init__(self, model_name="google/gemma-2-2b-it", hf_token=None):
        """
        í…ìŠ¤íŠ¸ ë‚œì´ë„ ë¼ë²¨ëŸ¬ ì´ˆê¸°í™”

        Args:
            model_name: HuggingFace ëª¨ë¸ëª…
            hf_token: HuggingFace í† í° (gated ëª¨ë¸ìš©)
        """
        self.model_name = model_name

        # HuggingFace ë¡œê·¸ì¸ (í•„ìš”ì‹œ)
        if hf_token:
            login(token=hf_token)
            print("âœ… HuggingFace ë¡œê·¸ì¸ ì™„ë£Œ")

        # ëª¨ë¸ ë¡œë“œ
        print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        self.load_model()
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

        # ê²°ê³¼ ì €ì¥ìš©
        self.results = []

    def load_model(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        try:
            # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            if torch.cuda.is_available():
                print(f"ğŸ® GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
                # 4bit ì–‘ìí™” ì„¤ì • (ìƒˆë¡œìš´ ë°©ì‹)
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    quantization_config=quantization_config,
                    device_map="auto",
                    dtype=torch.float16,  # torch_dtype ëŒ€ì‹  dtype ì‚¬ìš©
                )
            else:
                print("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰")
                # CPUì—ì„œ ì‹¤í–‰
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    device_map="cpu",
                    dtype=torch.float32,  # CPUëŠ” float32 ì‚¬ìš©
                )
        except Exception as e:
            print(f"âš ï¸ 4bit ë¡œë”© ì‹¤íŒ¨, ì¼ë°˜ ëª¨ë“œë¡œ ì¬ì‹œë„: {e}")
            # 4bit ì‹¤íŒ¨ì‹œ ì¼ë°˜ ë¡œë“œ
            if torch.cuda.is_available():
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    device_map="auto",
                    dtype=torch.float16,
                )
            else:
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    dtype=torch.float32,
                ).to('cpu')

        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)

        # ë””ë°”ì´ìŠ¤ í™•ì¸
        self.device = next(self.model.parameters()).device
        print(f"âœ… ëª¨ë¸ ë””ë°”ì´ìŠ¤: {self.device}")

        # íŒ¨ë”© í† í° ì„¤ì •
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

    def create_prompt(self, text):
        """ê¸ˆìœµ ë¬¸ì„œ íŠ¹í™” ë‚œì´ë„ + ìœ„í—˜ë„ ë“€ì–¼ í‰ê°€ í”„ë¡¬í”„íŠ¸ (Gemma-2 instruction í…œí”Œë¦¿)"""

        # Gemma-2ëŠ” <start_of_turn> íƒœê·¸ ì‚¬ìš©
        prompt = f"""<start_of_turn>user
ê¸ˆìœµ í…ìŠ¤íŠ¸ë¥¼ ë‚œì´ë„(1-10)ì™€ ìœ„í—˜ë„(1-10)ë¡œ í‰ê°€í•˜ì„¸ìš”.

í…ìŠ¤íŠ¸: "{text[:300]}"

ë°˜ë“œì‹œ ì´ í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•˜ì„¸ìš”:
ë‚œì´ë„: [ìˆ«ì]
ìœ„í—˜ë„: [ìˆ«ì]<end_of_turn>
<start_of_turn>model
"""

        return prompt

    def get_dual_labels(self, text):
        """í…ìŠ¤íŠ¸ ë‚œì´ë„ + ìœ„í—˜ë„ ë™ì‹œ í‰ê°€"""

        # ì—¬ëŸ¬ ë²ˆ ì‹œë„í•´ì„œ ì•ˆì •ì ì¸ ê²°ê³¼ ì–»ê¸°
        difficulty_attempts = []
        risk_attempts = []

        for i in range(3):  # 3ë²ˆ ì‹œë„
            difficulty, risk = self._single_evaluation(text, attempt_num=i)
            if difficulty != -1 and risk != -1:  # ìœ íš¨í•œ ì‘ë‹µ
                difficulty_attempts.append(difficulty)
                risk_attempts.append(risk)

        # ê²°ê³¼ ì²˜ë¦¬
        if not difficulty_attempts or not risk_attempts:
            print(f"[WARNING] ëª¨ë“  ì‹œë„ ì‹¤íŒ¨, í…ìŠ¤íŠ¸: {text[:50]}...")
            return 5, 5  # ì™„ì „ ì‹¤íŒ¨ì‹œ ì¤‘ê°„ê°’

        # ì¤‘ì•™ê°’ ë°˜í™˜ (outlier ì œê±°)
        if len(difficulty_attempts) >= 2:
            difficulty_attempts.sort()
            risk_attempts.sort()
            difficulty = difficulty_attempts[len(difficulty_attempts)//2]
            risk = risk_attempts[len(risk_attempts)//2]
        else:
            difficulty = difficulty_attempts[0]
            risk = risk_attempts[0]

        return difficulty, risk

    def _single_evaluation(self, text, attempt_num=0):
        """ë‹¨ì¼ í‰ê°€ ì‹œë„"""
        prompt = self.create_prompt(text)

        try:
            # í† í¬ë‚˜ì´ì§• (í”„ë¡¬í”„íŠ¸ê°€ ê¸¸ì–´ì¡Œìœ¼ë¯€ë¡œ max_length ì¦ê°€)
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=1024  # 768 â†’ 1024ë¡œ ì¦ê°€
            )

            # ëª¨ë¸ê³¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            if hasattr(self, 'device'):
                inputs = inputs.to(self.device)
            elif torch.cuda.is_available() and next(self.model.parameters()).is_cuda:
                inputs = inputs.to('cuda')
            else:
                inputs = inputs.to('cpu')

            # ìƒì„± - ì‹œë„ë§ˆë‹¤ ì•½ê°„ ë‹¤ë¥¸ ì„¤ì •
            temp_values = [0.1, 0.3, 0.5]
            temperature = temp_values[attempt_num % 3]

            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=50,       # 50ìœ¼ë¡œ ë„‰ë„‰í•˜ê²Œ ì¦ê°€
                    temperature=temperature,
                    do_sample=(temperature > 0.1),
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    top_k=50,                # 10 â†’ 50ìœ¼ë¡œ ì¦ê°€ (ë‹¤ì–‘ì„±)
                    top_p=0.95,              # 0.9 â†’ 0.95ë¡œ ì¦ê°€
                    repetition_penalty=1.1   # ë°˜ë³µ ë°©ì§€
                )

            # ë””ì½”ë”©
            response = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[-1]:],
                skip_special_tokens=True
            ).strip()

            # ë””ë²„ê¹… (í•„ìš”ì‹œ)
            if attempt_num == 0:  # ì²« ì‹œë„ë§Œ ì¶œë ¥
                print(f"[DEBUG] Response: '{response}' for: {text[:30]}...")
                # pass

            # ë“€ì–¼ íŒŒì‹± - ë‚œì´ë„ì™€ ìœ„í—˜ë„ ì¶”ì¶œ
            import re

            # "ë‚œì´ë„: X\nìœ„í—˜ë„: Y" í˜•ì‹ íŒŒì‹±
            match_difficulty = re.search(r'ë‚œì´ë„:\s*(\d+)', response)
            match_risk = re.search(r'ìœ„í—˜ë„:\s*(\d+)', response)

            if match_difficulty and match_risk:
                difficulty = int(match_difficulty.group(1))
                risk = int(match_risk.group(1))

                # 1-10 ë²”ìœ„ ì²´í¬
                difficulty = min(max(difficulty, 1), 10)
                risk = min(max(risk, 1), 10)

                return difficulty, risk

            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ëŒ€ì•ˆ ì‹œë„ (ìˆ«ì 2ê°œ ì°¾ê¸°)
            numbers = re.findall(r'\d+', response)
            if len(numbers) >= 2:
                difficulty = int(numbers[0])
                risk = int(numbers[1])

                # 1-10 ë²”ìœ„ ì²´í¬
                difficulty = min(max(difficulty, 1), 10)
                risk = min(max(risk, 1), 10)

                if attempt_num == 0:
                    print(f"[WARNING] í˜•ì‹ ë¶ˆì¼ì¹˜, ìˆ«ìë¡œ íŒŒì‹±: ë‚œì´ë„={difficulty}, ìœ„í—˜ë„={risk}")

                return difficulty, risk

            return -1, -1  # íŒŒì‹± ì‹¤íŒ¨

        except Exception as e:
            if attempt_num == 0:
                print(f"[ERROR] í‰ê°€ ì‹¤íŒ¨: {e}")
            return -1, -1

    def label_texts(self, texts, batch_save=10, checkpoint_path=None):
        """
        í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ ë¼ë²¨ë§

        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_save: Nê°œë§ˆë‹¤ ì¤‘ê°„ ì €ì¥
            checkpoint_path: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
        """
        # ì²´í¬í¬ì¸íŠ¸ í™•ì¸
        processed_texts = set()
        if checkpoint_path and os.path.exists(checkpoint_path):
            checkpoint_df = pd.read_csv(checkpoint_path)
            processed_texts = set(checkpoint_df['text'].tolist())
            self.results = checkpoint_df.to_dict('records')
            print(f"ğŸ“Œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {len(processed_texts)}ê°œ ì´ë¯¸ ì²˜ë¦¬ë¨")

        # ë¼ë²¨ë§ ì‹œì‘
        new_results = []

        for i, text in enumerate(tqdm(texts, desc="ë¼ë²¨ë§ ì§„í–‰")):
            # ì´ë¯¸ ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ëŠ” ìŠ¤í‚µ
            if text in processed_texts:
                continue

            try:
                # ë‚œì´ë„ + ìœ„í—˜ë„ ë™ì‹œ í‰ê°€
                difficulty, risk_level = self.get_dual_labels(text)

                result = {
                    'text': text,
                    'difficulty': difficulty,
                    'risk_level': risk_level,
                    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }

                new_results.append(result)
                self.results.append(result)

                # ë°°ì¹˜ ì €ì¥
                if checkpoint_path and len(new_results) % batch_save == 0:
                    self.save_checkpoint(new_results, checkpoint_path)
                    new_results = []

                # ì†ë„ ì¡°ì ˆ
                time.sleep(0.1)

            except Exception as e:
                print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
                print(f"   ë¬¸ì œ í…ìŠ¤íŠ¸: {text[:50]}...")
                continue

        # ë§ˆì§€ë§‰ ë°°ì¹˜ ì €ì¥
        if checkpoint_path and new_results:
            self.save_checkpoint(new_results, checkpoint_path)

        print(f"âœ… ë¼ë²¨ë§ ì™„ë£Œ: ì´ {len(self.results)}ê°œ")

        return pd.DataFrame(self.results)

    def save_checkpoint(self, new_results, checkpoint_path):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        df = pd.DataFrame(new_results)
        if os.path.exists(checkpoint_path):
            df.to_csv(checkpoint_path, mode='a', header=False, index=False)
        else:
            df.to_csv(checkpoint_path, index=False)
        print(f"  ğŸ’¾ {len(new_results)}ê°œ ì €ì¥ë¨")

    def save_results(self, output_dir='/content/drive/MyDrive'):
        """ê²°ê³¼ ì €ì¥ (CSV, Excel, JSON)"""
        df = pd.DataFrame(self.results)

        if df.empty:
            print("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # íƒ€ì„ìŠ¤íƒ¬í”„
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # CSV ì €ì¥
        csv_path = os.path.join(output_dir, f'labeled_data_{timestamp}.csv')
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"âœ… CSV ì €ì¥: {csv_path}")

        # Excel ì €ì¥
        excel_path = os.path.join(output_dir, f'labeled_data_{timestamp}.xlsx')
        df.to_excel(excel_path, index=False, engine='openpyxl')
        print(f"âœ… Excel ì €ì¥: {excel_path}")

        # JSON ì €ì¥ (Fine-tuningìš© - ë“€ì–¼ ë¼ë²¨)
        json_data = []
        for _, row in df.iterrows():
            json_data.append({
                "text": row['text'],
                "difficulty": int(row['difficulty']),  # 1-10
                "difficulty_name": self._get_difficulty_name(int(row['difficulty'])),
                "risk_level": int(row['risk_level']),  # 1-10
                "risk_name": self._get_risk_name(int(row['risk_level']))
            })

        json_path = os.path.join(output_dir, f'dual_training_data_{timestamp}.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… JSON ì €ì¥ (ë“€ì–¼ ë¼ë²¨): {json_path}")

        return csv_path, excel_path, json_path

    def _get_difficulty_name(self, level):
        """ë‚œì´ë„ ë ˆë²¨ì— ëŒ€í•œ ì´ë¦„ ë°˜í™˜"""
        names = {
            1: "ì•„ì£¼ ì‰¬ì›€",
            2: "ì‰¬ì›€",
            3: "ì•½ê°„ ì‰¬ì›€",
            4: "ë³´í†µ-ë‚®ìŒ",
            5: "ë³´í†µ",
            6: "ë³´í†µ-ë†’ìŒ",
            7: "ì–´ë ¤ì›€",
            8: "ë§¤ìš° ì–´ë ¤ì›€",
            9: "ì „ë¬¸ê°€-ìƒ",
            10: "ì „ë¬¸ê°€-ìµœìƒ"
        }
        return names.get(level, f"Level {level}")

    def _get_risk_name(self, level):
        """ìœ„í—˜ë„ ë ˆë²¨ì— ëŒ€í•œ ì´ë¦„ ë°˜í™˜"""
        names = {
            1: "ìœ„í—˜ ì—†ìŒ",
            2: "ê·¹íˆ ë‚®ì€ ìœ„í—˜",
            3: "ë‚®ì€ ìœ„í—˜",
            4: "ì•½ê°„ ë‚®ì€ ìœ„í—˜",
            5: "ë³´í†µ ìœ„í—˜",
            6: "ì•½ê°„ ë†’ì€ ìœ„í—˜",
            7: "ë†’ì€ ìœ„í—˜",
            8: "ë§¤ìš° ë†’ì€ ìœ„í—˜",
            9: "ì¹˜ëª…ì  ìœ„í—˜",
            10: "ê·¹ë„ë¡œ ì¹˜ëª…ì "
        }
        return names.get(level, f"Risk {level}")

    def visualize_results(self, save_path=None):
        """ê²°ê³¼ ì‹œê°í™”"""
        df = pd.DataFrame(self.results)

        if df.empty:
            print("âš ï¸ ì‹œê°í™”í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ê·¸ë˜í”„ ìƒì„±
        fig, axes = plt.subplots(1, 2, figsize=(15, 5))

        # 1. ë‚œì´ë„ ë¶„í¬
        difficulty_counts = df['difficulty'].value_counts().sort_index()
        colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.9, 10))  # ì´ˆë¡(ì‰¬ì›€)â†’ë¹¨ê°•(ì–´ë ¤ì›€)
        bar_colors = [colors[i-1] for i in difficulty_counts.index]

        axes[0].bar(difficulty_counts.index, difficulty_counts.values, color=bar_colors)
        axes[0].set_xlabel('ë‚œì´ë„ ë ˆë²¨')
        axes[0].set_ylabel('í…ìŠ¤íŠ¸ ê°œìˆ˜')
        axes[0].set_title('ë‚œì´ë„ ë¶„í¬')
        axes[0].set_xticks(range(1, 11))
        axes[0].grid(axis='y', alpha=0.3)

        # 2. ë‚œì´ë„ë³„ ë¹„ìœ¨
        # 10ë‹¨ê³„ ìƒ‰ìƒ ê·¸ë¼ë°ì´ì…˜
        pie_colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.9, len(difficulty_counts)))

        axes[1].pie(difficulty_counts.values,
                    labels=[f'Lv{i}' for i in difficulty_counts.index],
                    colors=pie_colors,
                    autopct='%1.1f%%',
                    startangle=90)
        axes[1].set_title('ë‚œì´ë„ë³„ ë¹„ìœ¨')

        plt.suptitle(f'í…ìŠ¤íŠ¸ ë‚œì´ë„ ë¶„ì„ ê²°ê³¼ (ì´ {len(df)}ê°œ)', fontsize=16)
        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=100, bbox_inches='tight')
            print(f"âœ… ê·¸ë˜í”„ ì €ì¥: {save_path}")

        plt.show()

    def print_summary(self):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        df = pd.DataFrame(self.results)

        if df.empty:
            print("âš ï¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        print("\n" + "="*50)
        print("ğŸ“Š ë¼ë²¨ë§ ê²°ê³¼ ìš”ì•½")
        print("="*50)

        print(f"ì´ í…ìŠ¤íŠ¸ ìˆ˜: {len(df)}ê°œ")
        print(f"\në‚œì´ë„ ë¶„í¬:")

        difficulty_names = {
            1: "ì•„ì£¼ ì‰¬ì›€ (ì´ˆë“± ì €í•™ë…„)",
            2: "ì‰¬ì›€ (ì´ˆë“± ê³ í•™ë…„)",
            3: "ì•½ê°„ ì‰¬ì›€ (ì´ˆÂ·ì¤‘ê°„)",
            4: "ë³´í†µ-ë‚®ìŒ (ì¤‘í•™ìƒ)",
            5: "ë³´í†µ (ì¤‘~ê³ êµ)",
            6: "ë³´í†µ-ë†’ìŒ (ë³µí•©ë¬¸ì¥)",
            7: "ì–´ë ¤ì›€ (ê³ ë“±í•™ìƒ)",
            8: "ë§¤ìš° ì–´ë ¤ì›€ (ëŒ€í•™ ì „ê³µ)",
            9: "ì „ë¬¸ê°€-ìƒ (ì „ë¬¸ìš©ì–´)",
            10: "ì „ë¬¸ê°€-ìµœìƒ (ë§¤ìš° ì „ë¬¸ì )"
        }

        for difficulty in range(1, 11):
            count = len(df[df['difficulty'] == difficulty])
            percentage = (count / len(df)) * 100 if len(df) > 0 else 0
            if count > 0:  # ì¡´ì¬í•˜ëŠ” ë ˆë²¨ë§Œ ì¶œë ¥
                print(f"  Level {difficulty:2d} - {difficulty_names[difficulty]}: {count}ê°œ ({percentage:.1f}%)")

        print(f"\ní‰ê·  ë‚œì´ë„: {df['difficulty'].mean():.2f}")
        print(f"ì¤‘ì•™ê°’: {df['difficulty'].median():.1f}")

        # ìƒ˜í”Œ ì¶œë ¥ (ì¡´ì¬í•˜ëŠ” ë ˆë²¨ë§Œ)
        print("\nğŸ“ ìƒ˜í”Œ í…ìŠ¤íŠ¸:")
        for difficulty in sorted(df['difficulty'].unique()):
            samples = df[df['difficulty'] == difficulty].head(1)
            if not samples.empty:
                text = samples.iloc[0]['text']
                preview = text[:80] + "..." if len(text) > 80 else text
                print(f"\nLevel {difficulty}: {preview}")


# ============ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ============

def extract_texts_from_pdf(pdf_path, split_mode='smart', use_improved=True):
    """
    PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê¸ˆìœµ/ë²•ë¥  ë¬¸ì„œì— ìµœì í™”)

    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        split_mode: í…ìŠ¤íŠ¸ ë¶„ë¦¬ ë°©ì‹
            - 'smart': ì§€ëŠ¥í˜• ë¶„ë¦¬ (ë²ˆí˜¸ í•­ëª© + ë¬¸ì¥ ë³µí•©)
            - 'sentence': ë¬¸ì¥ ë‹¨ìœ„ ë¶„ë¦¬
            - 'paragraph': ë‹¨ë½ ë‹¨ìœ„ ë¶„ë¦¬
            - 'bullet': ë²ˆí˜¸/ê¸°í˜¸ í•­ëª© ë‹¨ìœ„
            - 'page': í˜ì´ì§€ ë‹¨ìœ„
        use_improved: ê°œì„ ëœ ì¶”ì¶œê¸° ì‚¬ìš© ì—¬ë¶€

    Returns:
        í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    if not PDF_SUPPORT:
        print("âŒ pdfplumberê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   ì‹¤í–‰: pip install pdfplumber")
        return []

    # ê°œì„ ëœ ì¶”ì¶œê¸°ê°€ ìˆê³  ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •ëœ ê²½ìš°
    if use_improved and IMPROVED_EXTRACTOR and split_mode == 'smart':
        print("ğŸš€ ê°œì„ ëœ PDF ì¶”ì¶œê¸° ì‚¬ìš©")
        extractor = ImprovedPDFExtractor(pdf_path)
        return extractor.extract_all(mode='smart')

    texts = []

    try:
        with pdfplumber.open(pdf_path) as pdf:
            print(f"ğŸ“„ PDF íŒŒì¼ ì—´ê¸°: {pdf_path}")
            print(f"   ì´ {len(pdf.pages)}í˜ì´ì§€")
            print(f"   ë¶„ë¦¬ ëª¨ë“œ: {split_mode}")

            # ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            all_text = ""
            for page_num, page in enumerate(pdf.pages, 1):
                page_text = page.extract_text()
                if page_text:
                    all_text += page_text + "\n"

            if split_mode == 'smart':
                # ì§€ëŠ¥í˜• ë¶„ë¦¬: ê¸ˆìœµ/ë²•ë¥  ë¬¸ì„œì— ìµœì í™”
                texts = extract_smart_segments(all_text)

            elif split_mode == 'sentence':
                # ê¸°ë³¸ ë¬¸ì¥ ë‹¨ìœ„ ë¶„ë¦¬
                sentences = re.split(r'[.!?]+', all_text)
                for sentence in sentences:
                    sentence = sentence.strip()
                    if len(sentence) > 10:
                        texts.append(sentence)

            elif split_mode == 'paragraph':
                # ë‹¨ë½ ë‹¨ìœ„ ë¶„ë¦¬ (ì¤„ë°”ê¿ˆ 2ê°œ ì´ìƒ)
                paragraphs = re.split(r'\n\n+', all_text)
                for para in paragraphs:
                    para = para.strip()
                    if len(para) > 20:
                        texts.append(para)

            elif split_mode == 'bullet':
                # ë²ˆí˜¸/ê¸°í˜¸ í•­ëª© ë‹¨ìœ„ ë¶„ë¦¬
                texts = extract_bullet_items(all_text)

            else:  # 'page' ë˜ëŠ” ê¸°íƒ€
                # í˜ì´ì§€ ë‹¨ìœ„ë¡œ ì €ì¥
                for page_num, page in enumerate(pdf.pages, 1):
                    page_text = page.extract_text()
                    if page_text and len(page_text.strip()) > 10:
                        texts.append(page_text.strip())

        print(f"âœ… {len(texts)}ê°œ í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ ì™„ë£Œ")

        # ì²˜ìŒ 3ê°œ ìƒ˜í”Œ ì¶œë ¥
        if texts:
            print("\nğŸ“ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ:")
            for i, text in enumerate(texts[:3], 1):
                preview = text[:60] + "..." if len(text) > 60 else text
                print(f"   {i}. {preview}")

        return texts

    except Exception as e:
        print(f"âŒ PDF ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return []


def extract_smart_segments(text):
    """
    ì§€ëŠ¥í˜• í…ìŠ¤íŠ¸ ë¶„ë¦¬ (ê¸ˆìœµ/ë²•ë¥  ë¬¸ì„œ ìµœì í™”)
    ë²ˆí˜¸ í•­ëª©, ì¡°ê±´ì ˆ, ì„œë¸Œì„¹ì…˜ ë“±ì„ ê°œë³„ì ìœ¼ë¡œ ë¶„ë¦¬
    """
    segments = []

    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - í…Œì´ë¸” êµ¬ì¡° ì •ë¦¬
    text = preprocess_text_for_extraction(text)

    # 1ë‹¨ê³„: ì£¼ìš” ì„¹ì…˜ ë¶„ë¦¬ (ëŒ€ì œëª© ê¸°ì¤€)
    section_patterns = [
        r'^ì œ\s*\d+\s*[ì¡°í•­ê´€]',      # ì œ1ì¡°, ì œ2í•­ ë“±
        r'^\d+\s*\.\s*[ê°€-í£]+',      # 1. ì œëª©
        r'^[A-Z]\.\s*',               # A. B. C.
        r'^[â… â…¡â…¢â…£â…¤â…¥â…¦â…§â…¨â…©]\.',     # ë¡œë§ˆ ìˆ«ì
    ]

    # 2ë‹¨ê³„: ë²ˆí˜¸ í•­ëª© ë¶„ë¦¬ (ë” ì •í™•í•œ íŒ¨í„´)
    bullet_patterns = [
        r'^[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]',        # ì› ë²ˆí˜¸
        r'^[â¶â·â¸â¹âºâ»â¼â½â¾â¿]',        # ê²€ì€ ì› ë²ˆí˜¸
        r'^[â‘´â‘µâ‘¶â‘·â‘¸â‘¹â‘ºâ‘»â‘¼â‘½]',        # ê´„í˜¸ ë²ˆí˜¸
        r'^\d+\)',                    # 1) 2) 3)
        r'^[ê°€ë‚˜ë‹¤ë¼ë§ˆë°”ì‚¬ì•„ìì°¨]\)',  # ê°€) ë‚˜) ë‹¤)
        r'^[-â€¢â–ªâ–«â—¦â€»]',                # ë¶ˆë¦¿ í¬ì¸íŠ¸
        r'^\*',                       # ë³„í‘œ
    ]

    # ëª¨ë“  íŒ¨í„´ í†µí•©
    all_patterns = '|'.join(section_patterns + bullet_patterns)

    # í…ìŠ¤íŠ¸ë¥¼ ì¤„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    lines = text.split('\n')
    current_segment = []
    current_type = None

    for i, line in enumerate(lines):
        original_line = line
        line = line.strip()

        if not line:
            # ë¹ˆ ì¤„ì´ ë‚˜ì˜¤ë©´ í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ ì¢…ë£Œ
            if current_segment and len(' '.join(current_segment).strip()) > 10:
                segments.append(' '.join(current_segment).strip())
                current_segment = []
                current_type = None
            continue

        # í…Œì´ë¸” í—¤ë” ê°ì§€ (ì„œë¹„ìŠ¤êµ¬ë¶„, ìš°ëŒ€ë‚´ìš© ë“±)
        if is_table_header(line):
            # í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥
            if current_segment:
                segments.append(' '.join(current_segment).strip())
                current_segment = []
            # í…Œì´ë¸” ì²˜ë¦¬
            table_segments = extract_table_segments(lines[i:])
            segments.extend(table_segments)
            # í…Œì´ë¸” ë¶€ë¶„ ìŠ¤í‚µ
            skip_lines = count_table_lines(lines[i:])
            for _ in range(skip_lines - 1):
                if i < len(lines) - 1:
                    i += 1
            continue

        # íŒ¨í„´ ë§¤ì¹­ í™•ì¸
        pattern_match = re.match(all_patterns, line)

        if pattern_match:
            # ì´ì „ ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥
            if current_segment:
                segment_text = ' '.join(current_segment).strip()
                if len(segment_text) > 10:
                    segments.append(segment_text)
            # ìƒˆ ì„¸ê·¸ë¨¼íŠ¸ ì‹œì‘
            current_segment = [line]
            current_type = detect_segment_type(line)
        else:
            # ë“¤ì—¬ì“°ê¸°ë‚˜ ì—°ì†ëœ ë‚´ìš©ì¸ ê²½ìš°
            if current_segment:
                # ê°™ì€ ì„¸ê·¸ë¨¼íŠ¸ì— ì¶”ê°€
                current_segment.append(line)
            else:
                # ë…ë¦½ì ì¸ í…ìŠ¤íŠ¸
                if len(line) > 10:
                    segments.append(line)

    # ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥
    if current_segment:
        segment_text = ' '.join(current_segment).strip()
        if len(segment_text) > 10:
            segments.append(segment_text)

    # í›„ì²˜ë¦¬: ë„ˆë¬´ ê¸´ ì„¸ê·¸ë¨¼íŠ¸ëŠ” ë‹¤ì‹œ ë¶„ë¦¬
    final_segments = []
    for seg in segments:
        if len(seg) > 500:  # 500ì ì´ìƒì´ë©´
            # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë‹¤ì‹œ ë¶„ë¦¬
            sub_sentences = re.split(r'(?<=[.!?])\s+', seg)
            for sub in sub_sentences:
                if len(sub.strip()) > 10:
                    final_segments.append(sub.strip())
        else:
            final_segments.append(seg)

    # ì¤‘ë³µ ì œê±°
    unique_segments = []
    seen = set()

    for seg in final_segments:
        normalized = ' '.join(seg.split())
        if normalized not in seen and len(normalized) > 10:
            seen.add(normalized)
            unique_segments.append(normalized)

    return unique_segments


def preprocess_text_for_extraction(text):
    """
    í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - í…Œì´ë¸”ì´ë‚˜ íŠ¹ìˆ˜ êµ¬ì¡° ì •ë¦¬
    """
    # ê°„ë‹¨í•œ ì ‘ê·¼: í•œê¸€ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ì¤‘ë³µ ì œê±°
    # ìê¸ˆì¡°ë‹¬ê¸ˆë¦¬ â†’ ìê¸ˆì¡°ë‹¬ê¸ˆë¦¬ (ë³€í™” ì—†ìŒ)
    # ìììê¸ˆê¸ˆê¸ˆì¡°ì¡°ì¡°ë‹¬ë‹¬ë‹¬ê¸ˆê¸ˆê¸ˆë¦¬ë¦¬ë¦¬ â†’ ìê¸ˆì¡°ë‹¬ê¸ˆë¦¬

    import re

    # ë°©ë²• 1: í•œê¸€ 2ìŒì ˆ ì´ìƒ ë‹¨ìœ„ê°€ ì •í™•íˆ 3ë²ˆ ë°˜ë³µë˜ëŠ” íŒ¨í„´
    # ì˜ˆ: (ìê¸ˆ)(ìê¸ˆ)(ìê¸ˆ) â†’ (ìê¸ˆ)
    # í•˜ì§€ë§Œ ì´ê²ƒë„ ì˜ëª»ë  ìˆ˜ ìˆìŒ

    # ë°©ë²• 2: ë” ê°„ë‹¨í•˜ê²Œ - í•œê¸€ 1ìŒì ˆì´ ì •í™•íˆ 3ë²ˆ ì—°ì† ë°˜ë³µ
    # ìììê¸ˆê¸ˆê¸ˆ â†’ ìê¸ˆ
    result = []
    i = 0

    while i < len(text):
        # í˜„ì¬ ë¬¸ìê°€ í•œê¸€ì´ê³ , ë‹¤ìŒ 2ê°œê°€ ê°™ì€ ë¬¸ìì¸ì§€ í™•ì¸
        if i + 2 < len(text) and 'ê°€' <= text[i] <= 'í£':
            if text[i] == text[i+1] == text[i+2]:
                # 3ê°œê°€ ê°™ìœ¼ë©´ í•˜ë‚˜ë§Œ ì¶”ê°€
                result.append(text[i])
                i += 3
            else:
                result.append(text[i])
                i += 1
        else:
            result.append(text[i])
            i += 1

    text = ''.join(result)

    # ì—°ì†ëœ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ
    text = re.sub(r'[ \t]+', ' ', text)

    # í…Œì´ë¸” êµ¬ë¶„ì ì²˜ë¦¬
    text = re.sub(r'(\S)\s{3,}(\S)', r'\1 | \2', text)  # 3ê°œ ì´ìƒ ê³µë°±ì€ êµ¬ë¶„ìë¡œ

    # íŠ¹ìˆ˜ bullet ë¬¸ì ì •ê·œí™” (ì„ íƒì‚¬í•­)
    text = text.replace('â€£', 'â€¢')
    text = text.replace('âœ', 'â†’')

    return text


def is_table_header(line):
    """
    í…Œì´ë¸” í—¤ë”ì¸ì§€ í™•ì¸
    """
    table_headers = [
        'ì„œë¹„ìŠ¤êµ¬ë¶„', 'ìš°ëŒ€ë‚´ìš©', 'ìš°ëŒ€ì¡°ê±´', 'ì ìš©ê¸°ì¤€',
        'êµ¬ë¶„', 'ë‚´ìš©', 'ì¡°ê±´', 'ë¹„ê³ ', 'í•­ëª©', 'ì„¤ëª…'
    ]

    for header in table_headers:
        if header in line and len(line.split()) <= 5:
            return True
    return False


def extract_table_segments(lines):
    """
    í…Œì´ë¸” í˜•ì‹ì˜ ë°ì´í„°ë¥¼ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ì¶”ì¶œ
    """
    segments = []
    current_row = []

    for line in lines:
        line = line.strip()

        # í…Œì´ë¸” ë ê°ì§€
        if not line or re.match(r'^[â‘ â‘¡â‘¢â‘£â‘¤]', line):
            if current_row:
                segments.append(' '.join(current_row))
            break

        # ë²ˆí˜¸ í•­ëª©ì´ ìˆëŠ” í–‰
        if re.match(r'^[â¶â·â¸â¹âºâ»â¼â½â¾â¿]', line):
            if current_row:
                segments.append(' '.join(current_row))
            current_row = [line]
        else:
            if current_row:
                current_row.append(line)

    if current_row:
        segments.append(' '.join(current_row))

    return [s for s in segments if len(s) > 10]


def count_table_lines(lines):
    """
    í…Œì´ë¸”ì´ ì°¨ì§€í•˜ëŠ” ì¤„ ìˆ˜ ê³„ì‚°
    """
    count = 0
    for line in lines:
        line = line.strip()
        if not line or re.match(r'^[â‘ â‘¡â‘¢â‘£â‘¤]', line):
            break
        count += 1
    return count


def detect_segment_type(line):
    """
    ì„¸ê·¸ë¨¼íŠ¸ íƒ€ì… ê°ì§€
    """
    if re.match(r'^[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]', line):
        return 'main_item'
    elif re.match(r'^[â¶â·â¸â¹âºâ»â¼â½â¾â¿]', line):
        return 'sub_item'
    elif re.match(r'^ì œ\s*\d+\s*[ì¡°í•­ê´€]', line):
        return 'article'
    elif re.match(r'^\d+\)', line):
        return 'numbered'
    else:
        return 'other'


def extract_bullet_items(text):
    """
    ë²ˆí˜¸/ê¸°í˜¸ í•­ëª©ë§Œ ì¶”ì¶œ
    """
    items = []

    # ë²ˆí˜¸/ê¸°í˜¸ íŒ¨í„´
    patterns = [
        (r'[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]', 'ì›ë²ˆí˜¸'),
        (r'[â¶â·â¸â¹âºâ»â¼â½â¾â¿]', 'ê²€ì€ì›'),
        (r'\d+\)', 'ìˆ«ìê´„í˜¸'),
        (r'[ê°€ë‚˜ë‹¤ë¼ë§ˆë°”ì‚¬ì•„ìì°¨]\)', 'í•œê¸€ê´„í˜¸'),
        (r'[-â€¢â–ªâ–«â—¦]', 'ë¶ˆë¦¿'),
    ]

    lines = text.split('\n')
    current_item = []
    current_type = None

    for line in lines:
        line = line.strip()
        if not line:
            continue

        # íŒ¨í„´ ë§¤ì¹­
        matched = False
        for pattern, ptype in patterns:
            if re.match(f'^{pattern}', line):
                # ì´ì „ í•­ëª© ì €ì¥
                if current_item:
                    item_text = ' '.join(current_item).strip()
                    if len(item_text) > 10:
                        items.append(item_text)

                # ìƒˆ í•­ëª© ì‹œì‘
                current_item = [line]
                current_type = ptype
                matched = True
                break

        if not matched and current_item:
            # í˜„ì¬ í•­ëª©ì— ê³„ì† ì¶”ê°€
            current_item.append(line)

    # ë§ˆì§€ë§‰ í•­ëª© ì €ì¥
    if current_item:
        item_text = ' '.join(current_item).strip()
        if len(item_text) > 10:
            items.append(item_text)

    return items


def extract_texts_from_multiple_pdfs(pdf_paths, split_mode='smart'):
    """
    ì—¬ëŸ¬ PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ

    Args:
        pdf_paths: PDF íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        split_mode: í…ìŠ¤íŠ¸ ë¶„ë¦¬ ë°©ì‹ (smart/sentence/paragraph/bullet/page)

    Returns:
        ëª¨ë“  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    all_texts = []

    for pdf_path in pdf_paths:
        texts = extract_texts_from_pdf(pdf_path, split_mode)
        all_texts.extend(texts)

    print(f"\nì „ì²´ ì¶”ì¶œ ê²°ê³¼:")
    print(f"   â€¢ PDF íŒŒì¼ ìˆ˜: {len(pdf_paths)}ê°œ")
    print(f"   â€¢ ì´ í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸: {len(all_texts)}ê°œ")

    return all_texts


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    # Colab í™˜ê²½ í™•ì¸
    try:
        from google.colab import drive
        drive.mount('/content/drive')
        print(" Google Drive ë§ˆìš´íŠ¸ ì™„ë£Œ")
        is_colab = True
    except:
        print(" ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘...")
        is_colab = False

    # PDF ì§€ì› í™•ì¸
    if not PDF_SUPPORT:
        print("\n PDF ì²˜ë¦¬ë¥¼ ìœ„í•´ ì„¤ì¹˜:")
        print("   !pip install pdfplumber")

    # ì„¤ì •
    HF_TOKEN = None  # Colabì—ì„œ ì´ë¯¸ ë¡œê·¸ì¸í–ˆìœ¼ë©´ None, ë¡œì»¬ì—ì„œëŠ” í† í° ì…ë ¥

    # ëª¨ë¸ ì„ íƒ (í•˜ë‚˜ë§Œ ì£¼ì„ í•´ì œ)
    #MODEL_NAME = "google/gemma-2-2b-it"  # ê¸°ì¡´ (2B)
    MODEL_NAME = "google/gemma-2-9b-it"  # ë” í° Gemma (9B) â­

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    if is_colab:
        OUTPUT_DIR = "/content/drive/MyDrive/text_difficulty_labels"
    else:
        OUTPUT_DIR = "./labeled_data"

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # ë¼ë²¨ëŸ¬ ì´ˆê¸°í™”
    labeler = TextDifficultyLabeler(
        model_name=MODEL_NAME,
        hf_token=HF_TOKEN  # Noneì´ë©´ ê¸°ì¡´ ë¡œê·¸ì¸ ì„¸ì…˜ ì‚¬ìš©
    )

    # ===== í…ìŠ¤íŠ¸ ì¤€ë¹„ =====

    # split_mode ì˜µì…˜:
    # - 'smart': ì§€ëŠ¥í˜• ë¶„ë¦¬ (ê¸ˆìœµ/ë²•ë¥  ë¬¸ì„œ ì¶”ì²œ) â­
    # - 'sentence': ë¬¸ì¥ ë‹¨ìœ„ ë¶„ë¦¬
    # - 'paragraph': ë‹¨ë½ ë‹¨ìœ„ ë¶„ë¦¬
    # - 'bullet': ë²ˆí˜¸/ê¸°í˜¸ í•­ëª© ë‹¨ìœ„
    # - 'page': í˜ì´ì§€ ë‹¨ìœ„

    #ì˜µì…˜ 1: ë‹¨ì¼ PDF (smart ëª¨ë“œ ì‚¬ìš©)
    #pdf_path = "/content/drive/MyDrive/10000831_pi.pdf"
    #texts = extract_texts_from_pdf(pdf_path, split_mode='smart')

    #ì˜µì…˜ 2: ì—¬ëŸ¬ PDF íŒŒì¼ ì§ì ‘ ì§€ì •
    #pdf_files = [
    #    "/content/drive/MyDrive/doc1.pdf",
    #    "/content/drive/MyDrive/doc2.pdf",
    #]
    #texts = extract_texts_from_multiple_pdfs(pdf_files, split_mode='smart')

    #ì˜µì…˜ 3: ì—¬ëŸ¬ í´ë”ì—ì„œ ëª¨ë“  PDF ìë™ ìˆ˜ì§‘ â­
    import glob

    # í´ë” ëª©ë¡
    folders = [
        "/content/drive/MyDrive/NHìƒí’ˆì„¤ëª…ì„œ",
        "/content/drive/MyDrive/ëŒ€ì¶œ",
        "/content/drive/MyDrive/ëŒ€ì¶œ ì•½ê´€",
        "/content/drive/MyDrive/ì™¸í™˜",
        "/content/drive/MyDrive/ì €ì¶•"
    ]

    # ëª¨ë“  PDF íŒŒì¼ ìˆ˜ì§‘
    all_pdf_files = []
    for folder in folders:
        # ê° í´ë”ì—ì„œ PDF íŒŒì¼ ì°¾ê¸°
        pdf_pattern = f"{folder}/*.pdf"
        pdf_files = glob.glob(pdf_pattern)

        # í•˜ìœ„ í´ë”ë„ í¬í•¨í•˜ë ¤ë©´
        pdf_pattern_recursive = f"{folder}/**/*.pdf"
        pdf_files_recursive = glob.glob(pdf_pattern_recursive, recursive=True)

        # í•©ì¹˜ê¸°
        all_files = list(set(pdf_files + pdf_files_recursive))
        all_pdf_files.extend(all_files)

        print(f"ğŸ“ {folder}: {len(all_files)}ê°œ PDF ë°œê²¬")

    print(f"\nğŸ“š ì´ {len(all_pdf_files)}ê°œ PDF íŒŒì¼ ë°œê²¬")

    # íŒŒì¼ ëª©ë¡ ì¶œë ¥ (ì„ íƒì‚¬í•­)
    if len(all_pdf_files) <= 10:
        print("\níŒŒì¼ ëª©ë¡:")
        for i, file in enumerate(all_pdf_files, 1):
            print(f"  {i}. {file.split('/')[-1]}")
    else:
        print(f"\nì²˜ìŒ 10ê°œ íŒŒì¼:")
        for i, file in enumerate(all_pdf_files[:10], 1):
            print(f"  {i}. {file.split('/')[-1]}")
        print(f"  ... ì™¸ {len(all_pdf_files)-10}ê°œ")

    # ëª¨ë“  PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    texts = extract_texts_from_multiple_pdfs(all_pdf_files, split_mode='smart')



    print(f"ğŸ“š ì´ {len(texts)}ê°œ í…ìŠ¤íŠ¸ ì¤€ë¹„ ì™„ë£Œ")

    # ë¼ë²¨ë§ ì‹¤í–‰
    checkpoint_path = os.path.join(OUTPUT_DIR, "checkpoint.csv")
    df_results = labeler.label_texts(
        texts=texts,
        batch_save=10,
        checkpoint_path=checkpoint_path
    )

    # ê²°ê³¼ ìš”ì•½
    labeler.print_summary()

    # ê²°ê³¼ ì €ì¥
    csv_path, excel_path, json_path = labeler.save_results(OUTPUT_DIR)

    # ì‹œê°í™”
    graph_path = os.path.join(OUTPUT_DIR, "difficulty_distribution.png")
    labeler.visualize_results(save_path=graph_path)

    print("\n" + "="*50)
    print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("="*50)
    print(f"ì €ì¥ëœ íŒŒì¼:")
    print(f"  ğŸ“„ CSV: {csv_path}")
    print(f"  ğŸ“Š Excel: {excel_path}")
    print(f"  ğŸ“‹ JSON: {json_path}")
    print(f"  ğŸ“ˆ ê·¸ë˜í”„: {graph_path}")

    return df_results


# ì‹¤í–‰
if __name__ == "__main__":
    results = main()