import sys
import os
from typing import Dict, List, Optional, Tuple
import asyncio
from datetime import datetime, timezone

# eyetrack ëª¨ë“ˆ import (ê¸°ì¡´ ì½”ë“œ ì¬ì‚¬ìš©)
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'eyetrack'))

try:
    from comprehension_analyzer import ComprehensionAnalyzer, ComprehensionMetrics, RealTimeComprehensionMonitor
    from hybrid_analyzer import hybrid_analyzer
except ImportError:
    print("Warning: eyetrack ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
    ComprehensionAnalyzer = None
    ComprehensionMetrics = None
    RealTimeComprehensionMonitor = None
    hybrid_analyzer = None

class EyeTrackingService:
    """ê¸°ì¡´ eyetrack ëª¨ë“ˆì„ ì›¹ ì„œë¹„ìŠ¤ë¡œ ê°ì‹¸ëŠ” ì„œë¹„ìŠ¤ ë ˆì´ì–´"""
    
    def __init__(self):
        self.analyzer = ComprehensionAnalyzer() if ComprehensionAnalyzer else None
        self.monitor = RealTimeComprehensionMonitor() if RealTimeComprehensionMonitor else None
        self.session_data = {}  # consultation_idë³„ ì„¸ì…˜ ë°ì´í„°
    
    def get_text_difficulty(self, section_text: str) -> float:
        """í•œêµ­ì–´ ê¸ˆìœµ ì•½ê´€ í…ìŠ¤íŠ¸ì˜ ë‚œì´ë„ ë¶„ì„ (ê¸°ì¡´ ë¡œì§ ê°œì„ )"""
        
        # ì–´ë ¤ìš´ ê¸ˆìœµ ìš©ì–´ë“¤ (í™•ì¥)
        difficult_financial_terms = [
            # ê¸°ë³¸ ê¸ˆìœµ ìš©ì–´
            'ì¤‘ë„í•´ì§€', 'ìš°ëŒ€ê¸ˆë¦¬', 'ì˜ˆê¸ˆìë³´í˜¸', 'ë§Œê¸°ìë™ì—°ì¥', 'ë³µë¦¬', 'ë‹¨ë¦¬',
            # ì„¸ê¸ˆ ê´€ë ¨
            'ì„¸ì•¡ê³µì œ', 'ì›ì²œì§•ìˆ˜', 'ê³¼ì„¸í‘œì¤€', 'ì†Œë“ê³µì œ', 'ë¹„ê³¼ì„¸',
            # íˆ¬ì ê´€ë ¨
            'ê¸ˆìœµíˆ¬ììƒí’ˆ', 'íŒŒìƒê²°í•©ì¦ê¶Œ', 'í™˜ë§¤ì¡°ê±´ë¶€ì±„ê¶Œ', 'ì‹ íƒ', 'ìˆ˜ìµì¦ê¶Œ',
            'í€ë“œ', 'ìœ„í—˜ë“±ê¸‰', 'ì†ì‹¤ê°€ëŠ¥ì„±', 'ì›ê¸ˆë³´ì¥', 'ë³€ë™ì„±', 'ìœ ë™ì„±',
            # ëŒ€ì¶œ ê´€ë ¨
            'ë‹´ë³´ëŒ€ì¶œ', 'ì‹ ìš©ëŒ€ì¶œ', 'í•œë„ëŒ€ì¶œ', 'ê±°ì¹˜ê¸°ê°„', 'ìƒí™˜ë°©ì‹',
            # ë³´í—˜ ê´€ë ¨
            'ë³´í—˜ë£Œ', 'ë³´ì¥ë‚´ìš©', 'ë©´ì±…ê¸°ê°„', 'í•´ì§€í™˜ê¸‰ê¸ˆ', 'ë§Œê¸°ë³´í—˜ê¸ˆ'
        ]
        
        # ë³µì¡í•œ ë²•ë¥ /ê¸ˆìœµ í‘œí˜„ë“¤
        complex_expressions = [
            '~ì— ë”°ë¼', '~ì„ ì œì™¸í•˜ê³ ', '~ë¥¼ ì¡°ê±´ìœ¼ë¡œ', '~ì— í•œí•˜ì—¬', '~ì— ê´€í•˜ì—¬',
            'ë‹¨,', 'ë‹¤ë§Œ,', 'ë˜í•œ,', 'ë”°ë¼ì„œ,', 'ê·¸ëŸ¬ë‚˜,', 'ë˜ëŠ”', 'ë§Œì•½',
            'ìƒê¸°', 'í•´ë‹¹', 'ê´€ë ¨', 'ì¤€ìš©', 'ì ìš©', 'ì œì™¸', 'í¬í•¨'
        ]
        
        # ë¬¸ì¥ ë¶„ì„
        sentences = [s.strip() for s in section_text.split('.') if s.strip()]
        words = section_text.split()
        
        difficulty_score = 0.0
        
        # 1. ì–´ë ¤ìš´ ê¸ˆìœµ ìš©ì–´ ë¹„ìœ¨ (40% ê°€ì¤‘ì¹˜)
        financial_term_count = sum(1 for word in words 
                                  if any(term in word for term in difficult_financial_terms))
        if words:
            difficulty_score += (financial_term_count / len(words)) * 0.4
        
        # 2. ë³µì¡í•œ í‘œí˜„ ì‚¬ìš© ë¹ˆë„ (20% ê°€ì¤‘ì¹˜)
        complex_count = sum(1 for expr in complex_expressions if expr in section_text)
        difficulty_score += min(complex_count / 5, 0.2) * 0.2
        
        # 3. ë¬¸ì¥ ê¸¸ì´ì™€ ë³µì¡ì„± (25% ê°€ì¤‘ì¹˜)
        if sentences:
            avg_sentence_length = sum(len(s) for s in sentences) / len(sentences)
            long_sentence_ratio = len([s for s in sentences if len(s) > 50]) / len(sentences)
            difficulty_score += min(avg_sentence_length / 100, 0.15) * 0.15
            difficulty_score += long_sentence_ratio * 0.1
        
        # 4. ìˆ«ìì™€ í¼ì„¼íŠ¸ í¬í•¨ë„ (15% ê°€ì¤‘ì¹˜)
        import re
        numbers = re.findall(r'\d+(?:\.\d+)?%?', section_text)
        number_density = len(numbers) / max(len(words), 1)
        difficulty_score += min(number_density, 0.15) * 0.15
        
        return min(difficulty_score, 1.0)
    
    def calculate_confusion_probability(self, difficulty_score: float, reading_time: float, 
                                      expected_time: float = 30.0, section_length: int = 100) -> float:
        """ë” ì •êµí•œ í˜¼ë€ë„ ê³„ì‚°"""
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë”°ë¥¸ ê¸°ëŒ€ ì‹œê°„ ì¡°ì •
        adjusted_expected_time = expected_time * (section_length / 100)
        time_ratio = reading_time / adjusted_expected_time
        
        confusion_prob = 0.0
        
        # 1. í…ìŠ¤íŠ¸ ë‚œì´ë„ (50% ê°€ì¤‘ì¹˜)
        confusion_prob += difficulty_score * 0.5
        
        # 2. ì½ê¸° ì‹œê°„ íŒ¨í„´ (30% ê°€ì¤‘ì¹˜)
        if time_ratio > 2.5:  # ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¼
            confusion_prob += min((time_ratio - 1) / 4, 0.3) * 0.3
        elif time_ratio < 0.3:  # ë„ˆë¬´ ë¹¨ë¦¬ ì½ìŒ (ëŒ€ì¶© ì½ìŒ)
            confusion_prob += (0.3 - time_ratio) / 0.3 * 0.2
        
        # 3. ì‹œê°„ëŒ€ë³„ ê°€ì¤‘ì¹˜ (20% ê°€ì¤‘ì¹˜) - ì˜¤í›„ì—ëŠ” ì§‘ì¤‘ë ¥ ì €í•˜
        current_hour = datetime.now().hour
        if 14 <= current_hour <= 16:  # ì˜¤í›„ 2-4ì‹œ
            confusion_prob += 0.1
        elif current_hour >= 18:  # ì €ë… 6ì‹œ ì´í›„
            confusion_prob += 0.15
        
        return min(max(confusion_prob, 0.0), 0.95)
    
    def identify_confused_sentences(self, section_text: str, difficulty_score: float) -> List[int]:
        """ì–´ë ¤ìš´ ë¬¸ì¥ ì‹ë³„ (ê°œì„ ëœ ì•Œê³ ë¦¬ì¦˜)"""
        sentences = [s.strip() for s in section_text.split('.') if s.strip()]
        confused_sentences = []
        
        for i, sentence in enumerate(sentences):
            sentence_score = 0.0
            
            # ë¬¸ì¥ë³„ ë‚œì´ë„ ê³„ì‚°
            sentence_difficulty = self.get_text_difficulty(sentence)
            sentence_score += sentence_difficulty * 0.6
            
            # ë¬¸ì¥ ê¸¸ì´ ì ìˆ˜
            if len(sentence) > 60:
                sentence_score += 0.3
            
            # íŠ¹ë³„íˆ ì–´ë ¤ìš´ íŒ¨í„´
            difficult_patterns = ['ë‹¨,', 'ë‹¤ë§Œ,', 'ê·¸ëŸ¬ë‚˜', '~ì„ ì œì™¸í•˜ê³ ']
            if any(pattern in sentence for pattern in difficult_patterns):
                sentence_score += 0.2
            
            # ì„ê³„ê°’ì„ ë„˜ìœ¼ë©´ ì–´ë ¤ìš´ ë¬¸ì¥ìœ¼ë¡œ ë¶„ë¥˜
            if sentence_score > 0.6:
                confused_sentences.append(i + 1)
        
        # ìµœëŒ€ 4ê°œê¹Œì§€ë§Œ ë°˜í™˜ (UI ê³ ë ¤)
        return confused_sentences[:4]
    
    def generate_ai_explanation(self, section_text: str, confused_sentences: List[int] = None) -> str:
        """AI ì„¤ëª… ìƒì„± (GPT ìŠ¤íƒ€ì¼ ì‹œë®¬ë ˆì´ì…˜)"""
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ì„¤ëª… ë§¤í•‘
        explanations_db = {
            "ì¤‘ë„í•´ì§€": {
                "simple": "ì•½ì†í•œ ê¸°ê°„ ì „ì— ì˜ˆê¸ˆì„ ì°¾ëŠ” ê²ƒì…ë‹ˆë‹¤.",
                "example": "1ë…„ ì•½ì†ìœ¼ë¡œ ë„£ì€ ëˆì„ 6ê°œì›” ë§Œì— ì°¾ìœ¼ë©´ ì¤‘ë„í•´ì§€ì˜ˆìš”.",
                "impact": "ì•½ì†í•œ ë†’ì€ ì´ì ëŒ€ì‹  ë‚®ì€ ì´ìë§Œ ë°›ê²Œ ë©ë‹ˆë‹¤."
            },
            "ìš°ëŒ€ê¸ˆë¦¬": {
                "simple": "ì¡°ê±´ì„ ë§ì¶”ë©´ ë” ë†’ì€ ì´ìë¥¼ ì£¼ëŠ” í˜œíƒì…ë‹ˆë‹¤.",
                "example": "ê¸‰ì—¬í†µì¥ìœ¼ë¡œ ì“°ê±°ë‚˜ ì¹´ë“œ ì‚¬ìš©í•˜ë©´ ì¶”ê°€ ì´ìë¥¼ ë°›ì„ ìˆ˜ ìˆì–´ìš”.",
                "impact": "ì¡°ê±´ ë¯¸ì¶©ì¡±ì‹œ ê¸°ë³¸ ì´ìë§Œ ì ìš©ë©ë‹ˆë‹¤."
            },
            "ì˜ˆê¸ˆìë³´í˜¸": {
                "simple": "ì€í–‰ì´ ë¬¸ì œê°€ ìƒê²¨ë„ ëˆì„ ë³´ì¥í•´ì£¼ëŠ” ì œë„ì…ë‹ˆë‹¤.",
                "example": "ê°œì¸ë‹¹ ìµœëŒ€ 5ì²œë§Œì›ê¹Œì§€ ì •ë¶€ê°€ ë³´ì¥í•´ì¤ë‹ˆë‹¤.",
                "impact": "5ì²œë§Œì› ì´ˆê³¼ ê¸ˆì•¡ì€ ë³´ì¥ë°›ê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            },
            "ë³µë¦¬": {
                "simple": "ì´ìì— ë‹¤ì‹œ ì´ìê°€ ë¶™ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.",
                "example": "100ë§Œì›ì— 10% ì´ìê°€ ë¶™ìœ¼ë©´, ë‹¤ìŒì—” 110ë§Œì›ì— 10%ê°€ ë¶™ì–´ìš”.",
                "impact": "ì‹œê°„ì´ ì§€ë‚ ìˆ˜ë¡ ì´ìê°€ ì ì  ë” ë§ì´ ë°›ê²Œ ë©ë‹ˆë‹¤."
            },
            "ë³€ë™ê¸ˆë¦¬": {
                "simple": "ì‹œì¥ ìƒí™©ì— ë”°ë¼ ì´ììœ¨ì´ ë°”ë€ŒëŠ” ë°©ì‹ì…ë‹ˆë‹¤.",
                "example": "ì²˜ìŒì—” 3%ì˜€ë‹¤ê°€ ë‚˜ì¤‘ì— 4%ë¡œ ì˜¤ë¥´ê±°ë‚˜ 2%ë¡œ ë‚´ë¦´ ìˆ˜ ìˆì–´ìš”.",
                "impact": "ê¸ˆë¦¬ê°€ ì˜¤ë¥´ë©´ ì¢‹ì§€ë§Œ, ë‚´ë¦¬ë©´ ìˆ˜ìµì´ ì¤„ì–´ë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            }
        }
        
        # í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì°¾ê¸°
        for keyword, explanation in explanations_db.items():
            if keyword in section_text:
                return (f"**{keyword}**ë€?\n"
                       f"â€¢ {explanation['simple']}\n"
                       f"â€¢ {explanation['example']}\n"
                       f"ì£¼ì˜: {explanation['impact']}")
        
        # ê¸°ë³¸ ì„¤ëª… (í‚¤ì›Œë“œê°€ ì—†ëŠ” ê²½ìš°)
        if confused_sentences and len(confused_sentences) > 0:
            return (f"**ì´ ë¶€ë¶„ì´ ë³µì¡í•˜ì‹ ê°€ìš”?**\n"
                   f"â€¢ ê¸ˆìœµ ì•½ê´€ì—ëŠ” ë²•ì  ë³´í˜¸ë¥¼ ìœ„í•œ ì¤‘ìš”í•œ ë‚´ìš©ë“¤ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n"
                   f"â€¢ ì²œì²œíˆ ì½ì–´ë³´ì‹œê³ , ê¶ê¸ˆí•œ ì ì€ ì–¸ì œë“  ë¬¸ì˜í•˜ì„¸ìš”.\n"
                   f"ì£¼ì˜: íŠ¹íˆ {len(confused_sentences)}ê°œ ë¬¸ì¥ì€ ì£¼ì˜ ê¹Šê²Œ í™•ì¸í•´ë³´ì„¸ìš”.")
        
        return ("**ì•½ê´€ ë‚´ìš© ì•ˆë‚´**\n"
               "â€¢ ì´ ë‚´ìš©ì€ ìƒí’ˆì˜ ì¤‘ìš”í•œ ì¡°ê±´ë“¤ì„ ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n"
               "â€¢ ì´í•´ê°€ ì–´ë ¤ìš°ì‹œë©´ ì–¸ì œë“ ì§€ ì§ì›ì—ê²Œ ì„¤ëª…ì„ ìš”ì²­í•˜ì„¸ìš”.\n"
               "ì£¼ì˜: ê°€ì… ì „ì— ëª¨ë“  ì¡°ê±´ì„ ì¶©ë¶„íˆ ì´í•´í•˜ì‹œëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.")
    
    async def analyze_reading_session(self, consultation_id: str, section_name: str, 
                                    section_text: str, reading_time: float, 
                                    gaze_data: Optional[Dict] = None) -> Dict:
        """ì½ê¸° ì„¸ì…˜ ë¶„ì„ (ë©”ì¸ API í•¨ìˆ˜)"""
        
        try:
            # 1. í…ìŠ¤íŠ¸ ë‚œì´ë„ ë¶„ì„
            difficulty_score = self.get_text_difficulty(section_text)
            
            # 2. í˜¼ë€ë„ ê³„ì‚°
            confusion_probability = self.calculate_confusion_probability(
                difficulty_score, reading_time, section_length=len(section_text)
            )
            
            # 3. ì–´ë ¤ìš´ ë¬¸ì¥ ì‹ë³„
            confused_sentences = self.identify_confused_sentences(section_text, difficulty_score)
            
            # 4. AI ì„¤ëª… ìƒì„±
            ai_explanation = self.generate_ai_explanation(section_text, confused_sentences)
            
            # 5. ì´í•´ë„ ë ˆë²¨ ê²°ì •
            if confusion_probability > 0.7:
                comprehension_level = "low"
                status = "confused"
            elif confusion_probability > 0.4:
                comprehension_level = "medium"
                status = "moderate"
            else:
                comprehension_level = "high"
                status = "good"
            
            # 6. ì¶”ì²œì‚¬í•­ ìƒì„±
            recommendations = self._generate_recommendations(
                confusion_probability, difficulty_score, reading_time
            )
            
            # 7. ì„¸ì…˜ ë°ì´í„° ì—…ë°ì´íŠ¸
            if consultation_id not in self.session_data:
                self.session_data[consultation_id] = {
                    'sections': [],
                    'start_time': datetime.now(timezone.utc)
                }
            
            self.session_data[consultation_id]['sections'].append({
                'section_name': section_name,
                'difficulty_score': difficulty_score,
                'confusion_probability': confusion_probability,
                'comprehension_level': comprehension_level,
                'timestamp': datetime.now(timezone.utc)
            })
            
            # 7. í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ì¶”ê°€ (ìˆìœ¼ë©´)
            hybrid_data = {}
            if hybrid_analyzer:
                try:
                    hybrid_result = hybrid_analyzer.analyze_text_hybrid(section_text)
                    hybrid_data = {
                        "difficult_terms": hybrid_result.difficult_terms,
                        "underlined_sections": hybrid_result.underlined_sections,
                        "detailed_explanations": hybrid_result.detailed_explanations
                    }
                except Exception as e:
                    print(f"í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ì‹¤íŒ¨: {e}")
                    hybrid_data = {
                        "difficult_terms": [],
                        "underlined_sections": [],
                        "detailed_explanations": {}
                    }
            else:
                hybrid_data = {
                    "difficult_terms": [],
                    "underlined_sections": [],
                    "detailed_explanations": {}
                }
            
            result = {
                "status": status,
                "confused_sentences": confused_sentences,
                "ai_explanation": ai_explanation,
                "difficulty_score": round(difficulty_score, 2),
                "confusion_probability": round(confusion_probability, 2),
                "comprehension_level": comprehension_level,
                "recommendations": recommendations,
                "analysis_metadata": {
                    "section_length": len(section_text),
                    "reading_speed_wpm": len(section_text.split()) / (reading_time / 60) if reading_time > 0 else 0,
                    "analyzed_at": datetime.now(timezone.utc).isoformat()
                }
            }
            
            # í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„° ì¶”ê°€
            result.update(hybrid_data)
            return result
            
        except Exception as e:
            return {
                "status": "error",
                "error_message": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "confused_sentences": [],
                "ai_explanation": "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "difficulty_score": 0.5,
                "comprehension_level": "medium",
                "recommendations": ["ê¸°ìˆ ì  ë¬¸ì œë¡œ ì¸í•´ ë¶„ì„ì´ ì œí•œë©ë‹ˆë‹¤. ì§ì›ì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."]
            }
    
    def _generate_recommendations(self, confusion_prob: float, difficulty: float, reading_time: float) -> List[str]:
        """ìƒí™©ë³„ ë§ì¶¤ ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if confusion_prob > 0.8:
            recommendations.extend([
                "ğŸš¨ ì´ ë¶€ë¶„ì´ ë§¤ìš° ì–´ë ¤ì›Œ ë³´ì…ë‹ˆë‹¤. ì§ì›ì˜ ìƒì„¸ ì„¤ëª…ì„ ë“¤ì–´ë³´ì„¸ìš”.",
                "ğŸ“– í•œ ë¬¸ì¥ì”© ì²œì²œíˆ ë‹¤ì‹œ ì½ì–´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤.",
                "ì´í•´ë˜ì§€ ì•ŠëŠ” ìš©ì–´ê°€ ìˆìœ¼ë©´ ë°”ë¡œ ì§ˆë¬¸í•˜ì„¸ìš”."
            ])
        elif confusion_prob > 0.6:
            recommendations.extend([
                "ì¤‘ìš”í•œ ë‚´ìš©ì´ë‹ˆ ì¶©ë¶„í•œ ì‹œê°„ì„ ë“¤ì—¬ ì´í•´í•˜ì„¸ìš”.",
                "í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ë©”ëª¨í•˜ë©° ì½ì–´ë³´ì„¸ìš”.",
                "ë‹¤ì‹œ í•œë²ˆ ì½ì–´ë³´ì‹œê±°ë‚˜ ì§ì›ì—ê²Œ ì„¤ëª…ì„ ìš”ì²­í•˜ì„¸ìš”."
            ])
        elif confusion_prob > 0.4:
            recommendations.extend([
                "ì „ë°˜ì ìœ¼ë¡œ ì˜ ì´í•´í•˜ê³  ê³„ì‹­ë‹ˆë‹¤.",
                "ì„¸ë¶€ ì¡°ê±´ë“¤ì„ í•œë²ˆ ë” í™•ì¸í•´ë³´ì„¸ìš”.",
                "ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ì–¸ì œë“  ë¬¸ì˜í•˜ì„¸ìš”."
            ])
        else:
            recommendations.extend([
                "ì™„ë²½í•˜ê²Œ ì´í•´í•˜ê³  ê³„ì‹­ë‹ˆë‹¤!",
                "ì´ ì†ë„ë¡œ ê³„ì† ì§„í–‰í•˜ì‹œë©´ ë©ë‹ˆë‹¤.",
                "ë‹¤ìŒ ë‹¨ê³„ë¡œ ë„˜ì–´ê°€ì…”ë„ ì¢‹ìŠµë‹ˆë‹¤."
            ])
        
        # ì½ê¸° ì†ë„ ê´€ë ¨ ì¶”ì²œ
        if reading_time > 120:  # 2ë¶„ ì´ìƒ
            recommendations.append("ì¶©ë¶„í•œ ì‹œê°„ì„ ë“¤ì—¬ ê¼¼ê¼¼íˆ ì½ê³  ê³„ì‹œë„¤ìš”. ì¢‹ìŠµë‹ˆë‹¤!")
        elif reading_time < 15:  # 15ì´ˆ ë¯¸ë§Œ
            recommendations.append("âš¡ ë„ˆë¬´ ë¹¨ë¦¬ ì½ìœ¼ì‹  ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì¤‘ìš”í•œ ë‚´ìš©ì„ ë†“ì¹˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ì„¸ìš”.")
        
        return recommendations[:4]  # ìµœëŒ€ 4ê°œê¹Œì§€ë§Œ
    
    def get_session_summary(self, consultation_id: str) -> Optional[Dict]:
        """ì„¸ì…˜ ì „ì²´ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        if consultation_id not in self.session_data:
            return None
        
        session = self.session_data[consultation_id]
        sections = session['sections']
        
        if not sections:
            return None
        
        # ì „ì²´ í†µê³„ ê³„ì‚°
        avg_difficulty = sum(s['difficulty_score'] for s in sections) / len(sections)
        avg_confusion = sum(s['confusion_probability'] for s in sections) / len(sections)
        
        comprehension_counts = {
            'high': len([s for s in sections if s['comprehension_level'] == 'high']),
            'medium': len([s for s in sections if s['comprehension_level'] == 'medium']),
            'low': len([s for s in sections if s['comprehension_level'] == 'low'])
        }
        
        confused_sections = [s['section_name'] for s in sections if s['confusion_probability'] > 0.6]
        
        return {
            'consultation_id': consultation_id,
            'total_sections': len(sections),
            'avg_difficulty': round(avg_difficulty, 2),
            'avg_confusion': round(avg_confusion, 2),
            'comprehension_summary': comprehension_counts,
            'confused_sections': confused_sections,
            'session_duration': (datetime.now(timezone.utc) - session['start_time']).total_seconds() / 60,
            'last_updated': sections[-1]['timestamp'].isoformat() if sections else None
        }

# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
eyetrack_service = EyeTrackingService()