"""
í†µí•© ë¶„ì„ ì„œë¹„ìŠ¤
ì•„ì´íŠ¸ë˜í‚¹ + ì–¼êµ´ í‘œì • + í…ìŠ¤íŠ¸ ë‚œì´ë„ë¥¼ ì¢…í•©í•˜ì—¬ 
ê³ ê°ì˜ ì´í•´ë„ë¥¼ íŒë‹¨í•˜ê³  í•„ìš”ì‹œ ë¬¸ì¥ ê°„ì†Œí™” AIë¥¼ í˜¸ì¶œ
"""

import logging
from typing import Dict, Optional, Any, Tuple
from datetime import datetime, timezone

logger = logging.getLogger(__name__)

class IntegratedAnalysisService:
    """í†µí•© ë¶„ì„ ì„œë¹„ìŠ¤ - ëª¨ë“  ë°ì´í„°ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… íŒë‹¨"""
    
    def __init__(self):
        self.analysis_history = {}
        
        # ê°€ì¤‘ì¹˜ ì„¤ì •
        self.weights = {
            'text_difficulty': 0.35,  # í…ìŠ¤íŠ¸ ë‚œì´ë„
            'face_confusion': 0.35,    # ì–¼êµ´ í‘œì •
            'gaze_pattern': 0.30       # ì‹œì„  íŒ¨í„´
        }
        
        # ì„ê³„ê°’ ì„¤ì •
        self.thresholds = {
            'high_confusion': 0.7,     # ë§¤ìš° í˜¼ë€
            'moderate_confusion': 0.5, # ë³´í†µ í˜¼ë€
            'low_confusion': 0.3       # ì•½ê°„ í˜¼ë€
        }
    
    async def analyze_integrated(
        self,
        consultation_id: str,
        section_name: str,
        section_text: str,
        face_data: Optional[Dict] = None,
        gaze_data: Optional[Dict] = None,
        reading_time: Optional[float] = None
    ) -> Dict[str, Any]:
        """
        í†µí•© ë¶„ì„ ìˆ˜í–‰

        Args:
            consultation_id: ìƒë‹´ ID
            section_name: ì„¹ì…˜ ì´ë¦„
            section_text: ì„¹ì…˜ í…ìŠ¤íŠ¸
            face_data: ì–¼êµ´ ë¶„ì„ ë°ì´í„°
            gaze_data: ì‹œì„  ì¶”ì  ë°ì´í„°
            reading_time: ì½ê¸° ì‹œê°„ (ì´ˆ)

        Returns:
            í†µí•© ë¶„ì„ ê²°ê³¼
        """

        # 1. í…ìŠ¤íŠ¸ ë‚œì´ë„ ë¶„ì„ (AI ëª¨ë¸ ì‚¬ìš©)
        from services.ai_model_service import ai_model_manager
        text_difficulty = await ai_model_manager.hf_models.analyze_difficulty(section_text)
        logger.info(f"ğŸ“ í…ìŠ¤íŠ¸ ë‚œì´ë„ ë¶„ì„ ì™„ë£Œ: {text_difficulty:.2f}")

        # 2. ê° ëª¨ë‹¬ë¦¬í‹°ë³„ í˜¼ë€ë„ ê³„ì‚°
        text_confusion = self._calculate_text_confusion(text_difficulty, section_text)
        face_confusion = await self._calculate_face_confusion(face_data)
        gaze_confusion = self._calculate_gaze_confusion(gaze_data, reading_time, len(section_text))

        # 3. í†µí•© í˜¼ë€ë„ ê³„ì‚°
        integrated_confusion = self._calculate_integrated_confusion(
            text_confusion, face_confusion, gaze_confusion
        )

        # 4. ì´í•´ë„ ë ˆë²¨ ê²°ì •
        comprehension_level, need_ai_help = self._determine_comprehension_level(
            integrated_confusion, face_confusion, gaze_confusion
        )

        # 5. AI ê°„ì†Œí™” í•„ìš” ì—¬ë¶€ íŒë‹¨
        should_simplify = self._should_trigger_simplification(
            integrated_confusion, face_confusion, gaze_confusion, text_difficulty
        )

        # 6. ì¶”ì²œì‚¬í•­ ìƒì„±
        recommendations = self._generate_recommendations(
            comprehension_level, text_confusion, face_confusion, gaze_confusion
        )

        # 7. ë¶„ì„ ì´ë ¥ ì €ì¥
        self._save_analysis_history(
            consultation_id, section_name, integrated_confusion, comprehension_level
        )
        
        result = {
            'consultation_id': consultation_id,
            'section_name': section_name,
            'timestamp': datetime.now(timezone.utc).isoformat(),
            
            # ê°œë³„ ë¶„ì„ ê²°ê³¼
            'individual_scores': {
                'text_difficulty': text_difficulty,
                'text_confusion': text_confusion,
                'face_confusion': face_confusion,
                'gaze_confusion': gaze_confusion
            },
            
            # í†µí•© ë¶„ì„ ê²°ê³¼
            'integrated_confusion': integrated_confusion,
            'comprehension_level': comprehension_level,
            'need_ai_assistance': need_ai_help,
            'should_simplify_text': should_simplify,
            
            # ì¶”ì²œì‚¬í•­
            'recommendations': recommendations,
            
            # ë””ë²„ê·¸ ì •ë³´
            'debug_info': {
                'weights_used': self.weights,
                'thresholds': self.thresholds,
                'reading_time': reading_time
            }
        }
        
        logger.info(f"í†µí•© ë¶„ì„ ì™„ë£Œ - ìƒë‹´ID: {consultation_id}, "
                   f"ì„¹ì…˜: {section_name}, "
                   f"í†µí•© í˜¼ë€ë„: {integrated_confusion:.2f}, "
                   f"AI ë„ì›€ í•„ìš”: {need_ai_help}")
        
        return result
    
    def _calculate_text_confusion(self, difficulty: float, text: str) -> float:
        """í…ìŠ¤íŠ¸ ê¸°ë°˜ í˜¼ë€ë„ ê³„ì‚° - AI ëª¨ë¸(KLUE-BERT) ê²°ê³¼ ì‚¬ìš©"""
        # AI ë‚œì´ë„ ë¶„ì„ ëª¨ë¸(combe4259/difficulty_klue)ì´ ì´ë¯¸ í…ìŠ¤íŠ¸ ë‚œì´ë„ë¥¼ ë¶„ì„í–ˆìœ¼ë¯€ë¡œ
        # ì¶”ê°€ì ì¸ ìˆ˜ë™ ë³´ì • ì—†ì´ AI ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        return difficulty
    
    async def _calculate_face_confusion(self, face_data: Optional[Dict]) -> float:
        """ì–¼êµ´ í‘œì • ê¸°ë°˜ í˜¼ë€ë„ ê³„ì‚° - HuggingFace CNN-LSTM ê²°ê³¼ ì‚¬ìš©"""
        if not face_data:
            return 0.0

        # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ í”„ë ˆì„ ë°ì´í„°ê°€ ì˜¨ ê²½ìš° -> HuggingFace ëª¨ë¸ë¡œ ë¶„ì„
        frames = face_data.get('frames')
        if frames and len(frames) >= 30:
            try:
                from services.ai_model_service import ai_model_manager
                import base64
                import numpy as np
                import cv2

                logger.info(f"ğŸ“¹ ì–¼êµ´ í”„ë ˆì„ {len(frames)}ê°œ ìˆ˜ì‹  -> HuggingFace ëª¨ë¸ ë¶„ì„ ì‹œì‘")

                # HuggingFace confusion_tracker ì‚¬ìš©
                if ai_model_manager.hf_models and ai_model_manager.hf_models.confusion_tracker:
                    tracker = ai_model_manager.hf_models.confusion_tracker

                    # Base64 í”„ë ˆì„ë“¤ì„ numpy ë°°ì—´ë¡œ ë³€í™˜í•˜ê³  ìˆœì°¨ ì²˜ë¦¬
                    for frame_b64 in frames[:30]:
                        try:
                            # Base64 ë””ì½”ë”©
                            img_data = base64.b64decode(frame_b64.split(',')[1] if ',' in frame_b64 else frame_b64)
                            nparr = np.frombuffer(img_data, np.uint8)
                            frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

                            if frame is not None:
                                # process_frameìœ¼ë¡œ í”„ë ˆì„ ì²˜ë¦¬
                                tracker.process_frame(frame)
                        except Exception as e:
                            logger.warning(f"í”„ë ˆì„ ë””ì½”ë”© ì‹¤íŒ¨: {e}")
                            continue

                    # ìµœì¢… í˜¼ë€ë„ í™•ë¥  ê°€ì ¸ì˜¤ê¸°
                    confusion_prob = tracker.confusion_probability
                    logger.info(f"ğŸ§  HuggingFace ëª¨ë¸ ë¶„ì„ ì™„ë£Œ - Confusion: {confusion_prob:.2f}")
                    return float(confusion_prob)
                else:
                    logger.warning("HuggingFace confusion_trackerê°€ ì—†ìŒ")

            except Exception as e:
                logger.error(f"âŒ ì–¼êµ´ í”„ë ˆì„ ë¶„ì„ ì‹¤íŒ¨: {e}", exc_info=True)
                # ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë¡œì§ìœ¼ë¡œ í´ë°±

        # ì´ë¯¸ ë¶„ì„ëœ confusion ê°’ì´ ìˆëŠ” ê²½ìš° (ê¸°ì¡´ ë¡œì§)
        confusion_prob = face_data.get('confusion_probability', 0.0)

        # emotions ë”•ì…”ë„ˆë¦¬ì—ì„œë„ confusion í™•ì¸
        emotions = face_data.get('emotions', {})
        if emotions and 'confusion' in emotions:
            return emotions.get('confusion', confusion_prob)

        return confusion_prob
    
    def _calculate_gaze_confusion(self, gaze_data: Optional[Dict], 
                                  reading_time: Optional[float],
                                  text_length: int) -> float:
        """ì‹œì„  íŒ¨í„´ ê¸°ë°˜ í˜¼ë€ë„ ê³„ì‚°"""
        if not gaze_data:
            return 0.0
        
        confusion = 0.0
        
        # 1. ê³ ì • ì‹œê°„ ë¶„ì„
        fixation_duration = gaze_data.get('avg_fixation_duration', 250)
        if fixation_duration > 400:  # 400ms ì´ìƒì€ ì–´ë ¤ì›€
            confusion += 0.3
        elif fixation_duration > 300:
            confusion += 0.15
        
        # 2. íšŒê·€(ì¬ì½ê¸°) ë¶„ì„
        regression_count = gaze_data.get('regression_count', 0)
        if regression_count > 3:
            confusion += 0.3
        elif regression_count > 1:
            confusion += 0.15
        
        # 3. ì‹œì„  ë¶„ì‚°ë„
        dispersion = gaze_data.get('gaze_dispersion', 50)
        if dispersion > 150:  # ì‹œì„ ì´ ë§ì´ í©ì–´ì§
            confusion += 0.2
        elif dispersion > 100:
            confusion += 0.1
        
        # 4. ì½ê¸° ì†ë„ ë¶„ì„
        if reading_time and text_length > 0:
            # í‰ê·  ì½ê¸° ì†ë„: ë¶„ë‹¹ 200-250 ë‹¨ì–´ (í•œê¸€ ê¸°ì¤€ ì•½ ë¶„ë‹¹ 300-400ì)
            expected_time = text_length / 6  # ì´ˆë‹¹ ì•½ 6ê¸€ì
            if reading_time > expected_time * 1.5:
                confusion += 0.2
        
        # 5. ìŠ¤í‚µ íŒ¨í„´ (í…ìŠ¤íŠ¸ë¥¼ ê±´ë„ˆë›´ ê²½ìš°)
        skip_count = gaze_data.get('skip_count', 0)
        if skip_count > 2:
            confusion -= 0.1  # ìŠ¤í‚µì´ ë§ìœ¼ë©´ ì˜¤íˆë ¤ ê´€ì‹¬ ì—†ìŒ
        
        return max(0, min(confusion, 1.0))
    
    def _calculate_integrated_confusion(self, text: float, face: float, gaze: float) -> float:
        """í†µí•© í˜¼ë€ë„ ê³„ì‚° (ê°€ì¤‘ í‰ê·  + ë³´ì •)"""
        
        # ê¸°ë³¸ ê°€ì¤‘ í‰ê· 
        weighted_avg = (
            text * self.weights['text_difficulty'] +
            face * self.weights['face_confusion'] +
            gaze * self.weights['gaze_pattern']
        )
        
        # ê·¹ë‹¨ê°’ ë³´ì •: í•˜ë‚˜ë¼ë„ ë§¤ìš° ë†’ìœ¼ë©´ ì „ì²´ ìƒí–¥
        max_confusion = max(text, face, gaze)
        if max_confusion > 0.8:
            weighted_avg = max(weighted_avg, max_confusion * 0.85)
        
        # ì¼ì¹˜ë„ ë³´ë„ˆìŠ¤: ëª¨ë“  ì§€í‘œê°€ ë¹„ìŠ·í•˜ë©´ ì‹ ë¢°ë„ ë†’ìŒ
        variance = self._calculate_variance([text, face, gaze])
        if variance < 0.1:  # ì§€í‘œë“¤ì´ ì¼ì¹˜
            confidence_bonus = 0.05
            weighted_avg = min(weighted_avg + confidence_bonus, 1.0)
        
        return weighted_avg
    
    def _determine_comprehension_level(self, integrated: float, 
                                       face: float, gaze: float) -> Tuple[str, bool]:
        """ì´í•´ë„ ë ˆë²¨ ë° AI ë„ì›€ í•„ìš” ì—¬ë¶€ ê²°ì •"""
        
        need_ai = False
        
        if integrated > self.thresholds['high_confusion']:
            level = 'low'
            need_ai = True
        elif integrated > self.thresholds['moderate_confusion']:
            level = 'medium'
            # ì–¼êµ´ì´ë‚˜ ì‹œì„ ì´ íŠ¹íˆ í˜¼ë€ìŠ¤ëŸ¬ìš°ë©´ AI ë„ì›€
            if face > 0.7 or gaze > 0.7:
                need_ai = True
        elif integrated > self.thresholds['low_confusion']:
            level = 'medium'
        else:
            level = 'high'
        
        return level, need_ai
    
    def _should_trigger_simplification(self, integrated: float, face: float, 
                                       gaze: float, text_difficulty: float) -> bool:
        """ë¬¸ì¥ ê°„ì†Œí™” AI íŠ¸ë¦¬ê±° ì—¬ë¶€ ê²°ì •"""
        
        # ì¼€ì´ìŠ¤ 1: í†µí•© í˜¼ë€ë„ê°€ ì„ê³„ê°’ ì´ˆê³¼ (0.3ìœ¼ë¡œ ë‚®ì¶¤)
        if integrated > 0.3:
            return True
        
        # ì¼€ì´ìŠ¤ 2: ì–¼êµ´ í‘œì •ì´ í˜¼ë€ìŠ¤ëŸ½ê³  í…ìŠ¤íŠ¸ë„ ì–´ë ¤ì›€
        if face > 0.4 and text_difficulty > 0.4:
            return True
        
        # ì¼€ì´ìŠ¤ 3: ì‹œì„  íŒ¨í„´ì´ í˜¼ë€ìŠ¤ëŸ½ê³  í…ìŠ¤íŠ¸ë„ ì–´ë ¤ì›€  
        if gaze > 0.4 and text_difficulty > 0.4:
            return True
        
        # ì¼€ì´ìŠ¤ 4: ëª¨ë“  ì§€í‘œê°€ ë‚®ì€ ì¤‘ê°„ ì´ìƒ
        if face > 0.3 and gaze > 0.3 and text_difficulty > 0.3:
            return True
        
        return False
    
    def _generate_recommendations(self, level: str, text: float, 
                                  face: float, gaze: float) -> list:
        """ê°œì¸í™”ëœ ì¶”ì²œì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        if level == 'low':
            recommendations.append("ì´ ë¶€ë¶„ì´ ì–´ë ¤ìš°ì‹  ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì²œì²œíˆ ë‹¤ì‹œ ì„¤ëª…ë“œë¦´ê²Œìš”.")
            
            if text > 0.7:
                recommendations.append("ì „ë¬¸ ìš©ì–´ë¥¼ ì‰¬ìš´ ë§ë¡œ ë°”ê¿”ë“œë¦¬ê² ìŠµë‹ˆë‹¤.")
            if face > 0.7:
                recommendations.append("í˜¼ë€ìŠ¤ëŸ¬ì›Œ ë³´ì´ì‹œë„¤ìš”. ì§ˆë¬¸ ìˆìœ¼ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”.")
            if gaze > 0.7:
                recommendations.append("ì¤‘ìš”í•œ ë¶€ë¶„ì„ ë‹¤ì‹œ ê°•ì¡°í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.")
                
        elif level == 'medium':
            recommendations.append("ì´í•´ì— ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”.")
            
            if text > 0.5:
                recommendations.append("í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                
        else:  # high
            recommendations.append("ì˜ ì´í•´í•˜ê³  ê³„ì‹  ê²ƒ ê°™ìŠµë‹ˆë‹¤.")
        
        return recommendations
    
    def _save_analysis_history(self, consultation_id: str, section: str,
                               confusion: float, level: str):
        """ë¶„ì„ ì´ë ¥ ì €ì¥"""
        if consultation_id not in self.analysis_history:
            self.analysis_history[consultation_id] = []
        
        self.analysis_history[consultation_id].append({
            'section': section,
            'confusion': confusion,
            'level': level,
            'timestamp': datetime.now(timezone.utc).isoformat()
        })
    
    def _calculate_variance(self, values: list) -> float:
        """ë¶„ì‚° ê³„ì‚°"""
        if not values:
            return 0
        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / len(values)
        return variance
    
    def get_consultation_summary(self, consultation_id: str) -> Dict:
        """ìƒë‹´ ì „ì²´ ìš”ì•½"""
        if consultation_id not in self.analysis_history:
            return {}
        
        history = self.analysis_history[consultation_id]
        if not history:
            return {}
        
        avg_confusion = sum(h['confusion'] for h in history) / len(history)
        low_comprehension_count = sum(1 for h in history if h['level'] == 'low')
        
        return {
            'total_sections': len(history),
            'average_confusion': avg_confusion,
            'low_comprehension_sections': low_comprehension_count,
            'need_follow_up': low_comprehension_count > 2 or avg_confusion > 0.6
        }


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
integrated_analyzer = IntegratedAnalysisService()