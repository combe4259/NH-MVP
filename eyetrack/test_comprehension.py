import cv2
import numpy as np
from PIL import Image, ImageDraw, ImageFont
from collections import deque, defaultdict
import time
import os
from comprehension_analyzer import ComprehensionAnalyzer, RealTimeComprehensionMonitor
from text_gaze_analyzer import TextGazeAnalyzer

class ComprehensionTestSystem:
    def __init__(self):
        self.analyzer = TextGazeAnalyzer()
        self.comprehension = ComprehensionAnalyzer()
        self.monitor = RealTimeComprehensionMonitor()
        
        # ì‹œì„  ì¶”ì  ì„¤ì •
        self.h_scale = 200
        self.v_scale = 120
        self.offset_x = 0
        self.offset_y = 0
        self.gaze_history = deque(maxlen=5)
        
        # ì´í•´ë„ ì¸¡ì • ë°ì´í„°
        self.current_word = None
        self.word_start_time = None
        self.word_fixations = defaultdict(list)
        self.regression_count = 0
        self.total_words_read = []
        
    def create_test_text_image(self):
        """í…ŒìŠ¤íŠ¸ìš© í•œê¸€ í…ìŠ¤íŠ¸ ì´ë¯¸ì§€"""
        width, height = 1400, 900
        img = Image.new('RGB', (width, height), color='white')
        draw = ImageDraw.Draw(img)
        
        try:
            font_paths = [
                "/System/Library/Fonts/AppleSDGothicNeo.ttc",
                "/Library/Fonts/Arial Unicode.ttf",
            ]
            
            font_title = None
            font_body = None
            
            for font_path in font_paths:
                if os.path.exists(font_path):
                    try:
                        font_title = ImageFont.truetype(font_path, 32, index=0)
                        font_body = ImageFont.truetype(font_path, 24, index=0)
                        break
                    except:
                        continue
            
            if not font_title:
                font_title = ImageFont.load_default()
                font_body = ImageFont.load_default()
                
        except:
            font_title = ImageFont.load_default()
            font_body = ImageFont.load_default()
        
        # ì œëª©
        draw.text((50, 30), "ğŸ“– ë¬¸ë§¥ ì´í•´ë„ ì¸¡ì • í…ŒìŠ¤íŠ¸", font=font_title, fill='black')
        
        # ë³¸ë¬¸ (ê° ì¤„ì„ ë‹¨ì–´ë³„ë¡œ ë¶„ë¦¬ ê°€ëŠ¥í•˜ê²Œ)
        text_lines = [
            "",
            "InnoDB ë²„í¼ í’€ì€ ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ì˜ í•µì‹¬ì…ë‹ˆë‹¤.",
            "LRU ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ìì£¼ ì‚¬ìš©ë˜ëŠ” í˜ì´ì§€ë¥¼ ë©”ëª¨ë¦¬ì— ìœ ì§€í•©ë‹ˆë‹¤.",
            "",
            "ë²„í¼ í’€ì˜ í¬ê¸°ëŠ” ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ì˜ 70-80%ë¡œ ì„¤ì •í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤.",
            "ë„ˆë¬´ ì‘ìœ¼ë©´ ë””ìŠ¤í¬ I/Oê°€ ì¦ê°€í•˜ê³ , ë„ˆë¬´ í¬ë©´ ì‹œìŠ¤í…œì´ ë¶ˆì•ˆì •í•´ì§‘ë‹ˆë‹¤.",
            "",
            "ì–´ëŒ‘í‹°ë¸Œ í•´ì‹œ ì¸ë±ìŠ¤ëŠ” ìì£¼ ì ‘ê·¼í•˜ëŠ” ë°ì´í„°ì— ëŒ€í•œ í•´ì‹œ í…Œì´ë¸”ì…ë‹ˆë‹¤.",
            "B-Tree ì¸ë±ìŠ¤ íƒìƒ‰ì„ ê±´ë„ˆë›°ê³  ì§ì ‘ ë°ì´í„°ì— ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "",
            "MRUëŠ” Most Recently Usedì˜ ì•½ìë¡œ ê°€ì¥ ìµœê·¼ ì‚¬ìš©ëœ í˜ì´ì§€ì…ë‹ˆë‹¤.",
            "LRUëŠ” Least Recently Usedì˜ ì•½ìë¡œ ê°€ì¥ ì˜¤ë˜ ì‚¬ìš©ë˜ì§€ ì•Šì€ í˜ì´ì§€ì…ë‹ˆë‹¤."
        ]
        
        # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸° ë° ìœ„ì¹˜ ì €ì¥
        y_offset = 100
        line_height = 40
        self.text_regions = []  # ê° ë‹¨ì–´ì˜ ìœ„ì¹˜ ì €ì¥
        
        for line in text_lines:
            if line:
                words = line.split()
                x_offset = 50
                for word in words:
                    # ë‹¨ì–´ í¬ê¸° ê³„ì‚° (textbbox ëŒ€ì‹  ê°„ë‹¨í•œ ì¶”ì •)
                    word_width = len(word) * 15  # í•œ ê¸€ìë‹¹ ì•½ 15í”½ì…€
                    word_height = 30  # í°íŠ¸ ë†’ì´
                    
                    draw.text((x_offset, y_offset), word, font=font_body, fill='black')
                    
                    # ë‹¨ì–´ ì˜ì—­ ì €ì¥ (x, y, width, height)
                    self.text_regions.append({
                        'word': word,
                        'bbox': (x_offset, y_offset, word_width, word_height)
                    })
                    
                    # ë””ë²„ê¹…ìš© - ë‹¨ì–´ ì˜ì—­ í‘œì‹œ (í™œì„±í™”)
                    draw.rectangle([x_offset, y_offset, x_offset + word_width, y_offset + word_height], 
                                   outline='lightgray', width=1)
                    
                    x_offset += word_width + 15  # ë‹¤ìŒ ë‹¨ì–´ ìœ„ì¹˜
            
            y_offset += line_height
        
        return np.array(img)
    
    def get_word_at_gaze(self, gaze_x, gaze_y):
        """ì‹œì„  ìœ„ì¹˜ì˜ ë‹¨ì–´ ì°¾ê¸°"""
        for region in self.text_regions:
            x, y, w, h = region['bbox']
            # ì˜ì—­ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸ (ì—¬ìœ  ê³µê°„ ì¶”ê°€)
            if (x - 5) <= gaze_x <= (x + w + 5) and (y - 5) <= gaze_y <= (y + h + 5):
                return region['word']
        return None
    
    def smooth_gaze(self, x, y):
        """ì‹œì„  ìŠ¤ë¬´ë”©"""
        self.gaze_history.append((x, y))
        if len(self.gaze_history) < 2:
            return x, y
        
        weights = [0.1, 0.2, 0.3, 0.3, 0.1][-len(self.gaze_history):]
        total_weight = sum(weights)
        
        smoothed_x = sum(p[0] * w for p, w in zip(self.gaze_history, weights)) / total_weight
        smoothed_y = sum(p[1] * w for p, w in zip(self.gaze_history, weights)) / total_weight
        
        return int(smoothed_x), int(smoothed_y)
    
    def calculate_gaze(self, landmarks, frame_shape):
        """ì‹œì„  ìœ„ì¹˜ ê³„ì‚°"""
        left_eye_center = self.analyzer.gaze_tracker.get_eye_center(
            landmarks, self.analyzer.gaze_tracker.LEFT_EYE_INDICES, frame_shape)
        right_eye_center = self.analyzer.gaze_tracker.get_eye_center(
            landmarks, self.analyzer.gaze_tracker.RIGHT_EYE_INDICES, frame_shape)
        
        left_iris = self.analyzer.gaze_tracker.get_iris_position(
            landmarks, self.analyzer.gaze_tracker.LEFT_IRIS_INDICES, frame_shape)
        right_iris = self.analyzer.gaze_tracker.get_iris_position(
            landmarks, self.analyzer.gaze_tracker.RIGHT_IRIS_INDICES, frame_shape)
        
        left_gaze_vector = left_iris - left_eye_center
        right_gaze_vector = right_iris - right_eye_center
        avg_gaze_vector = (left_gaze_vector + right_gaze_vector) / 2
        
        face_center_x = landmarks.landmark[9].x * frame_shape[1]
        face_center_y = landmarks.landmark[9].y * frame_shape[0]
        
        eye_distance = np.linalg.norm(left_eye_center - right_eye_center)
        scale_factor = 150 / max(eye_distance, 50)
        
        gaze_x = face_center_x - avg_gaze_vector[0] * self.h_scale * scale_factor + self.offset_x
        gaze_y = face_center_y + avg_gaze_vector[1] * self.v_scale * scale_factor + self.offset_y
        
        gaze_x, gaze_y = self.smooth_gaze(gaze_x, gaze_y)
        
        gaze_x = np.clip(gaze_x, 0, frame_shape[1] - 1)
        gaze_y = np.clip(gaze_y, 0, frame_shape[0] - 1)
        
        return int(gaze_x), int(gaze_y)
    
    def run(self):
        text_image = self.create_test_text_image()
        
        cap = cv2.VideoCapture(0)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
        
        cv2.namedWindow('Camera', cv2.WINDOW_NORMAL)
        cv2.namedWindow('Reading Test', cv2.WINDOW_NORMAL)
        cv2.namedWindow('Comprehension Dashboard', cv2.WINDOW_NORMAL)
        
        print("=" * 60)
        print("ë¬¸ë§¥ ì´í•´ë„ ì¸¡ì • ì‹œìŠ¤í…œ")
        print("=" * 60)
        print("í…ìŠ¤íŠ¸ë¥¼ ì½ìœ¼ë©´ì„œ ì´í•´ë„ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¸¡ì •í•©ë‹ˆë‹¤")
        print("Q - ì¢…ë£Œ | R - ë¦¬ì…‹ | S - ë¦¬í¬íŠ¸ ì €ì¥")
        print("=" * 60)
        
        gaze_trail = deque(maxlen=30)
        heatmap = np.zeros(text_image.shape[:2], dtype=np.float32)
        session_start = time.time()
        last_word = None
        last_word_time = time.time()
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame = cv2.flip(frame, 1)
            current_time = time.time()
            
            landmarks = self.analyzer.gaze_tracker.detect_eyes(frame)
            
            display_image = text_image.copy()
            
            if landmarks:
                gaze_x, gaze_y = self.calculate_gaze(landmarks, text_image.shape)
                
                # ì‹œì„ ì´ ë³´ê³  ìˆëŠ” ë‹¨ì–´ ì°¾ê¸°
                current_word = self.get_word_at_gaze(gaze_x, gaze_y)
                
                if current_word:
                    # ë‹¨ì–´ ì‘ì‹œ ì‹œê°„ ì¸¡ì •
                    if current_word != last_word:
                        if last_word:
                            fixation_time = (current_time - last_word_time) * 1000
                            self.word_fixations[last_word].append(fixation_time)
                            
                            # ì´í•´ë„ ë¶„ì„
                            difficulty = self.comprehension.analyze_fixation_pattern(
                                last_word, fixation_time, (gaze_x, gaze_y)
                            )
                            
                            # íšŒê·€ ê²€ì‚¬
                            if last_word in self.total_words_read:
                                self.regression_count += 1
                        
                        self.total_words_read.append(current_word)
                        last_word = current_word
                        last_word_time = current_time
                        
                        # ë””ë²„ê¹…: ë‹¨ì–´ ê°ì§€ ë¡œê·¸
                        print(f"Looking at: {current_word}")
                    
                    # í˜„ì¬ ë³´ê³  ìˆëŠ” ë‹¨ì–´ í•˜ì´ë¼ì´íŠ¸
                    for region in self.text_regions:
                        if region['word'] == current_word:
                            x, y, w, h = region['bbox']
                            cv2.rectangle(display_image, (x, y), (x+w, y+h), 
                                        (0, 255, 255), 3)  # ë…¸ë€ìƒ‰ìœ¼ë¡œ ê°•ì¡°
                
                # ì‹œì„  ê¶¤ì 
                gaze_trail.append((gaze_x, gaze_y))
                cv2.circle(heatmap, (gaze_x, gaze_y), 20, 1.0, -1)
                
                for i in range(1, len(gaze_trail)):
                    alpha = i / len(gaze_trail)
                    cv2.line(display_image, gaze_trail[i-1], gaze_trail[i],
                            (0, int(255 * alpha), 0), max(1, int(3 * alpha)))
                
                # ì‹œì„  í¬ì¸í„°
                cv2.circle(display_image, (gaze_x, gaze_y), 15, (0, 255, 0), -1)
                cv2.circle(display_image, (gaze_x, gaze_y), 20, (0, 255, 0), 2)
            
            # ëŒ€ì‹œë³´ë“œ ìƒì„±
            dashboard = self.create_dashboard()
            
            # ì‹¤ì‹œê°„ í”¼ë“œë°±
            feedback = self.monitor.get_realtime_feedback()
            cv2.putText(display_image, feedback['message'], (50, display_image.shape[0] - 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, feedback['color'], 2)
            
            cv2.imshow('Camera', frame)
            cv2.imshow('Reading Test', display_image)
            cv2.imshow('Comprehension Dashboard', dashboard)
            
            key = cv2.waitKey(1) & 0xFF
            
            if key == ord('q'):
                break
            elif key == ord('r'):
                self.word_fixations.clear()
                self.total_words_read.clear()
                self.regression_count = 0
                gaze_trail.clear()
                heatmap = np.zeros(text_image.shape[:2], dtype=np.float32)
                print("Reset!")
            elif key == ord('s'):
                self.save_report()
                print("Report saved!")
            elif key == ord('w'):
                self.offset_y -= 20
            elif key == ord('s'):
                self.offset_y += 20
            elif key == ord('a'):
                self.offset_x -= 20
            elif key == ord('d'):
                self.offset_x += 20
        
        # ìµœì¢… ë¦¬í¬íŠ¸
        self.generate_final_report()
        
        cap.release()
        cv2.destroyAllWindows()
    
    def create_dashboard(self):
        """ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ"""
        dashboard = np.zeros((400, 600, 3), dtype=np.uint8)
        dashboard.fill(30)
        
        # ì œëª©
        cv2.putText(dashboard, "COMPREHENSION METRICS", (150, 40),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
        
        y_offset = 80
        
        # ì½ì€ ë‹¨ì–´ ìˆ˜
        words_read = len(set(self.total_words_read))
        cv2.putText(dashboard, f"Words Read: {words_read}", (30, y_offset),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 1)
        y_offset += 30
        
        # íšŒê·€ íšŸìˆ˜
        cv2.putText(dashboard, f"Regressions: {self.regression_count}", (30, y_offset),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 1)
        y_offset += 30
        
        # ì–´ë ¤ìš´ ë‹¨ì–´ (ì‘ì‹œ ì‹œê°„ ê¸´ ìˆœì„œ)
        difficult_words = []
        for word, times in self.word_fixations.items():
            if times:
                avg_time = np.mean(times)
                if avg_time > 500:  # 500ms ì´ìƒ
                    difficult_words.append((word, avg_time))
        
        difficult_words.sort(key=lambda x: x[1], reverse=True)
        
        cv2.putText(dashboard, "Difficult Words:", (30, y_offset),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 100, 100), 1)
        y_offset += 25
        
        for word, avg_time in difficult_words[:5]:
            cv2.putText(dashboard, f"  {word}: {avg_time:.0f}ms", (30, y_offset),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
            y_offset += 20
        
        # ì´í•´ë„ ì ìˆ˜ ê³„ì‚°
        if self.word_fixations:
            all_times = []
            for times in self.word_fixations.values():
                all_times.extend(times)
            
            if all_times:
                avg_fixation = np.mean(all_times)
                comprehension_score = max(0, min(100, 100 - (avg_fixation - 200) / 10))
                
                # ì ìˆ˜ í‘œì‹œ
                y_offset += 20
                cv2.putText(dashboard, f"Comprehension Score: {comprehension_score:.1f}%",
                           (30, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
                
                # ë§‰ëŒ€ ê·¸ë˜í”„
                bar_width = int(comprehension_score * 5)
                bar_color = (0, 255, 0) if comprehension_score > 70 else \
                           (255, 255, 0) if comprehension_score > 40 else (255, 0, 0)
                cv2.rectangle(dashboard, (30, y_offset + 10), 
                             (30 + bar_width, y_offset + 30), bar_color, -1)
        
        return dashboard
    
    def save_report(self):
        """ë¦¬í¬íŠ¸ ì €ì¥"""
        report = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'words_read': len(set(self.total_words_read)),
            'total_words': len(self.total_words_read),
            'regression_count': self.regression_count,
            'word_fixations': dict(self.word_fixations),
            'difficult_words': []
        }
        
        for word, times in self.word_fixations.items():
            if times and np.mean(times) > 500:
                report['difficult_words'].append({
                    'word': word,
                    'avg_time': np.mean(times),
                    'count': len(times)
                })
        
        # JSONìœ¼ë¡œ ì €ì¥
        import json
        with open('comprehension_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
    
    def generate_final_report(self):
        """ìµœì¢… ì´í•´ë„ ë¦¬í¬íŠ¸"""
        print("\n" + "=" * 60)
        print("ìµœì¢… ì´í•´ë„ ë¶„ì„ ë¦¬í¬íŠ¸")
        print("=" * 60)
        
        if self.word_fixations:
            # ì „ì²´ í†µê³„
            all_times = []
            for times in self.word_fixations.values():
                all_times.extend(times)
            
            if all_times:
                print(f"í‰ê·  ì‘ì‹œ ì‹œê°„: {np.mean(all_times):.0f}ms")
                print(f"íšŒê·€ íšŸìˆ˜: {self.regression_count}")
                print(f"ì½ì€ ê³ ìœ  ë‹¨ì–´ ìˆ˜: {len(set(self.total_words_read))}")
                
                # ì´í•´ë„ íŒì •
                avg_time = np.mean(all_times)
                if avg_time < 300:
                    print("ì´í•´ë„: ë†’ìŒ âœ…")
                elif avg_time < 600:
                    print("ì´í•´ë„: ë³´í†µ âš ï¸")
                else:
                    print("ì´í•´ë„: ë‚®ìŒ âŒ")
                
                # ì–´ë ¤ìš´ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
                print("\nì–´ë ¤ì› ë˜ ë‹¨ì–´ë“¤:")
                difficult = []
                for word, times in self.word_fixations.items():
                    if times and np.mean(times) > 500:
                        difficult.append((word, np.mean(times)))
                
                difficult.sort(key=lambda x: x[1], reverse=True)
                for word, avg_time in difficult[:5]:
                    print(f"  - {word}: {avg_time:.0f}ms")
        
        print("=" * 60)

if __name__ == "__main__":
    system = ComprehensionTestSystem()
    system.run()