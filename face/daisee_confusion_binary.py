"""
DAiSEE Confusion ì´ì§„ ë¶„ë¥˜ ëª¨ë¸
Confusion 0 vs 1~3 (ì—†ìŒ vs ìˆìŒ)ë§Œ í•™ìŠµ
"""

import os
import cv2
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
from typing import List, Tuple, Dict
from tqdm import tqdm
import glob

# GPU ì„¤ì • (Mac M3 Proì˜ ê²½ìš° MPS ì‚¬ìš©)
if torch.cuda.is_available():
    device = torch.device("cuda")
elif torch.backends.mps.is_available():
    device = torch.device("mps")
else:
    device = torch.device("cpu")
print(f"Using device: {device}")

class LocalDAiSEEConfusionDataset(Dataset):
    """ë¡œì»¬ DAiSEE ë°ì´í„°ì…‹ - Confusion ì´ì§„ ë¶„ë¥˜ìš©"""
    
    def __init__(self, 
                 data_root: str = "/Users/inter4259/Desktop/DAiSEE",
                 subset: str = 'Train',
                 sequence_length: int = 30,
                 image_size: Tuple[int, int] = (112, 112),
                 max_samples: int = None):
        """
        Args:
            data_root: DAiSEE ë£¨íŠ¸ í´ë”
            subset: 'Train', 'Test', 'Validation'
            sequence_length: ë¹„ë””ì˜¤ì—ì„œ ì¶”ì¶œí•  í”„ë ˆì„ ìˆ˜
            image_size: ì´ë¯¸ì§€ í¬ê¸°
            max_samples: ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (í…ŒìŠ¤íŠ¸ìš©)
        """
        self.data_root = data_root
        self.subset = subset
        self.sequence_length = sequence_length
        self.image_size = image_size
        
        # ë¼ë²¨ íŒŒì¼ ê²½ë¡œ
        labels_file = os.path.join(data_root, "Labels", f"{subset}Labels.csv")
        print(f"Loading labels from {labels_file}...")
        self.labels_df = pd.read_csv(labels_file)
        
        # ì»¬ëŸ¼ëª… ê³µë°± ì œê±°
        self.labels_df.columns = self.labels_df.columns.str.strip()
        print(f"Columns: {self.labels_df.columns.tolist()}")
        
        # ë¹„ë””ì˜¤ ê²½ë¡œì™€ ë¼ë²¨ ë§¤ì¹­
        self.video_paths = []
        self.labels = []
        
        video_dir = os.path.join(data_root, "DataSet", subset)
        print(f"Searching for videos in {video_dir}...")
        
        # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ë¹„ë””ì˜¤ íŒŒì¼ ì°¾ê¸°
        found_count = 0
        missing_count = 0
        
        # Confusion ë¶„í¬ ì¹´ìš´íŠ¸
        confusion_counts = {0: 0, 1: 0}
        
        for idx, row in tqdm(self.labels_df.iterrows(), total=len(self.labels_df), desc="Loading videos"):
            clip_id = row['ClipID']
            # .avi í™•ì¥ì ì œê±°
            if clip_id.endswith('.avi'):
                clip_id = clip_id[:-4]
            
            # ì‚¬ìš©ìIDì™€ ë¹„ë””ì˜¤ID ë¶„ë¦¬ (ì˜ˆ: 1100011002 -> 110001, 1002)
            user_id = clip_id[:-4]  # ë§ˆì§€ë§‰ 4ìë¦¬ ì œì™¸
            video_id = clip_id[-4:]  # ë§ˆì§€ë§‰ 4ìë¦¬
            full_video_id = user_id + video_id
            
            # ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            video_path = os.path.join(video_dir, user_id, full_video_id, f"{full_video_id}.avi")
            
            if os.path.exists(video_path):
                self.video_paths.append(video_path)
                
                # Confusionì„ ì´ì§„ ë¼ë²¨ë¡œ ë³€í™˜ (0 â†’ 0, 1~3 â†’ 1)
                confusion_level = row['Confusion']
                binary_label = 1 if confusion_level > 0 else 0
                self.labels.append(binary_label)
                
                confusion_counts[binary_label] += 1
                found_count += 1
                
                if max_samples and found_count >= max_samples:
                    break
            else:
                missing_count += 1
        
        print(f"âœ… Found {found_count} videos")
        if missing_count > 0:
            print(f"âš ï¸ Missing {missing_count} videos")
        
        # ì´ì§„ ë¶„ë¥˜ ë¶„í¬ ì¶œë ¥
        total = sum(confusion_counts.values())
        if total > 0:
            print(f"\nğŸ“Š Confusion Binary Distribution:")
            print(f"  Not Confused (0): {confusion_counts[0]} ({confusion_counts[0]/total*100:.1f}%)")
            print(f"  Confused (1~3):   {confusion_counts[1]} ({confusion_counts[1]/total*100:.1f}%)")
        
        # ì „ì²˜ë¦¬ ë³€í™˜
        self.transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize(image_size),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                              std=[0.229, 0.224, 0.225])
        ])
    
    def __len__(self):
        return len(self.video_paths)
    
    def __getitem__(self, idx):
        """ë¹„ë””ì˜¤ì™€ ë¼ë²¨ ë¡œë“œ"""
        video_path = self.video_paths[idx]
        label = self.labels[idx]
        
        # ë¹„ë””ì˜¤ ë¡œë“œ
        frames = self.load_video(video_path)
        
        return frames, label
    
    def load_video(self, video_path: str) -> torch.Tensor:
        """ë¹„ë””ì˜¤ íŒŒì¼ ë¡œë“œ ë° í”„ë ˆì„ ì¶”ì¶œ"""
        cap = cv2.VideoCapture(video_path)
        
        # ì „ì²´ í”„ë ˆì„ ìˆ˜ í™•ì¸
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        if total_frames == 0:
            print(f"Warning: Cannot read video {video_path}")
            cap.release()
            return torch.randn(self.sequence_length, 3, *self.image_size)
        
        # ê· ë“± ê°„ê²©ìœ¼ë¡œ í”„ë ˆì„ ì¸ë±ìŠ¤ ì„ íƒ
        if total_frames < self.sequence_length:
            indices = list(range(total_frames)) * (self.sequence_length // total_frames + 1)
            indices = indices[:self.sequence_length]
        else:
            indices = np.linspace(0, total_frames-1, self.sequence_length, dtype=int)
        
        frames = []
        for frame_idx in indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            
            if ret and frame is not None:
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frame = self.transform(frame)
                frames.append(frame)
            else:
                if frames:
                    frames.append(frames[-1])
                else:
                    frames.append(torch.zeros(3, *self.image_size))
        
        cap.release()
        
        return torch.stack(frames)


class DAiSEEConfusionNet(nn.Module):
    """CNN-LSTM ëª¨ë¸ - Confusion ì´ì§„ ë¶„ë¥˜ìš©"""
    
    def __init__(self, hidden_dim=256, num_layers=2):
        super(DAiSEEConfusionNet, self).__init__()
        
        # MobileNetV2 ë°±ë³¸
        self.cnn = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)
        self.cnn.classifier = nn.Identity()
        self.feature_dim = 1280
        
        # ì¼ë¶€ ë ˆì´ì–´ ê³ ì •
        for param in list(self.cnn.parameters())[:-10]:
            param.requires_grad = False
        
        # LSTM
        self.lstm = nn.LSTM(
            input_size=self.feature_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.3 if num_layers > 1 else 0
        )
        
        # Attention
        self.attention = nn.Sequential(
            nn.Linear(hidden_dim, 128),
            nn.Tanh(),
            nn.Linear(128, 1),
            nn.Softmax(dim=1)
        )
        
        # ì´ì§„ ë¶„ë¥˜ í—¤ë“œ (2 í´ë˜ìŠ¤: Not Confused, Confused)
        self.classifier = nn.Linear(hidden_dim, 2)
        
        self.dropout = nn.Dropout(0.3)
    
    def forward(self, x):
        batch_size, seq_len, c, h, w = x.size()
        
        # CNN íŠ¹ì§• ì¶”ì¶œ
        x = x.view(-1, c, h, w)
        features = self.cnn(x)
        features = features.view(batch_size, seq_len, -1)
        
        # LSTM
        lstm_out, _ = self.lstm(features)
        
        # Attention
        attention_weights = self.attention(lstm_out)
        attended = torch.sum(lstm_out * attention_weights, dim=1)
        
        # Dropout
        attended = self.dropout(attended)
        
        # ì´ì§„ ë¶„ë¥˜
        output = self.classifier(attended)
        
        return output


def train_confusion_model(num_epochs=10, batch_size=8, max_samples=None):
    """Confusion ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ"""
    
    print("=" * 50)
    print("Confusion Binary Classification Training")
    print("(0: Not Confused vs 1: Confused)")
    print("=" * 50)
    
    # ë°ì´í„°ì…‹ ìƒì„±
    print("\nğŸ“Š Loading datasets...")
    train_dataset = LocalDAiSEEConfusionDataset(
        subset='Train',
        max_samples=max_samples
    )
    
    val_dataset = LocalDAiSEEConfusionDataset(
        subset='Validation',
        max_samples=min(500, max_samples) if max_samples else None  # None = ì „ì²´ ì‚¬ìš©
    )
    
    # DataLoader ìƒì„±
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True,
        num_workers=0
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size * 2,
        shuffle=False,
        num_workers=0
    )
    
    print(f"Train samples: {len(train_dataset)}")
    print(f"Validation samples: {len(val_dataset)}")
    
    # ëª¨ë¸ ìƒì„±
    print("\nğŸ¤– Creating model...")
    model = DAiSEEConfusionNet().to(device)
    
    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ (ë¶ˆê· í˜• ë³´ì •)
    # Not Confused: 67.5%, Confused: 32.5% â†’ ê°€ì¤‘ì¹˜ ë¹„ìœ¨ 1:2.1
    class_weights = torch.tensor([1.0, 2.1]).to(device)
    criterion = nn.CrossEntropyLoss(weight=class_weights)
    
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', patience=3, factor=0.5
    )
    
    # í•™ìŠµ
    print("\nğŸš€ Starting training...")
    best_val_acc = 0
    
    for epoch in range(num_epochs):
        # Training
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")
        
        for batch_idx, (frames, labels) in enumerate(progress_bar):
            frames = frames.to(device)
            labels = labels.to(device)
            
            # Forward
            outputs = model(frames)
            loss = criterion(outputs, labels)
            
            # Backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # ì •í™•ë„ ê³„ì‚°
            _, predicted = torch.max(outputs, 1)
            train_total += labels.size(0)
            train_correct += (predicted == labels).sum().item()
            
            train_loss += loss.item()
            
            # Progress bar ì—…ë°ì´íŠ¸
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'acc': f'{train_correct/train_total*100:.2f}%'
            })
        
        # Validation
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0
        
        # Confusion Matrix ê³„ì‚°ìš©
        true_positives = 0
        false_positives = 0
        true_negatives = 0
        false_negatives = 0
        
        with torch.no_grad():
            for frames, labels in tqdm(val_loader, desc="Validation"):
                frames = frames.to(device)
                labels = labels.to(device)
                
                outputs = model(frames)
                loss = criterion(outputs, labels)
                
                _, predicted = torch.max(outputs, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()
                val_loss += loss.item()
                
                # Confusion Matrix ê³„ì‚°
                for i in range(labels.size(0)):
                    if labels[i] == 1 and predicted[i] == 1:
                        true_positives += 1
                    elif labels[i] == 0 and predicted[i] == 1:
                        false_positives += 1
                    elif labels[i] == 0 and predicted[i] == 0:
                        true_negatives += 1
                    elif labels[i] == 1 and predicted[i] == 0:
                        false_negatives += 1
        
        # í‰ê·  ê³„ì‚°
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        train_acc = train_correct / train_total * 100
        val_acc = val_correct / val_total * 100
        
        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        print(f"\nEpoch {epoch+1}/{num_epochs}")
        print(f"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%")
        print(f"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%")
        print(f"Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}")
        
        # Confusion Matrix
        print(f"\nConfusion Matrix:")
        print(f"              Predicted")
        print(f"              No  Yes")
        print(f"Actual No  [{true_negatives:4d} {false_positives:4d}]")
        print(f"       Yes [{false_negatives:4d} {true_positives:4d}]")
        
        # Learning rate ì¡°ì •
        scheduler.step(avg_val_loss)
        
        # ìµœê³  ëª¨ë¸ ì €ì¥
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), 'confusion_binary_model.pth')
            print(f"âœ… Best model saved (Val Acc: {val_acc:.2f}%)")
    
    print(f"\nâœ… Training completed. Best validation accuracy: {best_val_acc:.2f}%")
    return model


def test_confusion_model(model_path='confusion_binary_model.pth', max_samples=None):
    """í•™ìŠµëœ ëª¨ë¸ì„ Test ë°ì´í„°ë¡œ ìµœì¢… í‰ê°€"""
    
    print("\n" + "=" * 50)
    print("ğŸ§ª Testing Confusion Binary Model")
    print("=" * 50)
    
    # Test ë°ì´í„°ì…‹ ë¡œë“œ
    test_dataset = LocalDAiSEEConfusionDataset(
        subset='Test',
        max_samples=max_samples
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=8,
        shuffle=False,
        num_workers=0
    )
    
    print(f"Test samples: {len(test_dataset)}")
    
    # ëª¨ë¸ ë¡œë“œ
    model = DAiSEEConfusionNet().to(device)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()
    print(f"âœ… Model loaded from {model_path}")
    
    # í‰ê°€
    correct = 0
    total = 0
    true_positives = 0
    false_positives = 0
    true_negatives = 0
    false_negatives = 0
    
    with torch.no_grad():
        for frames, labels in tqdm(test_loader, desc="Testing"):
            frames = frames.to(device)
            labels = labels.to(device)
            
            outputs = model(frames)
            _, predicted = torch.max(outputs, 1)
            
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            
            # Confusion Matrix
            for i in range(labels.size(0)):
                if labels[i] == 1 and predicted[i] == 1:
                    true_positives += 1
                elif labels[i] == 0 and predicted[i] == 1:
                    false_positives += 1
                elif labels[i] == 0 and predicted[i] == 0:
                    true_negatives += 1
                elif labels[i] == 1 and predicted[i] == 0:
                    false_negatives += 1
    
    # ê²°ê³¼ ì¶œë ¥
    accuracy = correct / total * 100
    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    
    print("\nğŸ“Š Test Results:")
    print("-" * 30)
    print(f"Accuracy:  {accuracy:.2f}%")
    print(f"Precision: {precision:.3f}")
    print(f"Recall:    {recall:.3f}")
    print(f"F1 Score:  {f1:.3f}")
    
    print(f"\nConfusion Matrix:")
    print(f"              Predicted")
    print(f"              No  Yes")
    print(f"Actual No  [{true_negatives:4d} {false_positives:4d}]")
    print(f"       Yes [{false_negatives:4d} {true_positives:4d}]")
    
    return accuracy


if __name__ == "__main__":
    print("Confusion Binary Classification Model")
    print("=" * 50)
    
    # ì˜µì…˜ ì„ íƒ
    print("\n1. Quick test (100 samples)")
    print("2. Medium training (1000 samples)")
    print("3. Full training (all samples)")
    print("4. Test existing model")
    
    choice = input("\nSelect (1/2/3/4): ").strip()
    
    if choice == '1':
        model = train_confusion_model(num_epochs=3, batch_size=4, max_samples=100)
        test_confusion_model(max_samples=50)
        
    elif choice == '2':
        model = train_confusion_model(num_epochs=5, batch_size=8, max_samples=1000)
        test_confusion_model(max_samples=200)
        
    elif choice == '3':
        model = train_confusion_model(num_epochs=10, batch_size=16, max_samples=None)
        test_confusion_model()
        
    elif choice == '4':
        if os.path.exists('confusion_binary_model.pth'):
            test_confusion_model()
        else:
            print("âŒ No saved model found. Train first!")
    else:
        print("Invalid choice")