"""
ì‹¤ì‹œê°„ DAiSEE ê¸°ë°˜ ì–¼êµ´ ì´í•´ë„ ì¶”ì 
ì›¹ìº ì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ engagement, confusion, frustration, boredom ë ˆë²¨ ì˜ˆì¸¡
"""

import cv2
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
import torchvision.transforms as transforms
import numpy as np
from collections import deque
import time
import os
import mediapipe as mp

# GPU ì„¤ì •
if torch.cuda.is_available():
    device = torch.device("cuda")
elif torch.backends.mps.is_available():
    device = torch.device("mps")
else:
    device = torch.device("cpu")
print(f"Using device: {device}")


class DAiSEECNNLSTM(nn.Module):
    """CNN-LSTM ëª¨ë¸ """
    
    def __init__(self, hidden_dim=256, num_layers=2):
        super(DAiSEECNNLSTM, self).__init__()
        
        # MobileNetV2 ë°±ë³¸
        self.cnn = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)
        self.cnn.classifier = nn.Identity()
        self.feature_dim = 1280
        
        # ì¼ë¶€ ë ˆì´ì–´ ê³ ì •
        for param in list(self.cnn.parameters())[:-10]:
            param.requires_grad = False
        
        # LSTM
        self.lstm = nn.LSTM(
            input_size=self.feature_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.3 if num_layers > 1 else 0
        )
        
        # Attention
        self.attention = nn.Sequential(
            nn.Linear(hidden_dim, 128),
            nn.Tanh(),
            nn.Linear(128, 1),
            nn.Softmax(dim=1)
        )
        
        # ë¶„ë¥˜ í—¤ë“œ
        self.classifiers = nn.ModuleDict({
            'engagement': nn.Linear(hidden_dim, 4),
            'confusion': nn.Linear(hidden_dim, 4),
            'frustration': nn.Linear(hidden_dim, 4),
            'boredom': nn.Linear(hidden_dim, 4)
        })
        
        self.dropout = nn.Dropout(0.4)
    
    def forward(self, x):
        batch_size, seq_len, c, h, w = x.size()
        
        # CNN íŠ¹ì§• ì¶”ì¶œ
        x = x.view(-1, c, h, w)
        features = self.cnn(x)
        features = features.view(batch_size, seq_len, -1)
        
        # LSTM
        lstm_out, _ = self.lstm(features)
        
        # Attention
        attention_weights = self.attention(lstm_out)
        attended = torch.sum(lstm_out * attention_weights, dim=1)
        
        # Dropout
        attended = self.dropout(attended)
        
        # ë¶„ë¥˜
        outputs = {}
        for name, classifier in self.classifiers.items():
            outputs[name] = classifier(attended)
        
        return outputs


class RealtimeDAiSEETracker:
    """ì‹¤ì‹œê°„ DAiSEE ê¸°ë°˜ ì¶”ì ê¸°"""
    
    def __init__(self, model_path='daisee_local_model.pth', 
                 sequence_length=30, 
                 buffer_size=30,
                 prediction_interval=0.5):
        """
        Args:
            model_path: í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
            sequence_length: ëª¨ë¸ì— ì…ë ¥í•  í”„ë ˆì„ ìˆ˜
            buffer_size: ë²„í¼ í¬ê¸°
            prediction_interval: ì˜ˆì¸¡ ì£¼ê¸° (ì´ˆ)
        """
        self.sequence_length = sequence_length
        self.buffer_size = buffer_size
        self.prediction_interval = prediction_interval
        
        # í”„ë ˆì„ ë²„í¼ (dequeë¡œ ìë™ìœ¼ë¡œ ì˜¤ë˜ëœ í”„ë ˆì„ ì œê±°)
        self.frame_buffer = deque(maxlen=buffer_size)
        
        # ëª¨ë¸ ë¡œë“œ
        print(f"Loading model from {model_path}...")
        self.model = DAiSEECNNLSTM().to(device)
        
        if os.path.exists(model_path):
            self.model.load_state_dict(torch.load(model_path, map_location=device))
            self.model.eval()
            print("âœ… Model loaded successfully")
        else:
            print("âš ï¸ No trained model found. Using random initialization.")
        
        # ì „ì²˜ë¦¬ ë³€í™˜
        self.transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((112, 112)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                              std=[0.229, 0.224, 0.225])
        ])
        
        # MediaPipe ì–¼êµ´ ê°ì§€ ë° ë©”ì‹œ
        self.mp_face_detection = mp.solutions.face_detection
        self.mp_face_mesh = mp.solutions.face_mesh
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_drawing_styles = mp.solutions.drawing_styles
        
        self.face_detection = self.mp_face_detection.FaceDetection(
            min_detection_confidence=0.5
        )
        
        # Face Mesh ì¶”ê°€ (ëœë“œë§ˆí¬ í‘œì‹œìš©)
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        
        # ë§ˆì§€ë§‰ ì˜ˆì¸¡ ì‹œê°„
        self.last_prediction_time = 0
        
        # í˜„ì¬ ì˜ˆì¸¡ ê²°ê³¼
        self.current_predictions = {
            'engagement': 0,
            'confusion': 0,
            'frustration': 0,
            'boredom': 0
        }
        
        # ê°ì • ë ˆë²¨ ë¼ë²¨
        self.level_labels = ['Very Low', 'Low', 'High', 'Very High']
    
    def preprocess_frame(self, frame):
        """í”„ë ˆì„ ì „ì²˜ë¦¬"""
        # BGR to RGB
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # ì–¼êµ´ ê°ì§€ ë° í¬ë¡­ (ì„ íƒì )
        results = self.face_detection.process(frame_rgb)
        
        if results.detections:
            detection = results.detections[0]
            bbox = detection.location_data.relative_bounding_box
            h, w = frame.shape[:2]
            
            # ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚° (ì—¬ìœ  ê³µê°„ ì¶”ê°€)
            x1 = max(0, int((bbox.xmin - 0.1) * w))
            y1 = max(0, int((bbox.ymin - 0.1) * h))
            x2 = min(w, int((bbox.xmin + bbox.width + 0.1) * w))
            y2 = min(h, int((bbox.ymin + bbox.height + 0.1) * h))
            
            # ì–¼êµ´ ì˜ì—­ í¬ë¡­
            face_crop = frame_rgb[y1:y2, x1:x2]
            
            # í¬ë¡­ëœ ì˜ì—­ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ì „ì²´ í”„ë ˆì„ ì‚¬ìš©
            if face_crop.shape[0] > 50 and face_crop.shape[1] > 50:
                frame_rgb = face_crop
        
        # í…ì„œë¡œ ë³€í™˜
        tensor = self.transform(frame_rgb)
        return tensor
    
    def add_frame(self, frame):
        """í”„ë ˆì„ì„ ë²„í¼ì— ì¶”ê°€"""
        processed = self.preprocess_frame(frame)
        self.frame_buffer.append(processed)
    
    def predict(self):
        """í˜„ì¬ ë²„í¼ì˜ í”„ë ˆì„ìœ¼ë¡œ ì˜ˆì¸¡"""
        if len(self.frame_buffer) < self.sequence_length:
            return None
        
        # ê· ë“± ê°„ê²©ìœ¼ë¡œ í”„ë ˆì„ ì„ íƒ
        indices = np.linspace(0, len(self.frame_buffer)-1, 
                            self.sequence_length, dtype=int)
        
        # ì„ íƒëœ í”„ë ˆì„ë“¤ì„ í…ì„œë¡œ ìŠ¤íƒ
        frames = torch.stack([self.frame_buffer[i] for i in indices])
        frames = frames.unsqueeze(0).to(device)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        
        # ì˜ˆì¸¡
        with torch.no_grad():
            outputs = self.model(frames)
        
        # ê° ê°ì •ì˜ ë ˆë²¨ ê³„ì‚° (0-3)
        predictions = {}
        for emotion in ['engagement', 'confusion', 'frustration', 'boredom']:
            probs = F.softmax(outputs[emotion], dim=1)
            level = torch.argmax(probs, dim=1).item()
            confidence = probs[0, level].item()
            predictions[emotion] = {
                'level': level,
                'label': self.level_labels[level],
                'confidence': confidence
            }
        
        return predictions
    
    def process_frame(self, frame):
        """í”„ë ˆì„ ì²˜ë¦¬ ë° ì˜ˆì¸¡"""
        # í”„ë ˆì„ ë²„í¼ì— ì¶”ê°€
        self.add_frame(frame)
        
        # ì˜ˆì¸¡ ì£¼ê¸° í™•ì¸
        current_time = time.time()
        if current_time - self.last_prediction_time >= self.prediction_interval:
            predictions = self.predict()
            if predictions:
                self.current_predictions = predictions
                self.last_prediction_time = current_time
        
        return self.current_predictions
    
    def draw_face_landmarks(self, frame):
        """ì–¼êµ´ ëœë“œë§ˆí¬ í‘œì‹œ"""
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.face_mesh.process(frame_rgb)
        
        if results.multi_face_landmarks:
            for face_landmarks in results.multi_face_landmarks:
                # ì–¼êµ´ ë©”ì‹œ ê·¸ë¦¬ê¸°
                self.mp_drawing.draw_landmarks(
                    frame,
                    face_landmarks,
                    self.mp_face_mesh.FACEMESH_CONTOURS,
                    landmark_drawing_spec=self.mp_drawing.DrawingSpec(
                        color=(0, 255, 0), thickness=1, circle_radius=1
                    ),
                    connection_drawing_spec=self.mp_drawing.DrawingSpec(
                        color=(0, 255, 0), thickness=1, circle_radius=1
                    )
                )
                
                # ì£¼ìš” í¬ì¸íŠ¸ ê°•ì¡° (ëˆˆ, ëˆˆì¹, ì…)
                # ì™¼ìª½ ëˆˆ
                for idx in [33, 133, 157, 158, 159, 160, 161, 163]:
                    x = int(face_landmarks.landmark[idx].x * frame.shape[1])
                    y = int(face_landmarks.landmark[idx].y * frame.shape[0])
                    cv2.circle(frame, (x, y), 2, (0, 255, 255), -1)
                
                # ì˜¤ë¥¸ìª½ ëˆˆ
                for idx in [362, 263, 387, 388, 389, 390, 391, 393]:
                    x = int(face_landmarks.landmark[idx].x * frame.shape[1])
                    y = int(face_landmarks.landmark[idx].y * frame.shape[0])
                    cv2.circle(frame, (x, y), 2, (0, 255, 255), -1)
                
                # ì…
                for idx in [61, 291, 39, 269, 0, 17, 18, 200]:
                    x = int(face_landmarks.landmark[idx].x * frame.shape[1])
                    y = int(face_landmarks.landmark[idx].y * frame.shape[0])
                    cv2.circle(frame, (x, y), 2, (255, 0, 255), -1)
        
        return frame
    
    def draw_predictions(self, frame, predictions):
        """ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í”„ë ˆì„ì— í‘œì‹œ"""
        # ë¨¼ì € ì–¼êµ´ ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
        frame = self.draw_face_landmarks(frame)
        
        if not predictions:
            return frame
        
        # ë°°ê²½ ë°•ìŠ¤ (ë°˜íˆ¬ëª…)
        overlay = frame.copy()
        cv2.rectangle(overlay, (10, 10), (350, 180), (0, 0, 0), -1)
        frame = cv2.addWeighted(frame, 0.7, overlay, 0.3, 0)
        
        # íƒ€ì´í‹€
        cv2.putText(frame, "DAiSEE Emotion Analysis", 
                   (20, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        # ê° ê°ì • í‘œì‹œ
        y_offset = 65
        colors = {
            'engagement': (0, 255, 0),   # ë…¹ìƒ‰
            'confusion': (0, 165, 255),   # ì£¼í™©ìƒ‰
            'frustration': (0, 0, 255),   # ë¹¨ê°„ìƒ‰
            'boredom': (255, 0, 255)      # ë³´ë¼ìƒ‰
        }
        
        for emotion, color in colors.items():
            if emotion in predictions and predictions[emotion]:
                pred = predictions[emotion]
                text = f"{emotion.capitalize()}: {pred['label']}"
                confidence = f"({pred['confidence']*100:.1f}%)"
                
                # ê°ì • ì´ë¦„ê³¼ ë ˆë²¨
                cv2.putText(frame, text, 
                           (20, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 
                           0.5, color, 1)
                
                # ì‹ ë¢°ë„
                cv2.putText(frame, confidence, 
                           (220, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 
                           0.4, color, 1)
                
                # ë ˆë²¨ ë°”
                bar_width = int(pred['level'] * 25 + 25)  # 0-3 -> 25-100
                cv2.rectangle(frame, (280, y_offset-10), 
                            (280 + bar_width, y_offset), 
                            color, -1)
                
                y_offset += 30
        
        # FPS ë° ë²„í¼ ìƒíƒœ
        buffer_fill = len(self.frame_buffer)
        cv2.putText(frame, f"Buffer: {buffer_fill}/{self.buffer_size}", 
                   (20, 160), cv2.FONT_HERSHEY_SIMPLEX, 
                   0.4, (200, 200, 200), 1)
        
        return frame


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("=" * 50)
    print("ğŸ¥ Realtime DAiSEE Emotion Tracker")
    print("=" * 50)
    
    # íŠ¸ë˜ì»¤ ì´ˆê¸°í™”
    tracker = RealtimeDAiSEETracker(
        model_path='daisee_local_model.pth',
        sequence_length=30,
        buffer_size=30,
        prediction_interval=0.5  # 0.5ì´ˆë§ˆë‹¤ ì˜ˆì¸¡
    )
    
    # ì›¹ìº  ì‹œì‘
    cap = cv2.VideoCapture(0)
    
    # FPS ê³„ì‚°ìš©
    fps_time = time.time()
    fps_counter = 0
    current_fps = 0
    
    print("\nğŸ“¸ Camera started")
    print("Press 'q' to quit")
    print("\nCollecting frames for initial prediction...")
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # FPS ê³„ì‚°
        fps_counter += 1
        if time.time() - fps_time >= 1.0:
            current_fps = fps_counter
            fps_counter = 0
            fps_time = time.time()
        
        # í”„ë ˆì„ ì²˜ë¦¬ ë° ì˜ˆì¸¡
        predictions = tracker.process_frame(frame)
        
        # ì˜ˆì¸¡ ê²°ê³¼ í‘œì‹œ
        frame = tracker.draw_predictions(frame, predictions)
        
        # FPS í‘œì‹œ
        cv2.putText(frame, f"FPS: {current_fps}", 
                   (frame.shape[1] - 100, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
        
        # í”„ë ˆì„ í‘œì‹œ
        cv2.imshow('DAiSEE Realtime Tracker', frame)
        
        # 'q' í‚¤ë¡œ ì¢…ë£Œ
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    
    cap.release()
    cv2.destroyAllWindows()
    print("\nâœ… Tracker stopped")


if __name__ == "__main__":
    main()