"""
ì‹¤ì‹œê°„ DAiSEE ê¸°ë°˜ ì–¼êµ´ ì´í•´ë„ ì¶”ì  - HuggingFace ë²„ì „
HuggingFaceì—ì„œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì‚¬ìš©
"""

import cv2
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
import numpy as np
from collections import deque
import time
import os
import mediapipe as mp
from huggingface_hub import hf_hub_download

# GPU ì„¤ì • (Mac M3 Proì˜ ê²½ìš° MPS ì‚¬ìš©)
if torch.cuda.is_available():
    device = torch.device("cuda")
elif torch.backends.mps.is_available():
    device = torch.device("mps")
else:
    device = torch.device("cpu")
print(f"Using device: {device}")


class RealtimeDAiSEETrackerHF:
    """ì‹¤ì‹œê°„ DAiSEE ê¸°ë°˜ ì¶”ì ê¸° - HuggingFace ë²„ì „"""
    
    def __init__(self, 
                 repo_id='combe4259/face-comprehension',
                 sequence_length=30, 
                 buffer_size=30,
                 prediction_interval=0.5,
                 cache_dir='./model_cache'):
        """
        Args:
            repo_id: HuggingFace ëª¨ë¸ ë ˆí¬ì§€í† ë¦¬ ID
            sequence_length: ëª¨ë¸ì— ì…ë ¥í•  í”„ë ˆì„ ìˆ˜
            buffer_size: ë²„í¼ í¬ê¸°
            prediction_interval: ì˜ˆì¸¡ ì£¼ê¸° (ì´ˆ)
            cache_dir: ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬
        """
        self.sequence_length = sequence_length
        self.buffer_size = buffer_size
        self.prediction_interval = prediction_interval
        self.repo_id = repo_id
        
        # í”„ë ˆì„ ë²„í¼ (dequeë¡œ ìë™ìœ¼ë¡œ ì˜¤ë˜ëœ í”„ë ˆì„ ì œê±°)
        self.frame_buffer = deque(maxlen=buffer_size)
        
        # HuggingFaceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ
        print(f"ğŸ“¥ Downloading model from HuggingFace: {repo_id}")
        try:
            # ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            model_path = hf_hub_download(
                repo_id=repo_id,
                filename="pytorch_model.bin",
                cache_dir=cache_dir
            )
            print(f"âœ… Model downloaded to: {model_path}")
            
            # ëª¨ë¸ ë¡œë“œ (ì „ì²´ ëª¨ë¸ì´ ì €ì¥ëœ ê²½ìš°)
            print("Loading model...")
            self.model = torch.load(model_path, map_location=device)
            self.model.to(device)
            self.model.eval()
            print("âœ… Model loaded successfully from HuggingFace")
            
        except Exception as e:
            print(f"âŒ Failed to load model from HuggingFace: {e}")
            print("âš ï¸ Please check:")
            print("  1. Repository exists: https://huggingface.co/" + repo_id)
            print("  2. File 'pytorch_model.bin' exists in the repository")
            print("  3. You have internet connection")
            raise e
        
        # ì „ì²˜ë¦¬ ë³€í™˜
        self.transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((112, 112)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                              std=[0.229, 0.224, 0.225])
        ])
        
        # MediaPipe ì–¼êµ´ ê°ì§€ ë° ë©”ì‹œ
        self.mp_face_detection = mp.solutions.face_detection
        self.mp_face_mesh = mp.solutions.face_mesh
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_drawing_styles = mp.solutions.drawing_styles
        
        self.face_detection = self.mp_face_detection.FaceDetection(
            min_detection_confidence=0.5
        )
        
        # Face Mesh ì¶”ê°€ (ëœë“œë§ˆí¬ í‘œì‹œìš©)
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        
        # ë§ˆì§€ë§‰ ì˜ˆì¸¡ ì‹œê°„
        self.last_prediction_time = 0
        
        # í˜„ì¬ ì˜ˆì¸¡ ê²°ê³¼
        self.current_predictions = {
            'engagement': 0,
            'confusion': 0,
            'frustration': 0,
            'boredom': 0
        }
        
        # ê°ì • ë ˆë²¨ ë¼ë²¨
        self.level_labels = ['Very Low', 'Low', 'High', 'Very High']
    
    def preprocess_frame(self, frame):
        """í”„ë ˆì„ ì „ì²˜ë¦¬"""
        # BGR to RGB
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # ì–¼êµ´ ê°ì§€ ë° í¬ë¡­ (ì„ íƒì )
        results = self.face_detection.process(frame_rgb)
        
        if results.detections:
            detection = results.detections[0]
            bbox = detection.location_data.relative_bounding_box
            h, w = frame.shape[:2]
            
            # ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚° (ì—¬ìœ  ê³µê°„ ì¶”ê°€)
            x1 = max(0, int((bbox.xmin - 0.1) * w))
            y1 = max(0, int((bbox.ymin - 0.1) * h))
            x2 = min(w, int((bbox.xmin + bbox.width + 0.1) * w))
            y2 = min(h, int((bbox.ymin + bbox.height + 0.1) * h))
            
            # ì–¼êµ´ ì˜ì—­ í¬ë¡­
            face_crop = frame_rgb[y1:y2, x1:x2]
            
            # í¬ë¡­ëœ ì˜ì—­ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ì „ì²´ í”„ë ˆì„ ì‚¬ìš©
            if face_crop.shape[0] > 50 and face_crop.shape[1] > 50:
                frame_rgb = face_crop
        
        # í…ì„œë¡œ ë³€í™˜
        tensor = self.transform(frame_rgb)
        return tensor
    
    def add_frame(self, frame):
        """í”„ë ˆì„ì„ ë²„í¼ì— ì¶”ê°€"""
        processed = self.preprocess_frame(frame)
        self.frame_buffer.append(processed)
    
    def predict(self):
        """í˜„ì¬ ë²„í¼ì˜ í”„ë ˆì„ìœ¼ë¡œ ì˜ˆì¸¡"""
        if len(self.frame_buffer) < self.sequence_length:
            return None
        
        # ê· ë“± ê°„ê²©ìœ¼ë¡œ í”„ë ˆì„ ì„ íƒ
        indices = np.linspace(0, len(self.frame_buffer)-1, 
                            self.sequence_length, dtype=int)
        
        # ì„ íƒëœ í”„ë ˆì„ë“¤ì„ í…ì„œë¡œ ìŠ¤íƒ
        frames = torch.stack([self.frame_buffer[i] for i in indices])
        frames = frames.unsqueeze(0).to(device)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        
        # ì˜ˆì¸¡
        with torch.no_grad():
            outputs = self.model(frames)
        
        # ê° ê°ì •ì˜ ë ˆë²¨ ê³„ì‚° (0-3)
        predictions = {}
        for emotion in ['engagement', 'confusion', 'frustration', 'boredom']:
            probs = F.softmax(outputs[emotion], dim=1)
            level = torch.argmax(probs, dim=1).item()
            confidence = probs[0, level].item()
            predictions[emotion] = {
                'level': level,
                'label': self.level_labels[level],
                'confidence': confidence
            }
        
        return predictions
    
    def process_frame(self, frame):
        """í”„ë ˆì„ ì²˜ë¦¬ ë° ì˜ˆì¸¡"""
        # í”„ë ˆì„ ë²„í¼ì— ì¶”ê°€
        self.add_frame(frame)
        
        # ì˜ˆì¸¡ ì£¼ê¸° í™•ì¸
        current_time = time.time()
        if current_time - self.last_prediction_time >= self.prediction_interval:
            predictions = self.predict()
            if predictions:
                self.current_predictions = predictions
                self.last_prediction_time = current_time
        
        return self.current_predictions
    
    def draw_face_landmarks(self, frame):
        """ì–¼êµ´ ëœë“œë§ˆí¬ í‘œì‹œ"""
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.face_mesh.process(frame_rgb)
        
        if results.multi_face_landmarks:
            for face_landmarks in results.multi_face_landmarks:
                # ì–¼êµ´ ë©”ì‹œ ê·¸ë¦¬ê¸°
                self.mp_drawing.draw_landmarks(
                    frame,
                    face_landmarks,
                    self.mp_face_mesh.FACEMESH_CONTOURS,
                    landmark_drawing_spec=self.mp_drawing.DrawingSpec(
                        color=(0, 255, 0), thickness=1, circle_radius=1
                    ),
                    connection_drawing_spec=self.mp_drawing.DrawingSpec(
                        color=(0, 255, 0), thickness=1, circle_radius=1
                    )
                )
                
                # ì£¼ìš” í¬ì¸íŠ¸ ê°•ì¡° (ëˆˆ, ëˆˆì¹, ì…)
                # ì™¼ìª½ ëˆˆ
                for idx in [33, 133, 157, 158, 159, 160, 161, 163]:
                    x = int(face_landmarks.landmark[idx].x * frame.shape[1])
                    y = int(face_landmarks.landmark[idx].y * frame.shape[0])
                    cv2.circle(frame, (x, y), 2, (0, 255, 255), -1)
                
                # ì˜¤ë¥¸ìª½ ëˆˆ
                for idx in [362, 263, 387, 388, 389, 390, 391, 393]:
                    x = int(face_landmarks.landmark[idx].x * frame.shape[1])
                    y = int(face_landmarks.landmark[idx].y * frame.shape[0])
                    cv2.circle(frame, (x, y), 2, (0, 255, 255), -1)
                
                # ì…
                for idx in [61, 291, 39, 269, 0, 17, 18, 200]:
                    x = int(face_landmarks.landmark[idx].x * frame.shape[1])
                    y = int(face_landmarks.landmark[idx].y * frame.shape[0])
                    cv2.circle(frame, (x, y), 2, (255, 0, 255), -1)
        
        return frame
    
    def draw_predictions(self, frame, predictions):
        """ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í”„ë ˆì„ì— í‘œì‹œ"""
        # ë¨¼ì € ì–¼êµ´ ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
        frame = self.draw_face_landmarks(frame)
        
        if not predictions:
            return frame
        
        # ë°°ê²½ ë°•ìŠ¤ (ë°˜íˆ¬ëª…)
        overlay = frame.copy()
        cv2.rectangle(overlay, (10, 10), (350, 200), (0, 0, 0), -1)
        frame = cv2.addWeighted(frame, 0.7, overlay, 0.3, 0)
        
        # íƒ€ì´í‹€
        cv2.putText(frame, "DAiSEE Emotion Analysis", 
                   (20, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        # HuggingFace ëª¨ë¸ ì •ë³´
        cv2.putText(frame, f"Model: {self.repo_id}", 
                   (20, 55), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (200, 200, 200), 1)
        
        # ê° ê°ì • í‘œì‹œ
        y_offset = 85
        colors = {
            'engagement': (0, 255, 0),   # ë…¹ìƒ‰
            'confusion': (0, 165, 255),   # ì£¼í™©ìƒ‰
            'frustration': (0, 0, 255),   # ë¹¨ê°„ìƒ‰
            'boredom': (255, 0, 255)      # ë³´ë¼ìƒ‰
        }
        
        for emotion, color in colors.items():
            if emotion in predictions and predictions[emotion]:
                pred = predictions[emotion]
                text = f"{emotion.capitalize()}: {pred['label']}"
                confidence = f"({pred['confidence']*100:.1f}%)"
                
                # ê°ì • ì´ë¦„ê³¼ ë ˆë²¨
                cv2.putText(frame, text, 
                           (20, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 
                           0.5, color, 1)
                
                # ì‹ ë¢°ë„
                cv2.putText(frame, confidence, 
                           (220, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 
                           0.4, color, 1)
                
                # ë ˆë²¨ ë°”
                bar_width = int(pred['level'] * 25 + 25)  # 0-3 -> 25-100
                cv2.rectangle(frame, (280, y_offset-10), 
                            (280 + bar_width, y_offset), 
                            color, -1)
                
                y_offset += 30
        
        # FPS ë° ë²„í¼ ìƒíƒœ
        buffer_fill = len(self.frame_buffer)
        cv2.putText(frame, f"Buffer: {buffer_fill}/{self.buffer_size}", 
                   (20, 180), cv2.FONT_HERSHEY_SIMPLEX, 
                   0.4, (200, 200, 200), 1)
        
        return frame


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("=" * 50)
    print("ğŸ¥ Realtime DAiSEE Emotion Tracker (HuggingFace)")
    print("=" * 50)
    
    # íŠ¸ë˜ì»¤ ì´ˆê¸°í™” (HuggingFace ëª¨ë¸ ì‚¬ìš©)
    try:
        tracker = RealtimeDAiSEETrackerHF(
            repo_id='combe4259/face-comprehension',  # HuggingFace ëª¨ë¸
            sequence_length=30,
            buffer_size=30,
            prediction_interval=0.5  # 0.5ì´ˆë§ˆë‹¤ ì˜ˆì¸¡
        )
    except Exception as e:
        print(f"âŒ Failed to initialize tracker: {e}")
        return
    
    # ì›¹ìº  ì‹œì‘
    cap = cv2.VideoCapture(0)
    
    # FPS ê³„ì‚°ìš©
    fps_time = time.time()
    fps_counter = 0
    current_fps = 0
    
    print("\nğŸ“¸ Camera started")
    print("Press 'q' to quit")
    print("\nCollecting frames for initial prediction...")
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # FPS ê³„ì‚°
        fps_counter += 1
        if time.time() - fps_time >= 1.0:
            current_fps = fps_counter
            fps_counter = 0
            fps_time = time.time()
        
        # í”„ë ˆì„ ì²˜ë¦¬ ë° ì˜ˆì¸¡
        predictions = tracker.process_frame(frame)
        
        # ì˜ˆì¸¡ ê²°ê³¼ í‘œì‹œ
        frame = tracker.draw_predictions(frame, predictions)
        
        # FPS í‘œì‹œ
        cv2.putText(frame, f"FPS: {current_fps}", 
                   (frame.shape[1] - 100, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
        
        # í”„ë ˆì„ í‘œì‹œ
        cv2.imshow('DAiSEE Realtime Tracker (HuggingFace)', frame)
        
        # 'q' í‚¤ë¡œ ì¢…ë£Œ
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    
    cap.release()
    cv2.destroyAllWindows()
    print("\nâœ… Tracker stopped")


if __name__ == "__main__":
    main()