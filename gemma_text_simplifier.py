"""
Gemmaë¥¼ ì‚¬ìš©í•œ ê¸ˆìœµ í…ìŠ¤íŠ¸ ê°„ì†Œí™” ë° ë‚œì´ë„ í‰ê°€
ê¸°ì¡´ JSON íŒŒì¼ì˜ ë¬¸ì¥ë“¤ì„ ì‰½ê²Œ ë³€í™˜í•˜ê³  ë‚œì´ë„ë¥¼ í‰ê°€
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
try:
    from transformers import BitsAndBytesConfig
    BNBCONFIG_AVAILABLE = True
except ImportError:
    BNBCONFIG_AVAILABLE = False
    print("âš ï¸ BitsAndBytesConfig not available")
from huggingface_hub import login
import json
from tqdm import tqdm
import os
from datetime import datetime
from typing import List, Dict

class GemmaTextSimplifier:
    def __init__(self, hf_token=""):
        """
        Gemma ê¸°ë°˜ í…ìŠ¤íŠ¸ ê°„ì†Œí™” ë° ë‚œì´ë„ í‰ê°€
        """
        self.model_name = "google/gemma-2-2b-it"
        
        # HuggingFace ë¡œê·¸ì¸
        if hf_token:
            login(token=hf_token)
            print("âœ… HuggingFace ë¡œê·¸ì¸ ì™„ë£Œ")
        
        # ëª¨ë¸ ë¡œë“œ
        print(f"ğŸ”„ Gemma ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.load_model()
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        
        # ê²°ê³¼ ì €ì¥ìš©
        self.results = []
    
    def load_model(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        try:
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            if torch.cuda.is_available():
                print(f"ğŸ® GPU ì‚¬ìš©: {torch.cuda.get_device_name(0)}")
                self.device = "cuda"
                
                # BitsAndBytes ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ 4bit ì–‘ìí™” ì‹œë„
                if BNBCONFIG_AVAILABLE:
                    try:
                        print("ğŸ”§ 4bit ì–‘ìí™” ì‹œë„...")
                        quantization_config = BitsAndBytesConfig(
                            load_in_4bit=True,
                            bnb_4bit_compute_dtype=torch.float16,
                            bnb_4bit_use_double_quant=True,
                            bnb_4bit_quant_type="nf4"
                        )
                        self.model = AutoModelForCausalLM.from_pretrained(
                            self.model_name,
                            quantization_config=quantization_config,
                            device_map="auto"
                        )
                        print("âœ… 4bit ì–‘ìí™” ì„±ê³µ!")
                    except Exception as e:
                        print(f"âš ï¸ 4bit ì‹¤íŒ¨, ì¼ë°˜ ëª¨ë“œ: {e}")
                        self.model = AutoModelForCausalLM.from_pretrained(
                            self.model_name,
                            device_map="auto",
                            dtype=torch.float16
                        )
                else:
                    # ì–‘ìí™” ì—†ì´ ì¼ë°˜ ë¡œë“œ
                    print("ğŸ“¦ ì¼ë°˜ ëª¨ë“œ ë¡œë“œ (ì–‘ìí™” ì—†ìŒ)")
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.model_name,
                        device_map="auto",
                        dtype=torch.float16
                    )
            elif torch.backends.mps.is_available():
                print("ğŸ MPS (Mac GPU) ì‚¬ìš©")
                self.device = "mps"
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    device_map="mps",
                    dtype=torch.float16
                )
            else:
                print("ğŸ’» CPU ëª¨ë“œ")
                self.device = "cpu"
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    device_map="cpu",
                    dtype=torch.float32
                )
        except Exception as e:
            print(f"âš ï¸ ì–‘ìí™” ì‹¤íŒ¨, ì¼ë°˜ ëª¨ë“œë¡œ ì¬ì‹œë„: {e}")
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                device_map="auto"
            )
    
    def simplify_text(self, text: str) -> str:
        """ë³µì¡í•œ ê¸ˆìœµ í…ìŠ¤íŠ¸ë¥¼ ì‰¬ìš´ ë§ë¡œ ë³€í™˜"""
        
        prompt = f"""ì•„ë˜ ë¬¸ì¥ì„ ì‰¬ìš´ í•œêµ­ì–´ë¡œ ë‹¤ì‹œ ì¨ì£¼ì„¸ìš”. ì›ë³¸ ë¬¸ì¥ì˜ ì˜ë¯¸ë¥¼ ì •í™•íˆ ìœ ì§€í•˜ë©´ì„œ ì–´ë ¤ìš´ ë‹¨ì–´ë§Œ ì‰½ê²Œ ë°”ê¿”ì£¼ì„¸ìš”.

ì›ë³¸ ë¬¸ì¥: {text}

ìœ„ ë¬¸ì¥ì„ ì´ˆë“±í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‰½ê²Œ ë‹¤ì‹œ ì“°ë©´:"""
        
        try:
            # í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=512
            )
            
            # ëª¨ë¸ê³¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            if hasattr(self, 'device'):
                inputs = inputs.to(self.device)
            
            # ìƒì„±
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=100,
                    temperature=0.3,  # ë‚®ì¶°ì„œ ë” ì•ˆì •ì ìœ¼ë¡œ
                    do_sample=True,
                    top_p=0.85,
                    top_k=50,
                    repetition_penalty=1.1,  # ë°˜ë³µ ë°©ì§€
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # ë””ì½”ë”©
            response = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[-1]:],
                skip_special_tokens=True
            ).strip()
            
            # ì‘ë‹µì—ì„œ ì²« ë¬¸ì¥ë§Œ ì¶”ì¶œ (ë” ê¹”ë”í•˜ê²Œ)
            if '.' in response:
                response = response.split('.')[0] + '.'
            
            return response
            
        except Exception as e:
            print(f"[ERROR] ë³€í™˜ ì‹¤íŒ¨: {e}")
            return text  # ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜
    
    def process_json_file(self, input_path: str, output_path: str = None):
        """JSON íŒŒì¼ ì²˜ë¦¬: ì›ë³¸ -> ê°„ì†Œí™” -> ë‚œì´ë„ ë§¤í•‘"""
        
        # ì¶œë ¥ ê²½ë¡œ ì„¤ì •
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"simplified_data_{timestamp}.json"
        
        # JSON íŒŒì¼ ë¡œë“œ
        print(f"ğŸ“‚ Loading: {input_path}")
        with open(input_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # ë°ì´í„° í˜•ì‹ í™•ì¸
        if isinstance(data, dict) and 'sentences' in data:
            sentences = data['sentences']
        elif isinstance(data, list):
            sentences = data
        else:
            print("âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” JSON í˜•ì‹ì…ë‹ˆë‹¤.")
            return
        
        print(f"ğŸ“Š ì´ {len(sentences)} ê°œ ë¬¸ì¥ ì²˜ë¦¬ ì‹œì‘")
        
        # ê²°ê³¼ ì €ì¥ìš©
        results = []
        
        # ê° ë¬¸ì¥ ì²˜ë¦¬
        for item in tqdm(sentences, desc="Processing"):
            # ì›ë³¸ í…ìŠ¤íŠ¸ì™€ ë‚œì´ë„ ì¶”ì¶œ
            if isinstance(item, dict):
                original_text = item.get('text', item.get('sentence', str(item)))
                # ê¸°ì¡´ ë‚œì´ë„ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ 5
                difficulty = item.get('difficulty', item.get('score', 5))
            else:
                original_text = str(item)
                difficulty = 5  # ê¸°ë³¸ê°’
            
            # ê°„ì†Œí™”ë§Œ ìˆ˜í–‰ (ë‚œì´ë„ í‰ê°€ ì œê±°)
            simplified_text = self.simplify_text(original_text)
            
            # ê²°ê³¼ ì €ì¥
            result = {
                "complex": original_text,  # í•™ìŠµ ë°ì´í„° í˜•ì‹ì— ë§ì¶¤
                "simple": simplified_text,
                "difficulty": difficulty  # ê¸°ì¡´ ë‚œì´ë„ ì‚¬ìš©
            }
            
            results.append(result)
            
            # ì§„í–‰ìƒí™© ì¶œë ¥ (10ê°œë§ˆë‹¤)
            if len(results) % 10 == 0:
                print(f"âœ… {len(results)} ê°œ ì™„ë£Œ")
                # ìƒ˜í”Œ ì¶œë ¥
                print(f"  ì›ë³¸: {original_text[:50]}...")
                print(f"  ë³€í™˜: {simplified_text[:50]}...")
                print(f"  ë‚œì´ë„: {difficulty}")
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        output_data = {
            "metadata": {
                "source_file": input_path,
                "processed_date": datetime.now().isoformat(),
                "total_sentences": len(results),
                "model": self.model_name
            },
            "data": results
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"ğŸ“„ ê²°ê³¼ ì €ì¥: {output_path}")
        
        # í†µê³„ ì¶œë ¥
        self.print_statistics(results)
        
        return results
    
    def print_statistics(self, results: List[Dict]):
        """ì²˜ë¦¬ ê²°ê³¼ í†µê³„ ì¶œë ¥"""
        
        if not results:
            return
        
        # í†µê³„ ê³„ì‚°
        difficulties = [r['difficulty'] for r in results]
        
        print("\nğŸ“Š í†µê³„ ì •ë³´:")
        print("-" * 40)
        print(f"ì´ ì²˜ë¦¬ ë¬¸ì¥: {len(results)} ê°œ")
        print(f"í‰ê·  ë‚œì´ë„: {sum(difficulties) / len(difficulties):.2f}")
        print(f"ìµœëŒ€ ë‚œì´ë„: {max(difficulties)}")
        print(f"ìµœì†Œ ë‚œì´ë„: {min(difficulties)}")
        
        # ë‚œì´ë„ ë¶„í¬
        print("\në‚œì´ë„ ë¶„í¬:")
        for level in range(1, 11):
            count = sum(1 for d in difficulties if d == level)
            if count > 0:
                bar = "â–ˆ" * (count * 30 // len(results))
                print(f"  {level:2d}: {count:3d}ê°œ {bar}")
        
        # ìƒ˜í”Œ ì¶œë ¥
        print("\nğŸ“ ìƒ˜í”Œ ê²°ê³¼ (ë†’ì€ ë‚œì´ë„ ìˆœ):")
        print("-" * 40)
        sorted_results = sorted(results, key=lambda x: x['difficulty'], reverse=True)
        
        for i, result in enumerate(sorted_results[:3], 1):
            print(f"\n[{i}] ë‚œì´ë„ {result['difficulty']}")
            print(f"ì›ë³¸: {result['complex'][:80]}...")
            print(f"ë³€í™˜: {result['simple'][:80]}...")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("=" * 50)
    print("Gemma í…ìŠ¤íŠ¸ ê°„ì†Œí™” ì‹œìŠ¤í…œ")
    print("=" * 50)
    
    # ê°„ì†Œí™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    simplifier = GemmaTextSimplifier()
    
    # ì…ë ¥ íŒŒì¼ ê²½ë¡œ
    input_file = "/content/training_data_20250910_092856.json"
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(input_file):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
        # ëŒ€í™”í˜• ëª¨ë“œ
        print("\nëŒ€í™”í˜• ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
        while True:
            text = input("\në³µì¡í•œ ë¬¸ì¥ ì…ë ¥ (ì¢…ë£Œ: quit): ").strip()
            if text.lower() == 'quit':
                break
            
            simplified = simplifier.simplify_text(text)
            
            print(f"\nì›ë³¸: {text}")
            print(f"ë³€í™˜: {simplified}")
    else:
        # íŒŒì¼ ì²˜ë¦¬
        output_file = "financial_training_simplified.json"
        results = simplifier.process_json_file(input_file, output_file)
        
        print(f"\nâœ… ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"ê²°ê³¼ íŒŒì¼: {output_file}")


if __name__ == "__main__":
    main()